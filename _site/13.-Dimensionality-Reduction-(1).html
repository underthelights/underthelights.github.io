<!DOCTYPE html>
<html lang="en"><head>
  
  <script type="text/x-mathjax-config">
     MathJax.Hub.Config({
         TeX: {
         equationNumbers: {
             autoNumber: "AMS"
         }
         },
         tex2jax: {
         inlineMath: [ ['$', '$'] ],
         displayMath: [ ['$$', '$$'] ],
         processEscapes: true,
     }
     });
     MathJax.Hub.Register.MessageHook("Math Processing Error",function (message) {
         alert("Math Processing Error: "+message[1]);
         });
     MathJax.Hub.Register.MessageHook("TeX Jax - parse error",function (message) {
         alert("Math Processing Error: "+message[1]);
         });
     </script>
<script type="text/javascript" async src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
</script>
  

  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <title>13. Dimensionality Reduction (1) âœ± Kyuhwan Shim</title>

  <link rel="stylesheet" href="/assets/css/style.css">

  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Barlow:ital,wght@0,100;0,200;0,300;0,400;0,500;0,600;0,700;0,800;1,100;1,200;1,300;1,400;1,500;1,600;1,700;1,800&family=Noto+Sans+TC&display=swap" rel="stylesheet">

  <link href="/assets/fontawesome/all.min.css" rel="stylesheet">

  <!-- Bootstrap -->
  <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.2.2/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-Zenh87qX5JnK2Jl0vWa8Ck2rdkQ2Bzep5IDxbcnCeuOxjzrPF/et3URy9Bv1WTRi" crossorigin="anonymous">
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.2.2/dist/js/bootstrap.bundle.min.js" integrity="sha384-OERcA2EqjJCMA+/3y+gxIOqMEjwtxJY7qPCqsdltbNJuaOe923+mo//f6V8Qbsw3" crossorigin="anonymous"></script>
  <script src='https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js'></script>

  <!-- GA -->
  
<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-VG8LB4J4EL"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-VG8LB4J4EL');
</script>


  <!-- favicon -->
  <link rel="apple-touch-icon" sizes="57x57" href="/assets/images/main/favicon/favicon.ico">
  <link rel="apple-touch-icon" sizes="57x57" href="/assets/images/main/favicon/apple-icon-57x57.png">
  <link rel="apple-touch-icon" sizes="60x60" href="/assets/images/main/favicon/apple-icon-60x60.png">
  <link rel="apple-touch-icon" sizes="72x72" href="/assets/images/main/favicon/apple-icon-72x72.png">
  <link rel="apple-touch-icon" sizes="76x76" href="/assets/images/main/favicon/apple-icon-76x76.png">
  <link rel="apple-touch-icon" sizes="114x114" href="/assets/images/main/favicon/apple-icon-114x114.png">
  <link rel="apple-touch-icon" sizes="120x120" href="/assets/images/main/favicon/apple-icon-120x120.png">
  <link rel="apple-touch-icon" sizes="144x144" href="/assets/images/main/favicon/apple-icon-144x144.png">
  <link rel="apple-touch-icon" sizes="152x152" href="/assets/images/main/favicon/apple-icon-152x152.png">
  <link rel="apple-touch-icon" sizes="180x180" href="/assets/images/main/favicon/apple-icon-180x180.png">
  <link rel="icon" type="image/png" sizes="192x192"  href="/assets/images/main/favicon/android-icon-192x192.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/assets/images/main/favicon/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="96x96" href="/assets/images/main/favicon/favicon-96x96.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/assets/images/main/favicon/favicon-16x16.png">
  <link rel="manifest" href="">
  <meta name="msapplication-TileColor" content="#ffffff">
  <meta name="msapplication-TileImage" content="/assets/images/main/favicon/ms-icon-144x144.png">
  <meta name="theme-color" content="#ffffff">

  <!-- Opengraph -->
  <meta property="og:type" content="website">
  <meta property="og:image" content="/assets/images/main/favicon/og.png13.-Dimensionality-Reduction-(1).jpg">
  <meta property="og:title" content="13. Dimensionality Reduction (1) | Kyuhwan Shim">
  <meta property="og:description" content="Paper overview of  et al., ">

  <!-- Google scholar -->
  
  <meta name="citation_title" content="13. Dimensionality Reduction (1)">
  
  <meta name="citation_publication_date" content="">
  
  <meta name="citation_conference_title" content="">
  
  <meta name="citation_pdf_url" content="https://underthelights.github.io/assets/pdf/13.-Dimensionality-Reduction-(1).pdf">
  
</head>
<body><header class="page-content">
  <nav class="navbar navbar-expand-md navbar-light py-4">
    <div class="container-fluid">
      <button class="navbar-toggler ms-auto" type="button" data-bs-toggle="collapse" data-bs-target="#navbarTogglerDemo01" aria-controls="navbarTogglerDemo01" aria-expanded="false" aria-label="Toggle navigation">
        <span class="navbar-toggler-icon"></span>
      </button>
      <div class="collapse navbar-collapse" id="navbarTogglerDemo01">
        <ul class="navbar-nav ms-auto mt-4 mt-lg-0 navbar-nav-scroll">
          <li class="nav-item">
            <a class="nav-link " aria-current="page" href="/">Home</a>
          </li>
          <li class="nav-item">
            <a class="nav-link " aria-current="page" href="/news">News</a>
          </li>
          <li class="nav-item">
            <a class="nav-link " aria-current="page" href="/publication">publication</a>
          </li>
          <li class="nav-item">
            <a class="nav-link " aria-current="page" href="/project">Project</a>
          </li>
          <li class="nav-item">
            <a class="nav-link " aria-current="page" href="/oconnect">oconnect</a>
          </li>
          <li class="nav-item">
            <a class="nav-link " aria-current="page" href="/cv">CV</a>
          </li>
          <li class="nav-item dropdown">
              <a href="#" class="nav-link dropdown-toggle " data-bs-toggle="dropdown">fun</a>
              <div class="dropdown-menu">
                  <a href="/music" class="dropdown-item">music</a>
                  <a href="/artwork" class="dropdown-item">artwork</a>
              </div>
          </li>
          <li class="nav-item">
            <a class="nav-link " aria-current="page" target="_blank" rel="noopener noreferrer" href="https://underthelights.github.io/blog/">Blog<i class="fa-regular fa-arrow-up-right-from-square" style="padding-left: 5px;"></i></a>
          </li>
        </ul>
      </div>
    </div>
  </nav>
</header>

<style>
  .dropdown-toggle {
    background-color: transparent;
    border-color: #fff;
    border-style: solid;
    border-top: none;
    border-right: none;
    border-left: none;
    transition: color .15s ease-in-out, background-color .15s ease-in-out,border-color .15s ease-in-out;
  }
  .dropdown-toggle:hover {
    font-weight: 500
  }
  .dropdown:hover .dropdown-menu {
    display: block;
    margin-top: 0;
 }
 .dropdown-menu {
    --bs-dropdown-border-radius: 0 !important;
    padding-top: 0 !important;
    padding-bottom: 0 !important
 }
 .dropdown-item {
  font-size: .95rem !important;
  padding: .3rem .75rem !important;
 }
 .dropdown-item:active {
  background-color: #999 !important
 }
</style><main class="page-content" aria-label="Content">
      <div class="wrapper">
        <style>
  h1,
  h2,
  h3,
  h4,
  h5,
  h6 {
    
      font-family: 'Arial', sans-serif;
      /* Use a modern font */
      margin-top: 20px;
      margin-bottom: 10px;
  }
  /* í¬ìŠ¤íŠ¸ í—¤ë”ì— ì ìš©ë˜ëŠ” ì—¬ë°± ì œê±° */
  .post-header, h1, h2, h3, h4, h5, h6 {
      margin-left: 0 !important; 
      padding-left: 0 !important;
  }


  .post-title {
      font-size: 2em; /* í°íŠ¸ í¬ê¸° ì¦ê°€ */
      font-weight: bold; /* ë³¼ë“œì²´ */
      color: #333; /* ìƒ‰ìƒ ë³€ê²½ */
      box-shadow: inset 0 -20px 0 #bbb7e8;
      max-width: max-content;
    }

  /* Updated styling for h1 */
  h1 {
    font-size: 1.5em;
    /* Increase font size for main titles */
    box-shadow: inset 0 -10px 0 #bbb7e8;
    max-width: max-content; 
    font-weight: bold; /* ë³¼ë“œì²´ */
    border-bottom: none;
    /* Remove bottom border */
    padding-bottom: 0;
    /* Adjust padding if needed */
  }

  h2 {
    font-size: 1.2em;
    margin-left: none;
    font-weight: bold; /* ë³¼ë“œì²´ */
    max-width: max-content; 
    background-color: #cdcbe9;
  }

  h3 {
    font-size: 1.1em;
    max-width: max-content; 
    
    max-width: max-content; 
    background-color: #bbb7e8;
  }

  h4 {
    font-size: 0.75em;
    color: #888888;
  }

  h5 {
    font-size: 0.5em;
    color: #a6a6a6;
  }

  h6 {
    font-size: 1em;
    color: #bcbcbc;
  }

  /* Add some additional styling if needed */
  .post-content {
    margin-left: 20px; /* ì™¼ìª½ ì—¬ë°± ì¶”ê°€ */
    /* margin-right: 20px; ì˜¤ë¥¸ìª½ì— TOC ë„ˆë¹„ + ì—¬ë°± ë§Œí¼ ì¶”ê°€ */
    line-height: 1.6;/* Improve readability */
    color: #333;/* Dark grey color for text */
  }
  /* Category Chart Style */
  .category-chart {
    font-family: 'Arial', sans-serif;
    /* width: 100%; */
    /* height: 100px; Adjust as needed */
  }

  .category-list li {
    display: inline-block;
    margin-right: 20px;
  }
  /* ì´ë¯¸ì§€ ìŠ¤íƒ€ì¼ */
  img {
    display: block; /* ì´ë¯¸ì§€ë¥¼ ë¸”ë¡ ë ˆë²¨ë¡œ ì„¤ì • */
    width: 50%; /* ë˜ëŠ” ì›í•˜ëŠ” ë„ˆë¹„ */
    height: auto; /* ë¹„ìœ¨ ìœ ì§€ */
    margin-bottom: 5px; /* ì´ë¯¸ì§€ì™€ ìº¡ì…˜ ì‚¬ì´ì˜ ê°„ê²© */
    text-align: center;
  }

  /* ì´ë¯¸ì§€ ìº¡ì…˜ ìŠ¤íƒ€ì¼ë§ */
  .em {
    color: #757575;
    font-size: 0.8em;
    
    /* margin-top: 5px; */
    display: block; /* ìº¡ì…˜ì„ ë¸”ë¡ ìš”ì†Œë¡œ ì„¤ì •, ì´ë¯¸ì§€ ì•„ë˜ë¡œ ê°•ì œ ë°°ì¹˜ */
    text-align: center;
    margin-bottom: 5px;
  }
  /* Blockquote ìŠ¤íƒ€ì¼ */
  blockquote {
      margin-left: 20px;
      border-left: 1.5px solid #ccc; /* ì¢Œì¸¡ ì„  */
      border-top: 1.5px solid #ccc; /* ì¢Œì¸¡ ì„  */
      border-right: 1.5px solid #ccc; /* ì¢Œì¸¡ ì„  */
      border-bottom: 1.5px solid #ccc; /* ì¢Œì¸¡ ì„  */

      padding-left: 15px;
      color: #666; /* ê¸€ì ìƒ‰ìƒ */
  }



</style>

<article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">
  <!-- Table of Contents Container -->
<div class="toc-container">
    <nav class="toc">
      <strong>Table of Contents</strong>
      <!--  -->
      <!-- Jekyll TOC Liquid Code -->
      <ul><li><a href="#introduction-of-dimensionality-reduction">Introduction of â€˜Dimensionality Reductionâ€™</a><ul><li><a href="#dimensionality-reductionì˜-ëª©ì ">Dimensionality Reductionì˜ ëª©ì </a></li></ul></li><li><a href="#problems">Problems</a></li><li><a href="#feature-extractiondimensionality-reduction-1">Feature extraction/Dimensionality reduction 1</a><ul><li><a href="#dimensionality-reductionì˜-ëª©ì -revisited">Dimensionality Reductionì˜ ëª©ì  (Revisited)</a></li></ul></li><li><a href="#data-compression">Data Compression</a><ul><li><a href="#reduce-data-from-3d-to-2d">Reduce data from 3D to 2D</a></li></ul></li><li><a href="#feature-extractiondimensionality-reduction-2">Feature extraction/Dimensionality reduction 2</a><ul><li><a href="#basic-principle">Basic Principle</a></li></ul></li><li><a href="#principal-component-analysis-pca">Principal Component Analysis (PCA)</a><ul><li><a href="#calculation">Calculation</a></li><li><a href="#feature-extractiondimensionality-reduction-3">Feature extraction/Dimensionality reduction 3</a></li><li><a href="#covariance">Covariance</a></li><li><a href="#ì‹">ì‹</a></li><li><a href="#pca-example">PCA Example</a></li></ul></li><li><a href="#eigenface">Eigenface</a></li><li><a href="#sift-feature-visualization">SIFT feature visualization</a></li><li><a href="#pca-vs-lda">PCA vs. LDA</a><ul><li><a href="#pca-vs-lda-face-recognition-accuracy">PCA vs. LDA Face recognition accuracy</a></li><li><a href="#identification">Identification</a></li><li><a href="#verification">Verification</a></li></ul></li></ul>

      <!-- <ul><li><a href="#introduction-of-dimensionality-reduction">Introduction of â€˜Dimensionality Reductionâ€™</a><ul><li><a href="#dimensionality-reductionì˜-ëª©ì ">Dimensionality Reductionì˜ ëª©ì </a></li></ul></li><li><a href="#problems">Problems</a></li><li><a href="#feature-extractiondimensionality-reduction-1">Feature extraction/Dimensionality reduction 1</a><ul><li><a href="#dimensionality-reductionì˜-ëª©ì -revisited">Dimensionality Reductionì˜ ëª©ì  (Revisited)</a></li></ul></li><li><a href="#data-compression">Data Compression</a><ul><li><a href="#reduce-data-from-3d-to-2d">Reduce data from 3D to 2D</a></li></ul></li><li><a href="#feature-extractiondimensionality-reduction-2">Feature extraction/Dimensionality reduction 2</a><ul><li><a href="#basic-principle">Basic Principle</a></li></ul></li><li><a href="#principal-component-analysis-pca">Principal Component Analysis (PCA)</a><ul><li><a href="#calculation">Calculation</a></li><li><a href="#feature-extractiondimensionality-reduction-3">Feature extraction/Dimensionality reduction 3</a></li><li><a href="#covariance">Covariance</a></li><li><a href="#ì‹">ì‹</a></li><li><a href="#pca-example">PCA Example</a></li></ul></li><li><a href="#eigenface">Eigenface</a></li><li><a href="#sift-feature-visualization">SIFT feature visualization</a></li><li><a href="#pca-vs-lda">PCA vs. LDA</a><ul><li><a href="#pca-vs-lda-face-recognition-accuracy">PCA vs. LDA Face recognition accuracy</a></li><li><a href="#identification">Identification</a></li><li><a href="#verification">Verification</a></li></ul></li></ul> -->
    </nav>
</div>

<style>
    /* TOC ìŠ¤íƒ€ì¼ */
.toc-container {
  position: fixed;
  right: 10px;
  top: 100px;
  z-index: 1000;
  /* max-width: 50px; ê°€ë¡œ ë„ˆë¹„ ì¡°ì •, í•„ìš”ì— ë”°ë¼ ê°’ì„ ë³€ê²½í•˜ì„¸ìš” */
  font-size: 0.75rem;
}

.toc {
  border: 1px solid #ddd;
  padding: 10px;
  border-radius: 5px;
  background-color: white;
  box-shadow: 0 2px 5px rgba(0, 0, 0, 0.2);
  max-width: 200px; /* ê°€ë¡œ ë„ˆë¹„ ì¡°ì •, í•„ìš”ì— ë”°ë¼ ê°’ì„ ë³€ê²½í•˜ì„¸ìš” */
}


.toc strong {
  display: block;
  margin-bottom: 10px;
}

.toc ul {
  list-style: none;
  padding: 0;
  margin: 0;
}

.toc ul li a {
  text-decoration: none;
  color: #007bff;
  display: block;
  padding: 5px 0;
}

.toc ul li a:hover,
.toc ul li a.active { /* ì¶”ê°€ëœ active í´ë˜ìŠ¤ ìŠ¤íƒ€ì¼ */
  font-weight: bold;
  /* box-shadow: inset 0 -10px 0 #bbb7e8; */
  background-color: #bbb7e8;
}



  button {
    padding: 5px 10px;
    /* ìƒí•˜ 10px, ì¢Œìš° 20px íŒ¨ë”©ìœ¼ë¡œ í…ìŠ¤íŠ¸ ì£¼ë³€ ì—¬ìœ  ê³µê°„ ì¶”ê°€ */
    font-size: 0.87rem;
    /* ê¸€ì í¬ê¸° */
    color: white;
    /* ê¸€ì ìƒ‰ìƒ */
    background-color: #007bff;
    /* ë°°ê²½ ìƒ‰ìƒ */
    border: none;
    /* í…Œë‘ë¦¬ ì œê±° */
    border-radius: 5px;
    /* ëª¨ì„œë¦¬ ë‘¥ê¸€ê²Œ */
    cursor: pointer;
    /* ì»¤ì„œ ëª¨ì–‘ ë³€ê²½ */
    transition: background-color 0.3s;
    /* í˜¸ë²„ íš¨ê³¼ë¥¼ ìœ„í•œ ì „í™˜ ì„¤ì • */
    text-align: center;
    /* ê¸€ìë¥¼ ë²„íŠ¼ ì¤‘ì•™ì— ìœ„ì¹˜ */
    display: inline-block;
    /* ì¸ë¼ì¸ ë¸”ë¡ ìš”ì†Œë¡œ ì„¤ì •í•˜ì—¬ ìì—°ìŠ¤ëŸ½ê²Œ í…ìŠ¤íŠ¸ ì¤‘ì•™ ì •ë ¬ */
    line-height: normal;
    /* ê¸°ë³¸ ë¼ì¸ ë†’ì´ ì„¤ì • */
    vertical-align: middle;
    /* ìˆ˜ì§ ë°©í–¥ìœ¼ë¡œ ì¤‘ì•™ ì •ë ¬ */
    margin-top: 5px;
  }

  button:hover {
    background-color: #0056b3;
    /* í˜¸ë²„ ì‹œ ë°°ê²½ ìƒ‰ìƒ ë³€ê²½ */
  }


  /* reset base styles */
  * {
    box-sizing: border-box;
    margin: 0;
    padding: 0;
  }

  /* page header */
  header {
    margin-bottom: 2rem;
    padding-left: 6.5rem;
  }

  header h1 {
    font-size: 1.5rem;
  }

  header p {
    margin: .5rem 0;
    font-size: 1rem !important
  }

  main b {
    font-weight: 500
  }

  /* normal body content */
  h2 {
    font-size: 1.1rem;
    margin-bottom: 0.75rem;
    margin-left: 6.5rem;
    text-transform: uppercase;
  }

  h3 {
    border-bottom: 1px solid black;
    font-size: .9rem;
    margin: 1rem 0 .5rem 6.5rem
  }

  p {
    margin-bottom: 0.5rem;
  }

  a {
    color: inherit;
    /*#0000ee;*/
  }

  section {
    margin-bottom: 3rem;
  }

  /* misc */
  .pdf {
    font-size: .9rem !important;
    font-weight: 300;
    margin-left: 1.5rem;
    white-space: nowrap;
  }

  .pdf i {
    margin-right: .1rem;
  }

  .material {
    font-size: small;
    margin-left: .5rem;
  }

  :global(i) {
    padding-right: 4px !important
  }

  /* dated entries */
  .dated-entry {
    display: flex;
    flex-flow: row wrap;
    position: relative;
    margin-bottom: 1rem;
  }

  .dated-date {
    width: 6.5rem;
    text-align: right;
    padding-top: .15rem;
    padding-right: 1.5rem;
    font-size: .8rem
  }

  .dated-content {
    width: calc(100% - 6.5rem);
    font-size: .95rem
  }

  .oneline-entries {
    margin-bottom: 0.5rem;
  }

  .oneline-entries .dated-entry {
    margin-bottom: 0;
  }

  /* hide extra awards info for now, not sure what to include */
  #awards em {
    display: none;
  }

  .author-tooltip {
    font-weight: 400;
    font-size: .8rem !important;
    text-align: center;
  }

  /* on narrow displays, make the font smaller */
  @media (max-width: 480px) {
    html {
      font-size: 14px;
    }
  }

  /* when printing, make the font smaller and the page full-width */
  @media print {
    html {
      font-size: 12px;
    }

    main {
      margin-top: 0;
      max-width: 100%;
    }
  }

</style>

<script>
    document.addEventListener('scroll', function() {
      var sections = document.querySelectorAll('section'); // ì„¹ì…˜ ì„ íƒì ìˆ˜ì •
      var menu_links = document.querySelectorAll('.toc a'); // TOC ë§í¬ ì„ íƒì ìˆ˜ì •
    
      var fromTop = window.scrollY;
    
      sections.forEach(function(section) {
        if (section.offsetTop <= fromTop && section.offsetTop + section.offsetHeight > fromTop) {
          menu_links.forEach(function(link) {
            if (section.getAttribute('id') && link.getAttribute('href').includes(section.getAttribute('id'))) {
              link.classList.add('active');  // í˜„ì¬ ì„¹ì…˜ì˜ TOC ë§í¬ì— 'active' í´ë˜ìŠ¤ ì¶”ê°€
            } else {
              link.classList.remove('active'); // ë‹¤ë¥¸ ëª¨ë“  ë§í¬ì—ì„œ 'active' í´ë˜ìŠ¤ ì œê±°
            }
          });
        }
      });
    });
    </script>
    

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">13. Dimensionality Reduction (1)</h1>
    <p class="post-meta"><time class="dt-published" datetime="2022-06-07T00:00:00+02:00" itemprop="datePublished">
        Jun 7, 2022
      </time></p>
    <!-- íƒœê·¸ì™€ ì¹´í…Œê³ ë¦¬ í‘œì‹œ -->
    <div class="post-categories">
      
      <strong><span>ğŸ“š Categories:</span></strong>
      
      <a href="blog-categories-notes">Notes</a>
      
      
    </div>

    <div class="post-tags">
      
      <strong><span>ğŸ·ï¸ Tags:</span></strong>
      
      <a href="blog-tags-ml">ML</a>
      
      
    </div>


  </header>
  </header>

  <div class="post-content e-content" itemprop="articleBody">
    <h1 id="introduction-of-dimensionality-reduction">Introduction of â€˜Dimensionality Reductionâ€™</h1>

<ul>
  <li>Unsupervised learningì—ì„œ ì¤‘ìš”í•œ dimensionality reduction
    <ul>
      <li>PCA, LDA ë°©ì‹ìœ¼ë¡œ ìˆ˜í–‰ : eigen equationì„ ìƒì„±í•˜ì—¬ covariance ìƒì„±
        <ul>
          <li>Covarianceë¥¼ êµ¬í•˜ê³  data varì´ í° ë°©í–¥ìœ¼ë¡œ ë°©í–¥ vectorë¥¼ êµ¬í•¨ : eigenvector</li>
          <li>ë¶„ì‚° ëŸ‰ : eigenvalue</li>
        </ul>
      </li>
      <li>ë¶„ì‚° í° ìª½ìœ¼ë¡œ ì¢Œí‘œì¶•ì„ ì¡ì•„ í‘œí˜„í•˜ëŠ” ë°©ë²• â†’ dimension(data attribute) ì¤„ì´ëŠ” ë°©ë²• (ì°¨ì› ì¶•ì†Œ)</li>
    </ul>
  </li>
  <li>PCA Principal Component Analysis - label X
    <ul>
      <li>unsupervised learning</li>
    </ul>
  </li>
  <li>LDA Linear Discriminat Analysis - label O
    <ul>
      <li>supervised learning</li>
    </ul>
  </li>
  <li>Semisupervised learning : unsupervised learning (ë¶€ë¶„ì ìœ¼ë¡œ Supervised)
    <ul>
      <li>ë¬¼ë¡  Clusteringí•  ë•Œë„ ì¼ë¶€ dataì— labelì´ ì ìš©ë˜ì–´ ì•Œê³  ìˆì„ ìˆ˜ ìˆë‹¤ â†’ labelë¼ë¦¬ clusteringí•  ë•Œ ì •ë³´ë¥¼ í™œìš©í•˜ì—¬ groupí™” í• ìˆ˜ ìˆìŒ (semi-supervised = unsupervised + supervised)</li>
    </ul>
  </li>
</ul>

<h2 id="dimensionality-reductionì˜-ëª©ì ">Dimensionality Reductionì˜ ëª©ì </h2>

<ul>
  <li>1) Visualization ìš©ì´ : 3ì°¨ì› ì´í•˜ë¡œ Reductí•˜ë©´ visualize ìš©ì´í•´ì§</li>
  <li>2) Performace í–¥ìƒ : ë°ì´í„° ë‹¤ë£¨ê¸° ì‰¬ì›Œì ¸ performance (acc) í–¥ìƒ,</li>
  <li>3) computation cost ê°ì†Œ (time, computation ë³µì¡ë„, memory)</li>
</ul>

<p>PCA, LDA</p>

<h1 id="problems">Problems</h1>

<ul>
  <li>Imageì— ìˆëŠ” íŠ¹ì • Objectê²€ì¶œ / ë¶„ë¥˜ / ì˜ì—­ segmentation / caption / ì„ì˜ì˜anomaly ê²€ì¶œ / Etc
    <ul>
      <li>image detection : window ë¥¼ ì§€ì •í•˜ê³  objê°€ ìˆëŠ”ì§€ ì—†ëŠ”ì§€ O,Xë¡œ ê²€ìƒ‰</li>
      <li>image scaní•˜ë©° ë°˜ë³µ : ì „ì²´ Image í¬ê¸°ì˜ ì¼ì • ë¹„ìœ¨ë§Œí¼ ë˜ëŠ”ì§€ê¹Œì§€ ë°˜ë³µ - 1ì´ˆì— 10ì¥ ì´ìƒì˜ Image window, ì–¼êµ´ í•˜ë‚˜ë§Œ ê²€ì¶œí•´ë„ ê° windowì— ëŒ€í•´ ê³„ì† ë°˜ë³µí•´ì•¼ í•˜ë¯€ë¡œ</li>
      <li>computationally intensive â†’ ì—°ì‚°ëŸ‰ì„ ì¤„ì´ê¸° ìœ„í•œ dimensionality reduction</li>
    </ul>
  </li>
  <li>Object Detection: Many detection windows
    <ul>
      <li>Each window is very high dimensional data</li>
    </ul>

    <p>â†’ computationally intensive (ì§‘ì•½ì )</p>
  </li>
</ul>

<p><img src="/assets/img/2022-06-07-13.-Dimensionality-Reduction-(1).md/0.png" alt="0" /></p>

<ul>
  <li>
    <p>General framework</p>

    <p><img src="/assets/img/2022-06-07-13.-Dimensionality-Reduction-(1).md/1.png" alt="1" /></p>

    <ul>
      <li>ê³ ì°¨ì› Data</li>
      <li>Feature extractedëœ ê²ƒì€ ì¼ë°˜ì ìœ¼ë¡œ reductedëœ í˜•íƒœ</li>
      <li>Classifier / detection / segmentation</li>
      <li>input data ê°€ image / signal/ video/ audio/ textì´ê±´ General í•˜ê²Œ ì‚¬ìš©ê°€ëŠ¥</li>
    </ul>
  </li>
</ul>

<h1 id="feature-extractiondimensionality-reduction-1">Feature extraction/Dimensionality reduction 1</h1>

<h2 id="dimensionality-reductionì˜-ëª©ì -revisited">Dimensionality Reductionì˜ ëª©ì  (Revisited)</h2>

<ul>
  <li>1) Visualization ìš©ì´ : 3ì°¨ì› ì´í•˜ë¡œ Reductí•˜ë©´ visualize ìš©ì´í•´ì§</li>
  <li>2) Performace í–¥ìƒ : ë°ì´í„° ë‹¤ë£¨ê¸° ì‰¬ì›Œì ¸ performance (acc) í–¥ìƒ,</li>
  <li>3) computation cost ê°ì†Œ (time, computation ë³µì¡ë„, memory)</li>
  <li>It is impossible to process raw image data (pixels) directly
    <ul>
      <li>Too many of them (or data dimensionality too high)
        <ul>
          <li>dataì–‘ì´ ë„ˆë¬´ ë§ìŒ : million cellì„ ë„˜ì–´ì„  FHD, UHD ë“± Resolution ì´ìƒ ì¦ê°€</li>
          <li>e.g. 1M - 2M - 4M pixel ì¦ê°€í•˜ê¸°ì— ê·¸ëŒ€ë¡œ ì“¸ ìˆ˜ëŠ” ì—†ìŒ : ê·¸ëŒ€ë¡œ DNNì— ë„£ì–´ ì²˜ë¦¬í•˜ì§€ëŠ” ì•Šê³  500*500 =250Kì •ë„ë¡œ ì¤„ì—¬ì„œ ì‚¬ìš©í•¨</li>
        </ul>
      </li>
      <li>Curse of dimensionality problem (ì°¨ì›ì˜ ì €ì£¼)
        <ul>
          <li>Data dimensionì€ attributeë¡œì„œ 1d-2d-â€¦-nd (dataì˜ RGB, ê¸¸ì´, í¬ê¸° ë“±) ë§ìœ¼ë©´ ë§ì„ìˆ˜ë¡ ì¢‹ìŒ (ë” ë§ì€ ì •ë³´ë¥¼ ì·¨í•©í•´ì„œ ì²˜ë¦¬í•  ìˆ˜ ìˆìŒ)</li>
          <li>ê·¸ëŸ¬ë‚˜ ë„ˆë¬´ ë§ì•„ì§€ë©´ ì˜¤íˆë ¤ accuracyê°€ ë–¨ì–´ì§€ê²Œ ë˜ëŠ” í˜„ìƒ</li>
          <li>RGBì˜ ê°’ê³¼ ë¬´ê²Œ ë„“ì´ë¥¼ ì¸¡ì •í•˜ëŠ”ë° Measured value ë¼ëŠ” ê²ƒì€ observed valueë¡œì„œ Booleanê°’ì´ ì•„ë‹ˆê¸°ì— ì˜¤ì°¨ê°€ ìˆì„ ìˆ˜ ìˆìŒ.
            <ul>
              <li>(Booleanì´ ì•„ë‹Œ, ì˜¤ì°¨ê°€ í¬í•¨ëœ observed valueë¼ë©´ ê°’ì„ ë§ì´ ì¶”ê°€í•˜ë”ë¼ë„ ì˜¤íˆë ¤ accuracyê°€ ë–¨ì–´ì§€ê²Œ ë¨)</li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Process the raw pixel to produce a smaller set of numbers which will capture most information contained in the original data â€“ this is often called a feature vector
    <ul>
      <li>Raw data ì›ë³¸ dataë¥¼ ì²˜ë¦¬í•˜ì—¬ smaller setìœ¼ë¡œ ë§Œë“¬</li>
      <li>dì°¨ì›ì—ì„œ ì„ì˜ë¡œ 10ê°œ ë½‘ì•„ ì •ë¦¬í•˜ëŠ”ê²Œ ì•„ë‹Œ, ì›ë³¸ dataì˜ ìµœëŒ€í•œ ë§ì€ ì •ë³´ë¥¼ ë½‘ì„ ìˆ˜ ìˆë„ë¡ feature vector extract</li>
    </ul>
  </li>
  <li>Given data points in d-dimensions
    <ul>
      <li>Convert them to data points in $k(&lt;d)$ dimensions</li>
      <li>kì˜ ì•½ 10% ì •ë„ë¡œ dë¥¼ ë‹¤ë£¬ë‹¤</li>
      <li>With minimal loss of information</li>
    </ul>
  </li>
</ul>

<h1 id="data-compression">Data Compression</h1>

<p><img src="/assets/img/2022-06-07-13.-Dimensionality-Reduction-(1).md/2.png" alt="2" /></p>

<ul>
  <li>z_1 ìƒì— proejctioní•œ í˜•íƒœë¡œ reduct dimensionailty</li>
</ul>

<h2 id="reduce-data-from-3d-to-2d">Reduce data from 3D to 2D</h2>

<ul>
  <li>ì–´ë–¤ ë°©í–¥ìœ¼ë¡œ Dataë¥¼ íˆ¬ì˜í•˜ëŠëƒì— ë”°ë¼ ë¶„í¬ ëª¨ì–‘ì´ ë‹¬ë¼ì§</li>
  <li>ë„ˆë¬´ ë­‰ì³ì„œ íˆ¬ì˜ë˜ë©´ ì¢‹ì§€ ì•Šì€ reduction</li>
</ul>

<p><img src="/assets/img/2022-06-07-13.-Dimensionality-Reduction-(1).md/3.png" alt="3" /></p>

<p><img src="/assets/img/2022-06-07-13.-Dimensionality-Reduction-(1).md/4.png" alt="4" /></p>

<h1 id="feature-extractiondimensionality-reduction-2">Feature extraction/Dimensionality reduction 2</h1>

<ul>
  <li>Suppose we have a population measured on p random variables x1, â€¦, xd.
    <ul>
      <li>random variable</li>
    </ul>
  </li>
  <li>Note that these random variables represent the d-axes of the Cartesian coordinate system in which the population resides. Our goal is to develop a new set of k axes (linear combinations of the original d axes) in the directions of greatest variability: X2
    <ul>
      <li>cartesian coordinate system : ìƒˆë¡œìš´ axisë¥¼ ì°¾ì•„ë‚´ì„œ ë” ì˜ Representí•˜ë„ë¡ í‘œí˜„</li>
    </ul>
  </li>
</ul>

<p><img src="/assets/img/2022-06-07-13.-Dimensionality-Reduction-(1).md/5.png" alt="5" /></p>

<ul>
  <li>This is accomplished by rotating the axes ì›ë˜ì˜ ì¢Œí‘œì¶•ì„ rotateí•˜ëŠ” ë°©ë²•ìœ¼ë¡œ ì§„í–‰
    <ul>
      <li>Rotates multivariate dataset into a new configuration which is easier to interpret</li>
      <li>Purpose
        <ul>
          <li>simplify data (ì˜ ì••ì¶•í•˜ì—¬ ê°„ì†Œí™”)</li>
          <li>look at relationships between variables , patterns of units (Data ê°„ ê´€ê³„, íŒ¨í„´ ë¶„ì„ ê°€ëŠ¥)</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h2 id="basic-principle">Basic Principle</h2>

<ul>
  <li>From a raw data (vector) X of d-dimension to a new vector Y of k-dimensional (k &lt; &lt; d) via a transformation matrix A such that Y will capture most information in X</li>
</ul>

\[Y = AX\]

<p><img src="/assets/img/2022-06-07-13.-Dimensionality-Reduction-(1).md/6.png" alt="6" /></p>

<ul>
  <li>ì›ë˜ ë°ì´í„° Xì— ë³€í™˜ Matrix Aë¥¼ ê³±í•˜ì—¬ Yë¼ëŠ” ë³€í™˜ëœ matrix</li>
  <li>ë³€í™˜ëœ matrix : íšŒì „ëœ ì¢Œí‘œì¶•ì—ì„œì˜ data
    <ul>
      <li>â†’ X matrixë¥¼ raw matrixë¡œ ê°„ì£¼í•˜ê³  d dimension to k dimension matrix</li>
    </ul>
  </li>
</ul>

<h1 id="principal-component-analysis-pca">Principal Component Analysis (PCA)</h1>

<ul>
  <li>Goal: find k-dim projection that best preserves variance</li>
</ul>

<h2 id="calculation">Calculation</h2>

<h2 id="feature-extractiondimensionality-reduction-3">Feature extraction/Dimensionality reduction 3</h2>

<ul>
  <li>We can compress the data by only using the top few eigenvectors
    <ul>
      <li>Corresponds to choosing a â€œ<u>linear subspace</u>â€
        <ul>
          <li>ì¼ì • constantë¥¼ ê³±í•œ í˜•íƒœì´ë¯€ë¡œ linear</li>
        </ul>
      </li>
      <li>These eigenvectors are known as the <u>principal components</u></li>
    </ul>
  </li>
</ul>

<h2 id="covariance">Covariance</h2>

<ul>
  <li>ê° ì°¨ì› ë³„ Relation (corelation)</li>
  <li>Variance and Covariance:
    <ul>
      <li>Measure of the â€œspreadâ€ of a set of points around their center of mass (mean)</li>
      <li>ê° ì°¨ì›ì—ì„œì˜ dataê°€ ì–¼ë§ˆë‚˜ ë§ì´ ë¶„ì‚°ë˜ì–´ ìˆëŠëƒì— ëŒ€í•œ ì •ë³´ ì œê³µ</li>
    </ul>
  </li>
  <li>Variance
    <ul>
      <li>Measure of the deviation from the mean for points in <u>one dimension</u></li>
    </ul>
  </li>
  <li>
    <p>Covariance</p>

    <p><img src="/assets/img/2022-06-07-13.-Dimensionality-Reduction-(1).md/7.png" alt="7" /></p>

    <ul>
      <li>Measure of how much each of the dimensions vary from the mean with <u>respect to each other</u></li>
      <li>ì—¬ëŸ¬ ì°¨ì›ì— ê±¸ì³ ë¶„ì‚°ì„ ê³„ì‚°í•˜ë©´ -&gt; covarianceì •ë³´ ì–»ì„ ìˆ˜ ìˆìŒ</li>
      <li>Covariance is measured between two dimensions</li>
      <li>Covariance sees if there is a relation between two dimensions</li>
      <li>Covariance in one dimension is the variance</li>
    </ul>
  </li>
  <li>Positive: Both dimensions increase or decrease together
    <ul>
      <li>í•œ ì°¨ì›ì˜ ê°’ì´ ì¦ê°€í•˜ë©´ ë‹¤ë¥¸ ì°¨ì›ì˜ ê°’ë„ ì¦ê°€</li>
    </ul>
  </li>
</ul>

<p><img src="/assets/img/2022-06-07-13.-Dimensionality-Reduction-(1).md/8.png" alt="8" /></p>

<ul>
  <li>Negative: While one increase the other decrease
    <ul>
      <li>í•œ ì°¨ì›ì˜ ê°’ì´ ì¦ê°€í•˜ë©´ ë‹¤ë¥¸ ì°¨ì›ì˜ ê°’ì€ ë°˜ëŒ€ë¡œ ê°ì†Œ</li>
    </ul>
  </li>
</ul>

<p><img src="/assets/img/2022-06-07-13.-Dimensionality-Reduction-(1).md/9.png" alt="9" /></p>

<h2 id="ì‹">ì‹</h2>

\[\Sigma = \frac 1 N \Sigma_{i=1}^{N}{(x_i -\mu)(x_i -\mu)^T}\]

<ul>
  <li>$\Sigma v = \lambda v$
    <ul>
      <li>$\Sigma $ : Square Matrix</li>
      <li>$v $ : Eigenvector or characteristic vector (d)</li>
      <li>
        <p>$\lambda$ : Eigenvector or characteristic value (d*d)</p>

        <p><img src="/assets/img/2022-06-07-13.-Dimensionality-Reduction-(1).md/10.png" alt="10" /></p>
      </li>
    </ul>
  </li>
  <li>$\Sigma v = \lambda v$</li>
  <li>$(\Sigma -\lambda I) v=0$</li>
  <li>$\Sigma -\lambda I=0 \iff M = 0$</li>
  <li>
    <table>
      <tbody>
        <tr>
          <td>$</td>
          <td>\Sigma -\lambda I</td>
          <td>=0 \iff</td>
          <td>M</td>
          <td>= 0$ // Characteristic Equation</td>
        </tr>
      </tbody>
    </table>
  </li>
  <li>The zero vector cannot be an eigenvector</li>
  <li>The value zero can be an eigenvalue
    <ul>
      <li>Eigen solver - library ë³„ë¡œ Serving algorithmì´ ë‹¤ë¦„</li>
    </ul>
  </li>
  <li>ë‹¤ë¥¸ ì¢Œí‘œê³„ì˜ ê°’ìœ¼ë¡œ í•´ì„
    <ul>
      <li>ì°¨ì› ì¶•ì†Œë¡œ ì¸í•œ info lossë¥¼ ìµœì†Œí™”ì‹œí‚´</li>
      <li>ì›ë˜ ì¢Œí‘œê³„ì—ì„œ ì°¨ì› ì¶•ì†Œí•˜ë©´ info lossê°€ ë„ˆë¬´ í¬ê¸° ë•Œë¬¸ì— ë³€í™˜í•˜ì—¬</li>
    </ul>
  </li>
  <li>
    <p>From d original variables: $x_1, x_2, â€¦, x_d$</p>

    <p>x ë¼ëŠ” ì¢Œí‘œì¶•ì„ y ì¢Œí‘œì¶•ìœ¼ë¡œ ë³€í™˜ (y-axis: uncorrelated)</p>

    <ul>
      <li>Produce k new variables: $y_1, y_2, â€¦, y_k$ - Eigenvectorì˜ ê°œìˆ˜ë¥¼ kÂ«dê°œë¡œ í‘œí˜„í•  ìˆ˜ ìˆìŒ.</li>
      <li>yì¢Œí‘œê³„ì—ëŠ” data diensionê°„ correlationì´ ìµœì†Œí™”</li>
      <li>ë‹¤ë¥¸ ì¢Œí‘œì¶•ìœ¼ë¡œ ìƒˆë¡­ê²Œ ë³€í™˜í•˜ë©° dimension reduction
        <ul>
          <li>í‘œí˜„ ë°©ë²•ì€ ë‹¤ë¥´ì§€ë§Œ data ìì²´ëŠ” ë™ì¼í•˜ê²Œ d variables : y1~ y1d</li>
        </ul>
      </li>
    </ul>

    <p><img src="/assets/img/2022-06-07-13.-Dimensionality-Reduction-(1).md/11.png" alt="11" /></p>

    <p><img src="/assets/img/2022-06-07-13.-Dimensionality-Reduction-(1).md/12.png" alt="12" /></p>
  </li>
  <li>such that:
    <ul>
      <li>
        <p>$y_k$â€™s are uncorrelated (orthogonal)</p>

        <p>ìƒˆë¡œìš´ ì¢Œí‘œì¶• ykëŠ” ì„œë¡œ ìˆ˜ì§ì¸ ê´€ê³„ (uncorrelated, orthogonal)</p>
      </li>
      <li>$y_1$ explains as much as possible of original variance in data set
        <ul>
          <li>dataì˜ ê°€ì¥ í° Variance ë°©í–¥ìœ¼ë¡œ</li>
        </ul>
      </li>
      <li>$y_2$ explains as much as possible of remaining variance</li>
      <li>etc.</li>
    </ul>
  </li>
  <li>eigenvector / eigenvalue ëŒ€ì‘ â†’ characteristic equationìœ¼ë¡œ solve ê°€ëŠ¥
    <ul>
      <li>solverì— ë”°ë¼ solutionì´ ë‹¤ë¥´ê²Œ ë‚˜íƒ€ë‚˜ëŠ”ë° eigenvector valueê°€ ì˜ ì í•©í•˜ê²Œ êµ¬í•´ì¡ŒëŠ”ì§€ í™•ì¸ (unstableí•˜ê²Œ êµ¬í•´ì¡Œì„ ìˆ˜ê°€ ìˆìœ¼ë¯€ë¡œ)</li>
    </ul>
  </li>
  <li>$v_1 = \left[ \begin{matrix}v_{11} &amp; v_{12} &amp; â€¦&amp; v_{1d} \end{matrix} \right]^T$ is the <u>1st Eigenvector</u> of covariance matrix, and coefficients of $y_1$ ( $v_1$ is the first principal component)</li>
  <li>$v_2 = \left[ \begin{matrix}v_{21} &amp; v_{22} &amp; â€¦&amp; v_{2d} \end{matrix} \right]^T$is the <u>2nd Eigenvector</u> of covariance matrix, and coefficients of $y_2$ ( $v_2$ is the 2nd principal component)</li>
</ul>

<p>â€¦</p>

<ul>
  <li>$v_k = \left[ \begin{matrix}v_{k1} &amp; v_{k2} &amp; â€¦&amp; v_{kd} \end{matrix} \right]^T$is the kth Eigenvector of covariance matrix, and coefficients of $y_k$ ( $v_k$ is the kth principal component)</li>
</ul>

<p><img src="/assets/img/2022-06-07-13.-Dimensionality-Reduction-(1).md/13.png" alt="13" /></p>

<ul>
  <li>eigenvector, eigenvalueê°€ ë‚˜ì˜¤ê²Œ ë˜ê³  
2ê°œì˜ eigenvalueì˜ í¬ê¸° ìˆœì„œëŒ€ë¡œ eigenvectorì„ ë‚´ë¦¼ì°¨ìˆœìœ¼ë¡œ ì •ë ¬
    <ul>
      <li>â†’ ê°€ì¥ í¬ê²Œ varì´ ë˜ëŠ” ë°©í–¥ìœ¼ë¡œ ì •ë ¬í•˜ê²Œ ë¨</li>
    </ul>
  </li>
  <li>ìƒˆë¡œìš´ ì¢Œí‘œê³„ë¥¼ ë³´ë©´ ì œì¼ í° ë¶„ì‚° ë°©í–¥ìœ¼ë¡œ ë¶„ì‚°ì„ í‘œí˜„í•´ ë‚¸ ì¢Œí‘œì¶• v1, v2
    <ul>
      <li>ì²« ë²ˆì§¸ eigenvalueì— í•´ë‹¹í•˜ëŠ” ì¢Œí‘œì¶•ì„ ì¨ì„œ data í‘œí˜„í•´ë©´ â†’ info loss ìµœì†Œí™”</li>
      <li>
        <p>ë” ë¶„ë³„ë ¥ ì‡ëŠ” ë°©í–¥ìœ¼ë¡œ í‘œí˜„ë˜ë‚˜ data ìì²´ëŠ” ë™ì¼í•˜ë‹¤.</p>

        <p><img src="/assets/img/2022-06-07-13.-Dimensionality-Reduction-(1).md/14.png" alt="14" /></p>
      </li>
    </ul>
  </li>
  <li>ë” ë¶„ë³„ë ¥ ì‡ëŠ” ë°©í–¥ìœ¼ë¡œ í‘œí˜„ë˜ë‚˜ data ìì²´ëŠ” ë™ì¼ (x1, x2) â†’ (y1, y2)
    <ul>
      <li>ê·¸ ë³€í™˜ ê´€ê³„ëŠ” xì¢Œí‘œì™€ yì¢Œí‘œì˜ ì—°ê´€</li>
      <li>
        <p>dì°¨ì›ì„ kì°¨ì›ìœ¼ë¡œ ì¤„ì–´ë“¤ê²Œ ë¨ : ê°€ì¥ í° eigenvectorë¡œ í•´ì„œ ì¢Œí‘œì¶•ì„ ë³€í™˜í•´ë„ ì˜ dateë“¤ì´ í‘œí˜„ì´ ë¨ì„ í™•ì¸í•  ìˆ˜ ìˆë‹¤. (ìµœëŒ€ ë¶„ì‚° ë°©í–¥ì´ë¯€ë¡œ)</p>

        <p><img src="/assets/img/2022-06-07-13.-Dimensionality-Reduction-(1).md/15.png" alt="15" /></p>
      </li>
    </ul>
  </li>
</ul>

<h2 id="pca-example">PCA Example</h2>

<p><img src="/assets/img/2022-06-07-13.-Dimensionality-Reduction-(1).md/16.png" alt="16" /></p>

<p><img src="/assets/img/2022-06-07-13.-Dimensionality-Reduction-(1).md/17.png" alt="17" /></p>

<ul>
  <li>Calculate the covariance matrix
    <ul>
      <li>$cov = \left[ \begin{matrix}.616555556 &amp; .615444444 \ .615444444 &amp;.716555556\end{matrix} \right]$
        <ul>
          <li>covariance : $\sigma _{12} = \sigma _{21}$ì´ ê°™ì€ ê°’ì„ ê°€ì§€ëŠ” symmetric - &gt; correlationì´ positive :</li>
        </ul>
      </li>
      <li>since the non-diagonal elements in this covariance matrix are positive, we should expect that both the x and y variable increase together.
        <ol>
          <li>Zero mean dataë¥¼ ì´ìš©í•´ $XX^T$</li>
          <li>ê°ê°ì˜ dataì—ì„œ muë¥¼ ë¹¼ ì£¼ì–´ nìœ¼ë¡œ ë‚˜ëˆ„ì:  $\frac 1 n (x-\mu)(x-\mu)^T$</li>
        </ol>
        <ul>
          <li>$\frac 1 n$ìœ¼ë¡œ ë‚˜ëˆ„ì–´ì£¼ë©´ covariance</li>
          <li>ì•ˆ ë‚˜ëˆ„ì–´ì£¼ë©´ scatter matrix
            <ul>
              <li>: ë‚˜ëˆ„ì–´ì£¼ëƒ ë‚˜ëˆ„ì–´ì£¼ì§€ ì•ŠëŠëƒëŠ” ìƒìˆ˜ë¥¼ ê³±í•´ì£¼ëŠ” ê²ƒì´ê¸° ë•Œë¬¸ì— covë¡œ í•˜ë˜ scatterë¡œ í•˜ë˜ ë°©í–¥ê³¼ í¬ê¸°ë¥¼ ì´ì•¼ê¸°í•˜ëŠ” ê²ƒì´ê¸°ì— ê°™ê²Œ ë‚˜ì˜´.</li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
  <li>
    <p>Calculate the eigenvectors and eigenvalues of the covariance matrix</p>

    <p>characteristic equationì„ êµ¬í•´ lambdaë¥¼ êµ¬í•´ eigenvalue, vector</p>
  </li>
  <li>eigenvalues$= \left[ \begin{matrix}.0490833989 \ 1.28402771  \end{matrix} \right]$</li>
  <li>eigenvectors$= \left[ \begin{matrix}-.735178656 &amp;-.677873399 \ .677873399 &amp;-.735178656 \end{matrix} \right]$
    <ul>
      <li>í¬ê¸° ìˆœì„œëŒ€ë¡œ ì •ë ¬í•˜ê²Œ ë˜ë©´ v2 / v1</li>
      <li>(-) (-) ë¶ë™/ë‚¨ì„œë°©í–¥ : ì„œë¡œ ìˆ˜ì§ì¸ ê´€ê³„ë¡œ ì–»ì–´ì§</li>
    </ul>
  </li>
</ul>

<p><img src="/assets/img/2022-06-07-13.-Dimensionality-Reduction-(1).md/18.png" alt="18" /></p>

<ul>
  <li>eigenvectors are plotted as diagonal dotted lines on the plot.</li>
  <li>Note they are perpendicular to each other.</li>
  <li>Note one of the eigenvectors goes through the middle of the points, like drawing a line of best fit.</li>
  <li>The second eigenvector gives us the other, less important, pattern in the data, that all the points follow the main line, but are off to the side of the main line by some amount.</li>
  <li>
    <p>Reduce dimensionality and form feature vector</p>

    <p>ì°¨ì› ì¶•ì†Œëœ data : feature</p>

    <ul>
      <li>the eigenvector with the highest eigenvalue is the principle component of the data set.
        <ul>
          <li>ì›ë˜ data dimensionì„ dropí•œ í˜•íƒœê°€ ì•„ë‹Œ ì›ë˜ ê±°ë¥¼ ì˜ ì¡°í•©í•´ì„œ ë§Œë“  feature vector</li>
          <li>eigenvalueë¥¼ í¬ê¸° ìˆœëŒ€ë¡œ ëŒ€ì‘í•˜ì—¬ ì²« ë²ˆì§¸ componentë¡œ ê°„ì£¼í•œë‹¤.</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>In our example, the eigenvector with the largest eigenvalue was the one that goes through the middle of the data.</li>
  <li>Once eigenvectors are found from the covariance matrix, the next step is to order them by eigenvalue, highest to lowest. This gives you the components in order of significance.</li>
  <li>Eigen Feature Vector
    <ul>
      <li>
        <p>Feature Vector = (eig1 eig2 eig3 â€¦ eign)</p>

        <p>ëª¨ë“  eigenvectorë¥¼ ì‚¬ìš©í•˜ë©´ ì›ë˜ data ì‚¬ìš©</p>

        <ul>
          <li>dim reductionì€ ì—†ìœ¼ë‚˜ correlation ìµœì†Œë˜ë„ë¡ ì¢Œí‘œê°’ ìƒì„±</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>We can either form a feature vector with both of the eigenvectors:
    <ul>
      <li>eigenvectors$= \left[ \begin{matrix}-.677873399 &amp; -.735178656 \ -.735178656 &amp;.677873399  \end{matrix} \right]$</li>
    </ul>
  </li>
  <li>or, we can choose to leave out the smaller, less significant component and only have a single column:
    <ul>
      <li>less significant í•œ eigenvectorìˆœìœ¼ë¡œ dropì‹œí‚¤ë©´ ì°¨ì›ì˜ ìˆ˜ = data info loss ìµœì†Œí™” í•˜ëŠ” ë°©í–¥ìœ¼ë¡œ drop</li>
      <li>eigenvalues$= \left[ \begin{matrix}.677873399 \ - .735178656 \end{matrix} \right]$</li>
    </ul>
  </li>
  <li>Back to our example: Transform data to eigen-space $(xâ€™, yâ€™)$</li>
</ul>

<p><img src="/assets/img/2022-06-07-13.-Dimensionality-Reduction-(1).md/19.png" alt="19" /></p>

<p><img src="/assets/img/2022-06-07-13.-Dimensionality-Reduction-(1).md/20.png" alt="20" /></p>

<ul>
  <li>ì¢Œí‘œì¶•ì˜ ë°˜ì‹œê³„ë°©í–¥ íšŒì „ (dataì˜ ì‹œê³„ë°©í–¥ íšŒì „) ì´ë¼ê³ ë„ ë³¼ ìˆ˜ ìˆìŒ</li>
  <li>dataë¥¼ eigenvectorì— íˆ¬ì˜í•œë‹¤ëŠ” ê²ƒì€ :
    <ul>
      <li>Eigenvectorë¥¼ ì›ë˜ data pointì— ê³±í•˜ëŠ” ê²ƒì€
  ìƒˆë¡œìš´ ì¢Œí‘œê°’ì—ì„œì˜ ê°’ì´ ë‚˜ì˜¤ê²Œ ë¨.</li>
    </ul>
  </li>
</ul>

<h1 id="eigenface">Eigenface</h1>

<p><img src="/assets/img/2022-06-07-13.-Dimensionality-Reduction-(1).md/21.png" alt="21" /></p>

<ul>
  <li>data ë¶„ì„í•  ë•Œ : íŠ¹íˆ video ì²˜ë¦¬í•  ë•Œ image 100*100ë§Œ ë˜ë„ dimension 10000
    <ul>
      <li>â†’ ì²˜ë¦¬:</li>
      <li>image columnë°©í–¥ìœ¼ë¡œ ì´ì–´ë¶™ì—¬ì„œ column vectorë¥¾ ë§Œë“¤ì–´ ì²˜ë¦¬í–ˆëŠ”ë° ë„ˆë¬´ dimì´ ì»¸ìŒ
  PCAë¥¼ í†µí•´ dimension reductionì„ í•˜ë ¤ê³  í–ˆìŒ</li>
    </ul>
  </li>
  <li>When viewed as vectors of pixel values, face images are extremely high-dimensional
    <ul>
      <li>d = 100Â´100 image â†’ 10,000 dimensions</li>
      <li>Slow and lots of storage</li>
    </ul>
  </li>
  <li>However, very few of 10,000 dimensional vectors are valid face images</li>
  <li>We want to effectively model the subspace of face images</li>
  <li>$X_{new} = \Sigma_i w_i x_i$</li>
</ul>

<p><img src="/assets/img/2022-06-07-13.-Dimensionality-Reduction-(1).md/22.png" alt="22" /></p>

<ul>
  <li>Eigenspace ì–¼êµ´ ì´ë¯¸ì§€ë¥¼ ëª¨ì•„ì„œ í‰ê·  face - PCAí•  ë•Œ meanì„ ë¹¼ ì£¼ì–´ í‰ê·  ì–¼êµ´ì„ ë¹¼ ì£¼ì–´ ìˆ˜í–‰</li>
  <li>10000ì°¨ì› dataë¥¼ ì´ìš©í•´ covariance matë¥¼ êµ¬í•˜ê³  ì´ë¡œë¶€í„° eigenvector/valueë¥¼ êµ¬í•  ìˆ˜ ìˆìŒ
    <ul>
      <li>ê·¸ eigenvectorëŠ” 10000ì°¨ì›ì„ (d = 10000) / data ê°œìˆ˜ n = 100
        <ul>
          <li>
            <blockquote>
              <p>covariance matrixì˜ ì°¨ì›ì€ : 10000 * 10000</p>
            </blockquote>
          </li>
          <li>d*dë¡œ pcaí•˜ë©´ eigenvector dê°œ â†’ 10000ê°œ ë‚˜ì˜¤ê²Œ ë¨</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>eigenvector : 10,000ì°¨ì›ì˜ column vectorë¥¼ 100*100 ì´ë¯¸ì§€ë¡œ ë§Œë“¤ê²Œ ë˜ë©´ eigenvalueëª¨ì–‘ì´ ì–¼êµ´ ëª¨ì–‘ì„ ë‚˜íƒ€ë‚´ê²Œ ë¨.</li>
  <li>eigenvalueì™€ dataë¥¼ ê³±í•˜ê²Œ ë˜ë©´ ì´ë¥¼ feature vectorë¡œ ë‹¤ì„¯ ê°œë¡œ ì¤„ì–´ë“¦</li>
  <li>ì´ëŸ° ì‹ìœ¼ë¡œ feature vectorë¡œ ë‚˜ì˜¤ê²Œ ëœ ê²ƒì„ DT, bayesianì— ë„£ëŠ” ê²ƒë³´ë‹¤ë„ data ì „ì²´ë¥¼ ë‹¤ ë„£ì–´ì„œ NNì— ë„£ëŠ”ê²Œ ë” íš¨ìœ¨ì ì´ë”ë¼ :ë” ì¢‹ì€ featureë¥¼ ë½‘ì•„ classificationë„ ì˜ í•´ì¤Œ</li>
</ul>

<p><img src="/assets/img/2022-06-07-13.-Dimensionality-Reduction-(1).md/23.png" alt="23" /></p>

<ul>
  <li>$X_{\mu} = VY$</li>
  <li>$X = \mu + VY$</li>
</ul>

<p><img src="/assets/img/2022-06-07-13.-Dimensionality-Reduction-(1).md/24.png" alt="24" /></p>

<p><img src="/assets/img/2022-06-07-13.-Dimensionality-Reduction-(1).md/25.png" alt="25" /></p>

<ul>
  <li>(1) ì›ë˜ dataë¥¼ ì´ìš©í•˜ëŠ” ê²ƒ vs (2) eigenvectorë¥¼ ì´ìš©í•˜ëŠ” ê²ƒ ì¤‘ ì–´ëŠ ê²ƒì´ ë” íš¨ìœ¨ì ì¼ê¹Œ? (íš¨ê³¼ì ì¼ê¹Œ)</li>
</ul>

<p>â†’ eigenvectorê°€ ë” íš¨ìœ¨ì ì´ë‹¤.</p>

<p>ì´ìœ )</p>

<ul>
  <li>original data Xì—ëŠ” ë§ì€ duplicateë¥¼ í¬í•¨ë˜ì–´ ìˆë‹¤.</li>
</ul>

<p>ê·¸ëŸ¬ë‚˜</p>

<ul>
  <li>Eigenvectorì—ëŠ” ê·¸ duplicateê°€ ì œê±°ëœ correlationì´ ë°°ì œë¨</li>
  <li>ì‹¬ì§€ì–´ ì›ë˜ëŠ” i 100ê°œë¥¼ ë‹¤ ì¨ì•¼ í•˜ëŠ”ë° eigenvectorë¥¼ ì“°ë©´ 10000 iterationê¹Œì§€ë„ ê°ˆ ìˆ˜ ìˆìŒ/ data correlation ì œê±°í–ˆê¸° ë•Œë¬¸ì— ê³ ìœ ì„±ì„ ê°€ì§„ vectorë“¤ì€ ì ì€ dataë¥¼ ì‚¬ìš©í•´ë„ ì „ì²´ ë°ì´í„°ë¥¼ í‘œí˜„í•  ìˆ˜ ìˆë‹¤</li>
</ul>

<p>â†’ original data 100ê°œ ëŒë ¤ì„œ ì“°ëŠ” ê²ƒë³´ë‹¤ 10ê°œ í•´ì„œ ì“°ëŠ”ê²Œ ë” íš¨ê³¼ì ì¸ ê²°ê³¼ê°€ ë‚˜ì˜¬ ìˆ˜ ìˆë‹¤</p>

<h1 id="sift-feature-visualization">SIFT feature visualization</h1>

<p><img src="/assets/img/2022-06-07-13.-Dimensionality-Reduction-(1).md/26.png" alt="26" /></p>

<ul>
  <li>The top three principal components of SIFT descriptors from a set of images are computed</li>
  <li>data image pixelì„ ì¡°í•©í•˜ì—¬ imageí‘œí˜„í•˜ëŠ” ë°©ë²•</li>
  <li>ë“±ì¥ ë°°ê²½
    <ul>
      <li>ê³ ì°¨ì› data feature pixel(4k í•´ìƒë„ ë“±)ì„ ê°œë³„ì ìœ¼ë¡œ ì²˜ë¦¬í•˜ê¸°ì—ëŠ” ì–´ë ¤ì›€ì´ í¼</li>
      <li>â†’ dataë¥¼ 10*10ìœ¼ë¡œ ìª¼ê°œì„œ unit patternì˜ ì¡°í•© blockìœ¼ë¡œ SIFT Descriptorë¥¼ ë½‘ì•„ PCAë¥¼ ìˆ˜í–‰ì‹œí‚´ â†’ correlationì´ ë°°ì œëœ ìƒìœ„ ëª‡ ê°œì˜ eigenvector êµ¬í•´ì§</li>
    </ul>
  </li>
  <li>ìƒìœ„ nê°œì˜ eigenvectorë¥¼ ê°€ì§€ê³  ì›ë³¸ imageë¡œ mappingí•˜ê¸° ìœ„í•œ alphaê°’ì„ êµ¬í•  ìˆ˜ ìˆì„ ê²ƒ.
    <ul>
      <li>Map these principal components to the RGB space
        <ul>
          <li>$\Sigma_i \alpha_i v_i$</li>
          <li>alpha valueëŠ” 3ê°œë¡œ ë‚˜íƒ€ë‚¨ : ì–´ë–¤ imageì˜ patchì´ê±´ RGB</li>
        </ul>
      </li>
      <li>
        <blockquote>
          <p>imageì— ë¹„ìŠ·í•œ ê²½í–¥ì„ ë‚˜íƒ€ë‚˜ë©´ ë¹„ìŠ¤ë¬´ë¦¬í•œ ìƒ‰ìœ¼ë¡œ ë‚˜íƒ€ë‚˜ê²Œ ëœë‹¤.</p>
        </blockquote>
        <ul>
          <li>pixels with similar colors share similar structures 32</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<p>â†’ dataì— ì–´ë–¤ ë‚´ìš©ì´ í¬í•¨ ë˜ì–´ ìˆì„ì§€ì— ëŒ€í•œ ì—°êµ¬ë¥¼ ìˆ˜í–‰í•  ìˆ˜ ìˆë‹¤.</p>

<p><img src="/assets/img/2022-06-07-13.-Dimensionality-Reduction-(1).md/27.png" alt="27" /></p>

<ul>
  <li>ì„ì˜ì˜ ì°¨ì›ì˜ data blockì„ ì›í•˜ëŠ” ì°¨ì›ìœ¼ë¡œ ì¤„ì„
    <ul>
      <li>ì„ì˜ì˜ blockì— PCA ë¶„ì„ì„ ìˆ˜í–‰ì‹œí‚¤ê³  eigenvectorì— ì›í•˜ëŠ” ê°œìˆ˜ ë§Œí¼ ìˆ˜í–‰í•˜ë©´ ê·¸ alphaê°’ì„ í™œìš©í•˜ì—¬ dim reduction : unit patchì— ëŒ€í•œ pca ë¶„ì„</li>
    </ul>
  </li>
  <li>Divide the original 372x492 image into patches:</li>
  <li>Each patch is an instance that contains 12x12 pixels on a grid</li>
  <li>View each as a 144-D vector 33</li>
  <li>
    <p>PCA compression: 144-D â†’ 60 D</p>

    <p><img src="/assets/img/2022-06-07-13.-Dimensionality-Reduction-(1).md/28.png" alt="28" /></p>

    <ul>
      <li>144ê°œ eigenvector ëª¨ë‘ : ì›ë³¸ imageì™€ ë™ì¼í•œ quality
  60ê°œ eigenvector : data í•´ìƒë„ê°€ ë–¨ì–´ì§€ì§€ë§Œ dimì€ ì ˆë°˜ ì •ë„ë¡œ ì¤„ì–´ë“¬</li>
    </ul>
  </li>
  <li>
    <p>16 most important eigenvectors</p>

    <p><img src="/assets/img/2022-06-07-13.-Dimensionality-Reduction-(1).md/29.png" alt="29" /></p>
  </li>
  <li>
    <p>6 most important eigenvectors</p>

    <p><img src="/assets/img/2022-06-07-13.-Dimensionality-Reduction-(1).md/30.png" alt="30" /></p>
  </li>
  <li>
    <p>PCA compression: 144-D â†’ 3-D</p>

    <p><img src="/assets/img/2022-06-07-13.-Dimensionality-Reduction-(1).md/31.png" alt="31" /></p>
  </li>
  <li>
    <p>60 most important eigenvectors</p>

    <p><img src="/assets/img/2022-06-07-13.-Dimensionality-Reduction-(1).md/32.png" alt="32" /></p>
  </li>
  <li>blockì˜ ëŒ€ì†Œì— ë”°ë¥¸ compression ì •ë„ì˜ ì°¨ :
    <ul>
      <li>compression ì •ë„ - image quality ì‚¬ì´ì˜ tradeoff ì¡´ì¬</li>
    </ul>
  </li>
  <li>
    <p>ì¼ì • í¬ê¸°ì˜ patchì— ëŒ€í•´ pcaë¥¼ ë³´ê³  ìƒìœ„ nê°œì˜ eigenvector</p>

    <p>â†’ ì–´ë–¤ imageê±´ 12*12 pixelì€ 60ê°œì˜ ìˆ«ìë¡œ í‘œí˜„ë˜ëŠ” 60dim</p>

    <p>Compression ë§ì´ ë˜ë©´ ì˜ ë¶„ê°„í•  ìˆ˜ ì—†ì„ ì •ë„ê°€ ë  ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì ì ˆí•œ ì„ ì—ì„œ (1/10) ë§ˆë¬´ë¦¬í•˜ë©´ ê´œì°®ìŒ</p>
  </li>
</ul>

<h1 id="pca-vs-lda">PCA vs. LDA</h1>

<ul>
  <li>
    <p>Linear Discriminant Analysis(LDA) considers class information</p>

    <p>LDA</p>

    <ul>
      <li>data label ì´ìš©:</li>
    </ul>
  </li>
</ul>

<p><img src="/assets/img/2022-06-07-13.-Dimensionality-Reduction-(1).md/33.png" alt="33" /></p>

<p><img src="/assets/img/2022-06-07-13.-Dimensionality-Reduction-(1).md/34.png" alt="34" /></p>

<h2 id="pca-vs-lda-face-recognition-accuracy">PCA vs. LDA Face recognition accuracy</h2>

<ul>
  <li>
    <p>Database ìƒ Face ì¤‘ì—ì„œ ì–´ëŠ ê²ƒê³¼ ê°€ì¥ ê°™ì€ì§€ ë¹„êµ</p>

    <p><img src="/assets/img/2022-06-07-13.-Dimensionality-Reduction-(1).md/35.png" alt="35" /></p>

    <ul>
      <li>eigenvector ë„ˆë¬´ ì ê²Œí•˜ë©´ ì •ë³´ ì†ì‹¤ëœ ê²ƒì´ë‹ˆê¹Œ ì˜ ë¹„êµ ì•ˆë¨</li>
      <li>eigenvector(feature) ëŠ˜ì—¬ë³´ë©´ ì„±ëŠ¥ ì˜¬ë¼ê° / ë–¨ì–´ì§</li>
    </ul>
  </li>
  <li>PCA (eigenfaces): 80.0%</li>
  <li>LDA (fisherfaces): 93.2%</li>
</ul>

<p><img src="/assets/img/2022-06-07-13.-Dimensionality-Reduction-(1).md/36.png" alt="36" /></p>

<ul>
  <li>FERET database</li>
  <li>PCA : dim reduction
    <ul>
      <li>ì–¼êµ´ data ì´ìš©í•˜ì—¬ : row ë°©í–¥ì´ ì•„ë‹Œ column vector ëª¨ì–‘</li>
      <li>dataì— ëŒ€í•œ pca ë¶„ì„ ìˆ˜í–‰í•˜ë©´ eigenvectorê°€ ë‚˜ì˜¤ëŠ”ë°</li>
    </ul>
  </li>
  <li>y = V^T Xë¡œ yê°€ ë‚˜ì˜´
    <ul>
      <li>V eigenvector 10000ê°œ ì¤‘ k=100~200ë¡œ ì“°ë©´ 100~200 dimì˜ dataë¡œ ìˆ˜í–‰</li>
    </ul>
  </li>
  <li>ì‚¬ëŒì˜ ìˆ˜ = class ê°œìˆ˜ â†’ ìˆ˜ì²œë§Œ ê°œê°€ ë˜ë©´ classë³„ í•™ìŠµì´ ì–´ë ¤ì›€
    <ul>
      <li>ë³´í†µ ì–¼êµ´ì¸ì‹í•  ë•ŒëŠ” ì‚¬ëŒ ìˆ˜ê°€ ë˜ê²Œ ë§ë‹¤ (ìˆ˜ì‹­ ~ ìˆ˜ë°±ì´ë‚˜ í°ì—ëŠ” ë³´í†µ í•œ ëª…)</li>
      <li>ì¶œì… ì‹œìŠ¤í…œì— ë“±ë¡ëœ ì‚¬ëŒ : ìˆ˜ì‹­~ìˆ˜ë°±</li>
      <li>CCTV block system : ìˆ˜ì‹­ ëª…</li>
      <li>ìš´ì „ë©´í—ˆ, ì£¼ë¯¼ë“±ë¡ ì–¼êµ´ : ìˆ˜ì²œë§Œ ëª…</li>
      <li>ì–¼êµ´ ì¸ì‹í•  ë•Œ ë“±ë¡ë˜ëŠ” ì‚¬ëŒë“¤ ì¤‘ ì–¼êµ´</li>
    </ul>
  </li>
</ul>

<h2 id="identification">Identification</h2>

<ul>
  <li>one-to-n matching</li>
  <li>k nearest neighborë¥¼ ì‚¬ìš©í•˜ì—¬ l2 distanceê°€ ê°€ì¥ ê°€ê¹Œìš´ ê²ƒì„ ìƒˆë¡œìš´ ì–¼êµ´ì— ëŒ€ì‘ë˜ëŠ” idë¼ê³  ê°„ì£¼í•¨</li>
</ul>

<h2 id="verification">Verification</h2>

<ul>
  <li>one-to-one matching</li>
  <li>ì‚¬ìš©ìê°€ idë¥¼ ì´ì•¼ê¸°í•˜ê³  ë§ëŠ”ì§€ íŒë‹¨í•˜ëŠ” ê²½ìš°-í•´ë‹¹ idê°€ ë§ëŠëƒ í‹€ë¦¬ëŠëƒ</li>
  <li>Attributeê°€ ë§ì•„ì§€ëŠ”ë° ì„±ëŠ¥ ë–¨ì–´ì§€ë©´ curse of dimensionality / overfitting</li>
</ul>

  </div><a class="u-url" href="/13.-Dimensionality-Reduction-(1)" hidden></a>
</article>


      </div>
    </main><footer>
  <hr style="margin-left: 15px; color: #000 !important; background-color: #000 !important; opacity: 1 !important; width: 40px; height: .5px !important; margin-bottom: 2rem !important;">
  <div class="row m-0">
    <div class="col-12 col-md-3 col-sm-4 mb-4 mb-sm-0">
      <p>
        <a href="https://github.com/underthelights/underthelights.github.io" style="font-weight: 300">Kyuhwan Shim</a>
        <br>Up-to-date as of <span id="date">loading...</span></p>
      <i class="fas fa-location-dot mr-1"></i> Seoul is <img style="width: auto; height: 23px;" id="weather_icon">
      <span id="weather">Loading...</span>
    </div>
    <div class="col-12 col-md-9 col-sm-8">
      <p>
        <a style="font-weight: 300" href="https://linkedin.com/in/kyuhwan-shim"><i class="fab fa-linkedin" style="margin-right: 10.5px"></i>LinkedIn</a><br>
        <a style="font-weight: 300" href="https://fb.com/s.kyuhwn"><i class="fab fa-facebook" style="margin-right: 8px"></i>Facebook</a><br>
        <a style="font-weight: 300" href="https://www.instagram.com/s.kyuhwn"><i class="fab fa-instagram" style="margin-right: 10.5px"></i>Instagram</a><br>
        <a style="font-weight: 300" href="https://github.com/underthelights"><i class="fab fa-github" style="margin-right: 10.5px"></i>GitHub</a><br>
      </p>
    </div>
  </div>
</footer>

<script src="https://cdnjs.cloudflare.com/ajax/libs/moment.js/2.18.1/moment.min.js"></script>

<script src="/assets/js/weather.js"></script>
<script src="/assets/js/github_date.js"></script>
</body>

</html>
