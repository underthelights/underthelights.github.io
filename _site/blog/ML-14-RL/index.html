<h1 id="14-reinforcement-learning">14. Reinforcement Learning</h1>

<ul>
  <li>Sutton RL</li>
</ul>

<h1 id="characteristics-of-reinforcement-learning">Characteristics of Reinforcement Learning</h1>

<p><img src="14/Untitled.png" alt="Untitled" /></p>

<ul>
  <li>What makes reinforcement learning different from other machine learning paradigms?
    <ul>
      <li>supervised l. vs unsupervised l. vs. RL
        <ul>
          <li>supervised : label + data</li>
          <li>Unsupervised : just use given data</li>
          <li>RL : data + reward - Rewardì— í•´ë‹¹í•˜ëŠ” ì¶”ê°€ì ì¸ inputì´ ì¡´ì¬í•¨</li>
        </ul>
      </li>
    </ul>

    <p>â†’ There is no supervisor, only a reward signal</p>
  </li>
  <li>Feedback is delayed, not instantaneous</li>
  <li>Time really matters (sequential, non i.i.d. data)
    <ul>
      <li>ì‹œê°„ì´ ì¤‘ìš”í•œ ìš”ì†Œ ì¤‘ í•˜ë‚˜</li>
      <li>sequential : ì „ë°˜ì˜ ì„ íƒì´ í›„ë°˜ì˜ ì„ íƒì— ì˜í–¥
  iid = independent identically distributed - ìƒí˜¸ ì—°ê´€</li>
    </ul>
  </li>
  <li>Agentâ€™s actions affect the subsequent data it receives
    <ul>
      <li>agent actionì´ ì´í›„ dataì— ì˜í–¥ì„ ë¯¸ì¹œë‹¤.</li>
    </ul>
  </li>
</ul>

<h2 id="examples-of-reinforcement-learning">Examples of Reinforcement Learning</h2>

<ul>
  <li>Fly stunt manoeuvres in a helicopter
    <ul>
      <li>í—¬ë¦¬ì½¥í„°ì˜ ë¹„í–‰ ëª¨í˜•</li>
    </ul>
  </li>
  <li>Defeat the world champion at Backgammon
    <ul>
      <li>backgammon ê²Œì„ì—ì„œì˜ ì‘ìš©</li>
    </ul>
  </li>
  <li>Manageaninvestmentportfolio</li>
  <li>Controlapowerstation</li>
  <li>Makeahumanoidrobotwalk</li>
  <li>Play many different Atari games better than humans
    <ul>
      <li>ë¡œë´‡, íˆ¬ì í¬íŠ¸í´ë¦¬ì˜¤, ì•„íƒ€ë¦¬ ê²Œì„ì—ì„œì˜ í•™ìŠµ</li>
    </ul>

    <p><img src="14/Untitled_1.png" alt="Untitled" /></p>

    <p><img src="14/Untitled_2.png" alt="Untitled" /></p>
  </li>
</ul>

<h1 id="rewards">Rewards</h1>

<ul>
  <li>A reward ğ’• is a scalar feedback signal</li>
  <li>Indicate show well agent is doing at step t &amp; The agentâ€™s job is to maximize cumulative reward</li>
  <li>ê°ê°ì˜ ì‹œê°„ì— ì–¼ë§ˆë‚˜ ì˜ í–‰ë™ í–ˆëŠ”ì§€ ë³´ê³  reward ìµœëŒ€í™”ë˜ëŠ” ë°©í–¥ìœ¼ë¡œ í–‰ë™í•˜ë„ë¡ í•™ìŠµ</li>
  <li>Reinforcementlearning is based on the reward hypothesis
    <ul>
      <li>reward = ì‚¬ëŒì´ ë§Œë“  ê¸°ì¤€
  ex. Atari game : target ë³„ ìµœëŒ€í•œì˜ ì ìˆ˜ë¥¼ í•™ìŠµí•  ìˆ˜ ìˆë„ë¡ í•™ìŠµì´ ë˜ê¸°ë„ í•¨. ì ìˆ˜ê°€ ë§ì€ ìª½ì„ ë” ë¹¨ë¦¬ ì–»ì„ ìˆ˜ ìˆë„ë¡ í•™ìŠµì‹œí‚¤ëŠ” ì–‘ìƒì´ ìƒê¸¸ ìˆ˜ ìˆë‹¤,</li>
      <li>Reward hypothesis: all goals can be described by the m<strong>aximization of expected cumulative reward</strong></li>
    </ul>
  </li>
</ul>

<h2 id="examples-of-rewards">Examples of Rewards</h2>

<ul>
  <li>
    <p>Fly stunt manoeuvres in a helicopter</p>

    <p>(+) : ì›í•˜ëŠ” ê¶¤ì ì„ ê·¸ë¦¬ë©° ë‚ ì•„ê°ˆ ë•Œ
  (-) : crashing ì‹œ ë§ˆì´ë„ˆìŠ¤ ã…ê³¼</p>

    <ul>
      <li>+ve reward for following desired trajectory</li>
      <li>âˆ’ve reward for crashing</li>
    </ul>
  </li>
  <li>Defeat the world champion at Backgammon
    <ul>
      <li>+/âˆ’ve reward for winning/losing a game</li>
    </ul>
  </li>
  <li>
    <p>Manage an investment portfolio</p>

    <p>(+) : ì›í•˜ëŠ” ì´ìµ
  (-) : ì†ì‹¤</p>

    <ul>
      <li>+ve reward for each $ in bank</li>
    </ul>
  </li>
  <li>
    <p>Control a power station</p>

    <p>(+) : ì ì ˆí•œ ì „ë ¥ ê³µê¸‰
  (-) :</p>

    <ul>
      <li>+ve reward for producing power</li>
      <li>âˆ’ve reward for exceeding safety thresholds</li>
    </ul>
  </li>
  <li>
    <p>Make a humanoid robot walk</p>

    <p>(+) : ì£¼ì–´ì§„ í™˜ê²½ì—ì„œ target ë¬¼ì§ˆì„ í™•ë³´ì—ì„œ mission ì˜ ìˆ˜í–‰
  (-) : ë„˜ì–´ì§</p>

    <ul>
      <li>+ve reward for forward motion</li>
      <li>âˆ’ve reward for falling over</li>
    </ul>
  </li>
  <li>
    <p>Play many different Atari games better than humans</p>

    <p>(+) : ì ìˆ˜ ì–»ê±°ë‚˜
  (-) : ì ìˆ˜ ìƒê±°ë‚˜
  -&gt; ë¹ ë¥¸ ì‹œê°„ ì•ˆì— ì ìˆ˜ë¥¼ ë§ì´ ì–»ëŠ” ë°©í–¥ìœ¼ë¡œ</p>

    <ul>
      <li>+/âˆ’ve reward for increasing/decreasing score</li>
    </ul>
  </li>
</ul>

<h1 id="sequential-decision-making">Sequential Decision Making</h1>

<ul>
  <li>í˜„ì¬ì˜ actionì´ ë‹¤ìŒ í„´ actionì— ì˜í–¥ì„ ë¯¸ì¹˜ëŠ”ë°, ì˜¤ëœ turnì— ëŒ€í•´ ì˜í–¥ì„ ë¼ì¹ ìˆ˜ë„ ìˆìŒ.</li>
  <li>Goal: select actions to maximize total future reward
    <ul>
      <li>ì¼ë ¨ì˜ í–‰ë™ì— ë”°ë¥¸ rewardê°€ ìµœëŒ€ê°€ ë˜ë„ë¡ í•™ìŠµí•œë‹¤</li>
    </ul>
  </li>
  <li>Actions may have long term consequences
    <ul>
      <li>stateê°€ ìˆê³  actionì„ ì·¨í•´ì„œ s1-(a1)-&gt;s2-(a2)-&gt;s3</li>
    </ul>
  </li>
  <li>Reward may be delayed rewardëŠ” delayë¥¼ ìˆ˜ë°˜í•˜ì—¬ ì£¼ì–´ì§ˆ ìˆ˜ ìˆë‹¤</li>
  <li>í˜„ì¬ actionìœ¼ë¡œ ì¸í•œ rewardì— ë” ì¤‘ì ì„ ë‘˜ ê²ƒì¸ì§€, ë¯¸ë˜ì˜ rewardì— ì¤‘ì ì„ ë” ë‘˜ ê²ƒì¸ì§€ : user settingí•  ìˆ˜ë„ ìˆê³  í•™ìŠµ ë‹¨ê³„ì—ì„œ ì–´ë–»ê²Œ parameterë¥¼ ì„¤ì •í–ˆëŠ”ì§€ì— ë”°ë¼ / í•™ìŠµì´ ì˜ íš¨ê³¼ì ìœ¼ë¡œ ì´ë£¨ì–´ì§ˆìˆ˜ ìˆëŠ”ì§€ë¥¼ ê³ ë ¤í•˜ì—¬ ëª¨ìˆ˜ ì¡°ì •
    <ul>
      <li>(greedy) í˜„ì¬ rewardì— ì´ˆì ì„ ë§ì¶”ëŠ” ê²½ìš° - current reward</li>
      <li>(optimal) ì „ì²´ rewardì— ì´ˆì ì„ ë§ì¶”ëŠ” ê²½ìš° - total reward
        <ul>
          <li>Itmay be better to sacrifice immediate reward to gain more long-term reward (greedy optimal)</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>
    <p>Examples:</p>

    <p>Ex.
  -íˆ¬ì :ë‹¹ì¥ì€ ì†í•´ê°€ ë‚˜ë”ë¼ë„ ë¯¸ë˜ ì‹œì ì— ìˆ˜ìµ
  -í—¬ë¦¬ì½¥í„° ì£¼í–‰ ì¤‘ ì—°ë£Œ ì£¼ì… : crashí•˜ë©´ negative penaltyí•˜ê¸°ì— í˜„ì¬ë¡œì„œëŠ” reward ì¤„ì§€ë§Œ optimalí•˜ê²ŒëŠ” ëŠ˜ì–´ë‚˜ëŠ” reward
  -ì²´ìŠ¤ì—ì„œ ìƒëŒ€ë°© ì´ë™ : ë³¸ì¸ ì ìˆ˜ ì·¨í•˜ëŠ” ê²ƒë³´ë‹¤ ìƒëŒ€ë°© ë°©í•´ê°€ ì „ì²´ì ìœ¼ë¡œ ë” ì´ë“ì¼ìˆ˜ë„ ìˆëŠ” ê²½ìš°</p>

    <ul>
      <li>A financial investment (may take months to be mature)</li>
      <li>Refueling a helicopter (might prevent a crash in several hours)</li>
      <li>Blocking opponent moves (might help winning chances many moves from now)</li>
    </ul>
  </li>
</ul>

<h1 id="agent-and-environment">Agent and Environment</h1>

<p><img src="14/Untitled_3.png" alt="Untitled" /></p>

<ul>
  <li>At each stept the agent: agentê°€ ì£¼ë³€ì„ ê´€ì°°í•˜ê³ , rewardë¥¼ ë°›ì•„ actionì„ ì·¨í•¨
    <ul>
      <li>Executes action At</li>
      <li>Receives observation Ot</li>
      <li>Receives scalar reward Rt</li>
    </ul>
  </li>
</ul>

<p><img src="14/Untitled_4.png" alt="Untitled" /></p>

<ul>
  <li>The environment:
    <ul>
      <li>Receives action At</li>
      <li>Emits observation Ot+1</li>
      <li>Emits scalar reward Rt+1</li>
    </ul>
  </li>
  <li>t increments at env. step</li>
</ul>

<p>&lt;agent, environmentì˜ ìƒí˜¸ì‘ìš©&gt;
agentëŠ” actionì„ ì·¨í•˜ê³  stateì— ë”°ë¼ Rewardë¥¼ ë°›ê²Œ ë¨
envëŠ” actionì„ ë°›ì•„ë“¤ì—¬ì„œ agentì—ê²Œ ì£¼ê³  ë³€í™˜ëœ statementë¥¼ agentì—ê²Œ ì¤Œ</p>

<ul>
  <li>tíƒ€ì„ìœ¼ë¡œ ì´ë£¨ì–´ì§€ëŠ” ìš”ì†Œë“¤</li>
</ul>

<p>actionì— ëŒ€í•´ì„œ rewardì™€ statementì˜ ë³€í™”</p>

<p><img src="14%20Reinforcement%20Learning%20e7de39297aeb4f5dab69c0ae48410224/IMG_0328.jpg" alt="IMG_0328.jpg" /></p>

<h1 id="major-components-of-an-rl-agent">Major Components of an RL Agent</h1>

<ul>
  <li>An RL agent may include one or more of these components:
    <ul>
      <li>Policy: agentâ€™s behavior function í–‰ë™ ì •ì˜</li>
      <li>Value function: how good is each state and/or action ì–¼ë§ˆë‚˜ ì¢‹ì€ê°€</li>
      <li>Model: agentâ€™s representation of the environment  í•™ìŠµ ëª¨ë¸</li>
    </ul>
  </li>
</ul>

<h2 id="example---maze">Example - Maze</h2>

<p><img src="14/Untitled_5.png" alt="Untitled" /></p>

<ul>
  <li>Agent: explores environment and gets reward</li>
  <li>Environment: agent ëŒì•„ë‹¤ë‹ˆëŠ” í™˜ê²½ situation being explored by the agent</li>
  <li>States: ìœ„ì¹˜ - positions/locations in the environment</li>
  <li>Actions: ìƒí•˜ì¢Œìš° - allowed movements for the agent</li>
  <li>Reward: what agent gets as it moves</li>
</ul>

<p><img src="14/Untitled_6.png" alt="Untitled" /></p>

<ul>
  <li>
    <p>For example, bomb has reward -10, germ has reward 10, every other move has rewards -1</p>

    <p>â†’ ë¶ˆí•„ìš”í•œ ì´ë™ì„ ìµœì†Œí™”ì‹œí‚¤ê¸° ìœ„í•œ ì¥ì¹˜</p>
  </li>
</ul>

<p>s6 is blocked</p>

<p><img src="14/Untitled_7.png" alt="Untitled" /></p>

<p><img src="14/Untitled_8.png" alt="Untitled" /></p>

<h1 id="bellman-equation">Bellman equation</h1>

\[V(s) = max_a(R(s,a) + \gamma V(s'))\]

<ul>
  <li>$R(s,a)$Â : reward: stateì—ì„œ ì·¨í•œ actionì— ë”°ë¥¸ reward</li>
  <li>$V(s)$ : is the value function - value function:ì „ì²´ reward ë¥¼ ì–´ë–»ê²Œ í‘œí˜„í•  ê²ƒì¸ê°€</li>
  <li>$\gamma$ : is the discounting factor
    <ul>
      <li>í˜„ì¬-ë¯¸ë˜ rewardì¤‘ ì–´ëŠ ê²ƒì— ì´ˆì ì„ ë§ì¶œ ê²ƒì¸ì§€ ì¤‘ìš”ë„ ë§ì¶”ëŠ” ìƒìˆ˜</li>
    </ul>
  </li>
  <li>$sâ€™$ : is the next state agent can go from
    <ul>
      <li>s : í˜„ì¬ state, sâ€™ : next state</li>
    </ul>
  </li>
  <li>Bellman equation is used to calculate the value function 
â†’ ê° stateì— ëŒ€í•œ value functionê°’ìœ¼ë¡œ ì£¼ì–´ì§€ê²Œ ë¨ : í™˜ê²½ì´ ë°”ë€Œë©´ ê²°ê³¼ê°€ ë°”ë€Œê²Œ ë¨
R(s,a) : current reward
V(sâ€™) : all futer reward</li>
  <li>ì¼ë°˜ì ì¸ ê·œì¹™:í­íƒ„,ë³´ì„ì´ ìˆê³  envê°€ ë‹¬ë ¤ì ”ì„ ë•Œ í•™ìŠµì„ ë” ì˜ í• ê²ƒì¸ê°€-&gt;bellman eqë¡œ value fnìœ¼ë¡œ í•˜ëŠ”ê±°ëŠ” í™˜ê²½ ë°”ë€Œë©´ ë‹¤ì‹œ ì ìš©í•´ì•¼ í•¨</li>
</ul>

<p><img src="14/Untitled_9.png" alt="Untitled" /></p>

<ul>
  <li>T calculate V(1) ,consider a path s1- s2-s3-s7-s11-s12</li>
</ul>

<p>s1ì— ëŒ€í•´ ê°€ì¥ í° state functionì˜ ê²°ê³¼ë¥¼ ë§Œë“œëŠ” ê°’ì„ ì·¨í•˜ë„ë¡ í–ˆë‹¤.</p>

<ul>
  <li>(assume $\gamma = 1$)
    <ul>
      <li>V(1) = R(s1, â†’) + V(2) = -1+V(2)</li>
      <li>V(2) = R(s2, â†’) + V(3) = -1+V(3)</li>
      <li>V(3) = R(s3, $\downarrow$) + V(7) = -1+V(7)</li>
      <li>V(7) = R(s7, $\downarrow$) + V(11) = -1+V(11)</li>
      <li>V(11) = R(s11, â†’) + V(12) = -1+V(12)</li>
    </ul>
  </li>
  <li>Since V(12) = 10
    <ul>
      <li>We can get V(11)=9, V(7)=8, V(3)=7, V(2) = 6, V(1) = 5</li>
    </ul>
  </li>
  <li>We can consider other path, s1-s2-s3-s4- s3-s7-s11-s12 to calculate V(1), in which case V(1) will be less than 5 14</li>
</ul>

<p><img src="14/Untitled_10.png" alt="Untitled" /></p>

\[V(s) = max_a(R(s,a) + \gamma V(s'))\]

<ul>
  <li>By calculating V(s) for all states
    <ul>
      <li>Agent can move to the state with larger state value</li>
    </ul>
  </li>
  <li>ì„ì˜ì˜ ì¶œë°œì ì—ì„œ state function ì»¤ì§€ëŠ” ìª½ìœ¼ë¡œ actionì„ ì·¨í•˜ë©´ ëœë‹¤
â†’ equationì„ ì´ìš©í•´ì„œ value funcitionì„ êµ¬í•œí›„ ìµœì ì˜ pathë¥¼ êµ¬í•  ìˆ˜ ìˆë‹¤</li>
</ul>
