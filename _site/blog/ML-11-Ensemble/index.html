<h1 id="11-ensemble-learning">11. Ensemble Learning</h1>

<p>Property 1: Bishop 14</p>

<h1 id="ensemble-learning">Ensemble Learning</h1>

<p>ì§€ê¸ˆê¹Œì§€ ë°°ìš´ ì—¬ëŸ¬ê°œì˜ classifier ê°ê° : parameter ë³€í™”í•˜ì—¬ ì—¬ëŸ¬ê°€ì§€ model</p>

<p>í•œ ê°€ì§€ modelë³´ë‹¤ëŠ” ì—¬ëŸ¬ modelì„ í™œìš©í•˜ì—¬ prediction</p>

<h2 id="introduction">Introduction</h2>

<p>model ì—¬ëŸ¬ê°œë¥¼ ì¨ì„œ ì—¬ëŸ¬ê°œì˜ modelì„ ì¡°í•©í•˜ì—¬ ìµœì¢… modelì„ ê²°ì •</p>

<ul>
  <li>Ensemble learning is a process that uses a set of models, each of them obtained by applying a learning process to a given problem. This set of models (ensemble) is integrated in some way to obtain the final prediction</li>
  <li>Aggregation of multiple learned models with the goal of improving accuracy
    <ul>
      <li>ëª©í‘œ : ì •í™•ë„ ë†’ì´ê¸° / classification, regression acc, clustering accì—ì„œ ì¢‹ì€ ì„±ëŠ¥ ì–»ê¸°</li>
      <li>Intuition: simulate what we do when we combine an expert panel in a human decision-making process
        <ul>
          <li>ì‚¬ëŒë“¤ì´ ê²°ì • ë‚´ë¦´ ë•Œë„ í•œ ë‘ëª… ìƒê°ë³´ë‹¤ëŠ” ì „ë¬¸ ì§‘ë‹¨ panelì— ì˜í•´ì„œ ê²°ì • ë‚´ë¦¬ë©´ ì¢€ ë” í•©ë¦¬ì ì´ê³  ì¢€ ë” ì¢‹ì€ ê²°ì •ì„ ë‚´ë¦´ê±°ë¼ ìƒê°í•˜ëŠ” ê²ƒì²˜ëŸ¼ ê²°ì •</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h2 id="types-of-ensembles">Types of ensembles</h2>

<ul>
  <li>fusion
    <ul>
      <li>ë‹¤ë¥¸ versionì˜ dataset, algorithmì„ ê°–ê³  ì¢…í•©í•´ ìµœì¢… íŒë‹¨</li>
      <li>ë‘ ê°œì˜ ìš©ì–´ëŠ” ë°”ìŠ·í•œ ê°œë…ìœ¼ë¡œ ì‚¬ìš©ë˜ê³ ë„ ìˆê³  ì—„ë°€í•˜ê²Œ ë¶„ë¥˜í•˜ê¸° ì• ë§¤í•¨</li>
    </ul>
  </li>
  <li>ensemble
    <ul>
      <li>randomness : í•œ ìª½ì€ NN, SVM &lt;- 2 model, no randomness</li>
      <li>NN ì‚¬ìš© ì‹œ w randomly init : w ì´ˆê¸°ê°’ì´ ë‹¤ë¥¸ modelì„ ì—¬ëŸ¬ ê°€ì§€ ë§Œë“¤ë©´ randomnessë¡œ model ë§Œë“  ensemble</li>
    </ul>
  </li>
</ul>

<p>â†’ randomness ì¶”ê°€ëœ ë¶€ë¶„ : ensemble / Randomness excluded : Fusion (ì„ì–´ì„œ ì‚¬ìš©í•˜ê¸°ë„ í•¨)</p>

<hr />

<ul>
  <li>Ensemble methods are used for:
    <ul>
      <li>Classification
        <ul>
          <li>ê° modelë“¤ì´ ë¶„ë¥˜ ê²°ê³¼ë¥¼ ë§Œë“¤ì–´ë‚˜ë©´ ì´ë¥¼ ì¡°í•©í•˜ì—¬ ìµœì¢… (average)</li>
          <li>classification alg ë°”ê¾¸ê¸° or dataset ë³€í™”ë¥¼ ì£¼ì–´ ë‹¤ë¥¸ ëª¨ë¸ ë§Œë“¤ ìˆ˜ ìˆìŒ</li>
        </ul>
      </li>
      <li>Regression</li>
      <li>Clustering (also known as consensus clustering)
        <ul>
          <li>clustering : ì—¬ëŸ¬ versionì˜ clusteringì—ì„œ averageë¥¼ ì·¨í•˜ì—¬ ì–´ë–»ê²Œ avgí•  ê²ƒì¸ê°€ê°€ ì°¨ì´
            <ul>
              <li>groupì˜ categoryë¡œ ë¶„ë¥˜ : labelì´ í•™ìŠµí•  ë•Œ ì£¼ì–´ì§„ê²Œ ì•„ë‹˜
  â†’ 1ë²ˆ ê·¸ë£¹ ì•ˆ ì–´ë–¤ sampleë“¤ì´ ë™ì¼í•œ ê·¸ë£¹ ì•ˆì— ìˆë‹¤ëŠ” ê²Œ ì˜ë¯¸</li>
              <li>1ë²ˆ, 2ë²ˆ groupì€ ë‹¤ë¥¸ groupì„ì´ ì˜ë¯¸
  -&gt; clusteringì˜ ê²°ê³¼ê°€ ë‚˜íƒ€ë‚  ë•Œ ì–´ë–»ê²Œ í•©ì¹  ê²ƒì¸ê°€</li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Ensembles can also be classifed as :
    <ul>
      <li>Homogeneous: It uses only one induction algorithm
        <ul>
          <li>ë™ì¼í•œ ë¶„ë¥˜ algorithmì˜ ê²½ìš°ì—ë„ (induction algorithm = classifier)
            <ul>
              <li>Bayesian classifierì—ì„œë„ pdfë¡œ gaussian ì‚¬ìš© -&gt; classifierì˜ êµ¬ì¡°ë¥¼ ë°”ê¾¸ë©´ ë‹¤ë¥¸ êµ¬ì¡°ì˜ algorithm</li>
              <li>ì•„ì˜ˆ ë‹¤ë¥´ê²Œ ë°”ê¾¸ê²Œ ë˜ë©´ perceptron / svm/ decision tree ë“± DNN/ CNN ìì²´ë„ êµ¬ì¡°ëŠ” ì°¨ì´ê°€ ìˆìŒ</li>
            </ul>
          </li>
        </ul>
      </li>
      <li>Heterogeneous: It uses different induction algorithms
        <ul>
          <li>algorithmì˜ êµ¬ì¡°ì— ì°¨ì´ê°€ ìˆìœ¼ë©´ heterogeneous</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h2 id="some-comments">Some Comments</h2>

<ul>
  <li>
    <p>Combining models adds complexity</p>

    <ul>
      <li>
        <p>It is, in general, more difficult to characterize and explain predictions</p>

        <p>Modelì˜ ê°œìˆ˜ê°€ ëŠ˜ì–´ë‚˜ë‹ˆê¹Œ ì „ì²´ complexity ì¦ê°€ â†’ ì„¤ëª…í•˜ê¸°ë„ ì–´ë ¤ì›Œì§€ê³ , ì–´ë–¤ ê²°ê³¼ê°’ì´ ë‚˜ì˜¬ì§€, ê·¸ ê²°ê³¼ê°’ ì„¤ëª…í•˜ê¸°ë„ ì–´ë ¤ì›Œì§</p>
      </li>
      <li>
        <p>The accuracy may increase</p>

        <p>í•˜ì§€ë§Œ ì •í™•ë„ëŠ” ì¼ë°˜ì ìœ¼ë¡œ í–¥ìƒë¨</p>
      </li>
    </ul>
  </li>
  <li>
    <p>Violation of Ockhamâ€™s Razor: <strong>â€œSimplicity leads to better accuracyâ€</strong></p>
    <ul>
      <li>simple decision boundaryê°€ ì¼ë°˜ì ìœ¼ë¡œ ì¢‹ì€ ì„±ëŠ¥ì„ ë³´ì¸ë‹¤</li>
      <li>Identifying the best model requires identifying the proper â€œmodel complexityâ€
        <ul>
          <li>ì¢‹ì€ ì„±ëŠ¥ì„ ë‚´ëŠ” modelì„ ì°¾ìœ¼ë ¤ë©´ complexityë¥¼ ê³ ë¯¼í•´ì•¼ í•˜ëŠ”ë°</li>
          <li>occamì— ê±°ìŠ¤ë¥´ê¸´ í•˜ì§€ë§Œ, ì—¬ëŸ¬ model ì‚¬ìš©í•˜ê²Œ ë˜ë©´ ê°œë³„ modelì€ ë³µì¡í•˜ì§€ë§Œ ì¢…í•©í•œ modelì€ smoothí•œ ëª¨ì–‘ì¼ìˆ˜ë„ ìˆìŒ</li>
        </ul>

        <p>â†’ simpleí•˜ê²Œ ë§Œë“œëŠ” ê³¼ì •ì´ë¼ ë³¼ ìˆ˜ ìˆê³ , occamâ€™s razorì— ì í•©</p>

        <ul>
          <li>simple boundary = simple algì´ë¼ê³  ìƒê°í–ˆëŠ”ë°, complexityë¥¼ ì˜¬ë¦¼ìœ¼ë¡œ dbê°€ simpleí•´ì§ˆìˆ˜ë„ ìˆë‹¤!</li>
        </ul>
      </li>
      <li>Decision boundary may become simpler, eventually. E</li>
    </ul>
  </li>
</ul>

<h2 id="the-ensemble-learning-process">The Ensemble Learning Process</h2>

<p>dataê°€ ì£¼ì–´ì§€ë©´ ê·¸ì— ëŒ€í•´ í•¨ìˆ˜ë“¤ì„ ìƒì„± - ê°ê°ì´ ë¶„ë¥˜ í•¨ìˆ˜ë“¤ kê°œì˜ model ìƒì„±â†’ pruning</p>

<p>â†’ ìµœì¢… í•¨ìˆ˜ fk ë„ì¶œ</p>

<p><img src="11/Untitled.png" alt="Untitled" /></p>

<h1 id="methods-to-generate-ensembles">Methods to Generate Ensembles</h1>

<ul>
  <li>
    <p>Data Manipulation</p>

    <ol>
      <li>Train setì— ë³€í™”</li>
    </ol>

    <p>Supervised learningì—ì„œ train setì— ëŒ€í•´ train ë˜ì–´ ì–»ì–´ì§„ model</p>

    <ul>
      <li>ë‹¤ë¥¸ train setì„ ì‚¬ìš©í•˜ë©´ -&gt; ë‹¤ë¥¸ model : ë¶„ë¥˜ ì„±ëŠ¥ì´ ë‹¤ë¦„</li>
      <li>it changes the training set in order to obtain different models</li>
    </ul>
  </li>
  <li>
    <p>Modeling process manipulation</p>

    <ol>
      <li>algorihtmì— ë³€í™”</li>
    </ol>

    <ul>
      <li>model process manipulation : algorithmì˜ ë³€í™”</li>
      <li>parameterë§Œ ë³€í™”í•˜ëŠ” ê²½ìš°ë„ ìˆê³ , classifier ìì²´ë¥¼ ë³€ê²½í• ìˆ˜ë„ ìˆìŒ, algorithm ìì²´ë¥¼ ë³€í™”ì‹œí‚¬ìˆ˜ë„ ìˆê³ </li>
      <li>â†’ ë‹¤ì–‘í•œ model (f1 _ /// fk)</li>
      <li>it changes the induction algorithm, the parameter set or the model in order to obtain different models</li>
    </ul>
  </li>
</ul>

<p><img src="11/Untitled_1.png" alt="Untitled" /></p>

<h2 id="data-manipulation">Data manipulation</h2>

<p>Data, algorithmì— ëŒ€í•œ êµ¬ë¶„</p>

<ul>
  <li>data : dataì˜ srcë¡œë¶€í„° subset ì¶”ì¶œ</li>
</ul>

<p>Data src / sensor (visible, thermal, near infrared)</p>

<p>ë¹›ì˜ íŒŒì¥ì´ ê°€ì‹œê´‘ì„ , ì ì™¸ì„  ë“±ì— ë”°ë¼ ì˜ìƒì´ ë‹¤ì–‘íˆ ë‚˜íƒ€ë‚˜ëŠ”ë° ê°œë³„ì ì¸ srcë¡œ íŒë‹¨í•  ìˆ˜ ìˆê³  ì´ë¥¼ ì¡°í•©í•˜ëŠ” ê²ƒ ë˜í•œ ensembleì´ë¼ ë³¼ ìˆ˜ ìˆë‹¤</p>

<ul>
  <li>
    <p>ê·¸ëŸ° ì‹ì˜ ensembleì„ Fusionì´ë¼ê³  ë³¼ ìˆ˜ë„ ìˆë‹¤.</p>
  </li>
  <li>
    <p>Manipulating the input features</p>

    <p><img src="11/Untitled_2.png" alt="Untitled" /></p>

    <p>movie data features : rating, actor, genre
  -&gt; í¥í–‰í• ê²ƒì¸ì§€, ìˆ˜ìµì´ ì–´ëŠì •ë„ì¼ ê²ƒì¸ì§€, ê·¸ë£¹ì˜ ì‚¬ëŒë“¤ì´ ì¢‹ì•„í• ì§€</p>

    <ul>
      <li>ì„¸ featureë¥¼ ë‹¤ë¥´ê²Œ ì¡°í•©í•˜ì—¬ ê°ê°ì˜ ê²½ìš°ê°€ ëª¨ë‘ ë‹¤ë¥¸ classifier model</li>
    </ul>
  </li>
  <li>
    <p>Sub-sampling from the training set</p>

    <p><img src="11/Untitled_3.png" alt="Untitled" /></p>

    <p>dataì˜ ë¶„í•  : subsetë“¤ ì¡°í•©í•˜ì—¬ ìµœì¢… ê²°ê³¼</p>
  </li>
</ul>

<h2 id="modeling-process-manipulation">Modeling process manipulation</h2>

<ul>
  <li>Manipulating the parameter sets
    <ul>
      <li>hyperparameter : network layer, ì´ˆê¸°ê°’ì„ ì–´ë–»ê²Œ ì„¤ì •í• ì§€,
  node ê°œìˆ˜, activation fnì€ ì–´ë–»ê²Œ ì„¤ì •í• ì§€</li>
    </ul>

    <p><img src="11/Untitled_4.png" alt="Untitled" /></p>
  </li>
  <li>
    <p>Manipulating the induction algorithm</p>

    <p><img src="11/Untitled_5.png" alt="Untitled" /></p>
  </li>
</ul>

<h2 id="how-to-combine-models">How to Combine Models</h2>

<ul>
  <li>Algebraic methods : Scoreë¥¼ ì–´ë–»ê²Œ ì²˜ë¦¬í•  ê²ƒì¸ê°€
    <ul>
      <li>Average</li>
      <li>Weighted average</li>
      <li>Sum</li>
      <li>Weighted sum</li>
      <li>Product</li>
      <li>Maximum</li>
      <li>Minimum</li>
      <li>Median</li>
    </ul>
  </li>
  <li>Voting methods
    <ul>
      <li>Majority voting</li>
      <li>Weighted majority voting</li>
      <li>Borda count</li>
    </ul>
  </li>
</ul>

<p><img src="11/Untitled_6.png" alt="Untitled" /></p>

<p><img src="11/Untitled_7.png" alt="Untitled" /></p>

<h1 id="characteristics-of-the-base-models">Characteristics of the Base Models</h1>

<ul>
  <li>The Base classifiers should be as accurate as possible and having diverse errors, while each classifier provides some positive evidences
    <ul>
      <li>diverseí•œ errorê°€ ë‚˜íƒ€ë‚˜ì•¼ í•¨. (Alg1, alg2 â€¦ ê²°ê³¼ê°€ diverse)</li>
      <li>ì–´ëŠ ì •ë„ì˜ ì •í™•ë„ë¥¼ ê°€ì§€ë©´ì„œ ì •ë‹µì˜ ë‹¤ì–‘ì„±ì„ ê°€ì ¸ì•¼ classifyì˜ ì˜ë¯¸ê°€ ìˆë‹¤</li>
      <li>ì—¬ëŸ¬ versionì˜ classifierë¥¼ ensembleí•˜ì—¬ ì¢‹ì€ classifier</li>
    </ul>
  </li>
  <li>The average error of the base learners should be as small as possible</li>
  <li>The variance (of the predicted values) of the base learners should be as small as possible
    <ul>
      <li>Variance : when alg1 is trained</li>
    </ul>

    <p>Randomí•œ initial valueì— ì˜í•˜ì—¬ train ì—¬ëŸ¬ë²ˆí•˜ëŠ”ë°</p>

    <p>Variance ì •í™•ë„ê°€ ë§ì´ ë³€í™”í•œë‹¤ë©´ ì¢‹ì€ classifierê°€ ì•„ë‹ ê²ƒ</p>

    <p>diversity</p>

    <p>Stability : í•œ ì•Œê³ ë¦¬ì¦˜ì„ ë³¼ ë•Œ ë‹¤ì–‘í•˜ì§€ ì•Šì€ ê²°ê³¼</p>

    <p>bias : ansewrê³¼ì˜ ì°¨ì´</p>

    <p>Variance : ì–¼ë§ˆë‚˜ ì •ë‹µì´ ë‹¤ì–‘í•˜ê² ëŠ”ê°€</p>
  </li>
</ul>

<p><img src="11/Untitled_8.png" alt="Untitled" /></p>

<h2 id="popular-ensemble-methods">Popular Ensemble Methods</h2>

<p>Bagging:</p>

<ul>
  <li>
    <p>Averaging the prediction over a collection of predictors generated from <strong>bootstrap samples</strong> (both classification and regression)</p>

    <p>bootstrap sample :trian dataìˆìœ¼ë©´ subset sampling</p>

    <ul>
      <li>
        <p>ê°ê° samplingìœ¼ë¡œë¶€í„° classifier í•™ìŠµ</p>

        <p>Randomí•˜ê²Œ samplingí•˜ë©° ë‹¤ì–‘í•œ model</p>
      </li>
    </ul>
  </li>
</ul>

<p>Boosting:</p>

<ul>
  <li>
    <p>Weighted vote with a collection of classifiers that were trained sequentially from training sets given priority to instances <strong>wrongly classified</strong></p>

    <p>Boosting : ì—¬ëŸ¬ ë‹¨ê³„ë¥¼ ê±°ì³ classifier í•™ìŠµ</p>

    <ul>
      <li>ì´ì „ ë‹¨ê³„ì˜ classifierì˜ ì˜¤ë‹µì— ì´ˆì ì„ ë§ì¶˜ë‹¤.</li>
    </ul>

    <p>ì˜¤ë¥˜ê°€ ë‚˜ì˜¤ëŠ” dataë“¤ì„ ëª¨ì•„ ë‹¤ìŒ stageì—ì„œ ì´ˆì ì„ ë§ì¶”ì–´ í•™ìŠµí•˜ì—¬ ìœµí•©í•œë‹¤</p>
  </li>
</ul>

<p>RandomForest:</p>

<ul>
  <li>Averaging the prediction over a collection of trees constructed using a <strong>randomly selected subset of features</strong>
    <ul>
      <li>treeë¥¼ randomlyìƒì„±í•˜ì—¬ randomly selectí•´ì„œ ë§Œë“ ë‹¤.</li>
    </ul>
  </li>
</ul>

<p>Ensemble learning via negative correlation learning:</p>

<ul>
  <li>Generating sequentially new predictors <strong>negatively correlated</strong> with the existing ones
    <ul>
      <li>í˜„ì¬ classifierí•˜ê³  negative corelationê°–ëŠ” classifierë¥¼ í•™ìŠµí•˜ì—¬ ìœµí•©í•œë‹¤</li>
    </ul>
  </li>
</ul>

<p>Heterogeneousensembles:</p>

<ul>
  <li>Combining a set of <strong>heterogeneous predictors</strong>
    <ul>
      <li>NN + SVM + DT ë“± ìœµí•©</li>
    </ul>
  </li>
</ul>

<h1 id="bagging-bootstrap-aggregating">Bagging: Bootstrap AGGregatING</h1>

<p><img src="11/Untitled_9.png" alt="Untitled" /></p>

<ul>
  <li>
    <p>Analogy: Diagnosis based on multiple doctorsâ€™ majority vote</p>

    <p>ì—¬ëŸ¬ ëª…ì˜ ì˜ì‚¬ë“¤ì˜ ì§„ë‹¨ ê²°ê³¼ë¥¼ ìœµí•©í•˜ëŠ” ë°©ë²•</p>

    <p>Ex. Max score, average score ë“±</p>

    <ul>
      <li>ì—¬ëŸ¬ modelì„ ë§Œë“¤ê¸° ìœ„í•´ì„œ bootstrap sampling</li>
    </ul>
  </li>
  <li>
    <p>Training</p>
    <ul>
      <li>Given a set D of d tuples, at each iteration i, a training set $D_i$ of $d$ tuples is sampled with replacement from D (i.e. bootstrap)
        <ul>
          <li>bootstrap ë°©ë²• : sampling with replacement - ì „ì²´ datasetìœ¼ë¡œë¶€í„° samplingí•˜ì—¬ modelingí•˜ê³  ë‹¤ì‹œ ë³µì›</li>
          <li>ê°ê°ì˜ data subsetì— ëŒ€í•˜ì—¬ modelì„ ë§Œë“¬</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<p>average, Sum ì€ ê°™ì€ ë°©ì‹ : sumì—ì„œ classifier numberë§Œí¼ ë‚˜ëˆ ì£¼ë©´ average value</p>

<ul>
  <li>bootstrapping = original dataë¡œë¶€í„° sampling</li>
  <li>aggregating = ê·¸ê²ƒë“¤ë¡œë¶€í„° ê°ê°ì˜ classifierë¥¼ ë§Œë“¤ì–´ ë³‘í•©í•˜ëŠ” ë°©ë²•</li>
  <li>Classification: classify an unknown sample X
    <ul>
      <li>Each classifier $M_i$ returns its class prediction</li>
      <li>The bagged classifier $M^*$ counts the votes and assigns the class with the most votes to X
        <ul>
          <li>ê° classifierê°€ sampleì— ëŒ€í•œ class ì˜ˆì¸¡ê°’ì„ ê³„ì‚°í•˜ê³ , ê·¸ë¦¬ê³  ìµœì¢… íŒë‹¨ì€ voting / sum/ ë“± ì—¬ëŸ¬ ë°©ë²•ì„ ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤.</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Prediction:
    <ul>
      <li>can be applied to the prediction of continuous values by taking the average value of each prediction for a given test tuple</li>
    </ul>
  </li>
  <li>Accuracy
    <ul>
      <li>
        <p>Often significantly better than a single classifier derived from</p>

        <p>significantly better : 5 ~ 10% ìƒìŠ¹</p>

        <ul>
          <li>ë¬¼ë¡  modelì„ ì—¬ëŸ¬ë²ˆ ì“°ê³  ì—°ì‚°ëŸ‰ì€ ê·¸ë§Œí¼ ì¦ê°€</li>
          <li>test stage : Test sampleì—ì„œëŠ” modelë“¤ ë‹¤ ìœ ì§€í•´ì„œ ê·¸ë§Œí¼ ë¶„ë¥˜ ì‘ì—… ìˆ˜í–‰ í›„ ìœµí•©</li>
        </ul>

        <p>Train + test stage ì—°ì‚°ëŸ‰ ì¦ê°€</p>
      </li>
      <li>For noisy data: not considerably worse, more robust
        <ul>
          <li>noisy data : robustí•˜ê²Œ ë¨ (boost sampling : Noisy dataê°€ ë¹ ì§„ í˜•íƒœë¡œ í•™ìŠµ)</li>
        </ul>
      </li>
      <li>Proved improved accuracy in prediction</li>
    </ul>
  </li>
  <li>Requirement: need unstable classifier types
    <ul>
      <li>Unstablemeansasmallchangetothetrainingdatamayleadtomajor decision changes</li>
      <li>requirement : unstable classifier</li>
    </ul>

    <p>Unstable : train dataë¥¼ ì¡°ê¸ˆ ë°”ê¿€ ê²½ìš° model decisionì´ í¬ê²Œ ë°”ë€ŒëŠ” modelì„ ì˜ë¯¸í•¨</p>

    <p>(Remind) Modelì´ ë°”ë€ train dataì— ëŒ€í•´ì„œ diverseí•œ error = varianceê°€ ì»¤ì•¼ í•œë‹¤.</p>

    <ul>
      <li>baggingì˜ ê´€ì ì—ì„œëŠ” var í°ê²Œ ì¢‹ë‹¤ (ì¼ë°˜ì ìœ¼ë¡œëŠ” ë³„ë¡œ ì•ˆ ì¢‹ë‹¤)</li>
    </ul>
  </li>
  <li>Stability in Training
    <ul>
      <li>Training: construct classifier from</li>
      <li>
        <p>Stability: small changes on results in small changes on</p>

        <p>Training : fë¥¼ dë¡œë¶€í„° í˜•ì„±</p>
      </li>
      <li>Decision trees are a typical unstable classifier</li>
    </ul>
  </li>
</ul>

<p><img src="11/Untitled_10.png" alt="Untitled" /></p>

<p><img src="11/Untitled_11.png" alt="Untitled" /></p>

<h1 id="boosting">Boosting</h1>

<ul>
  <li>
    <p>Analogy: Consult several doctors, when there are disagreements, we focus more attention on that case</p>

    <p>ì˜ì‚¬ë“¤ì˜ ì˜ê²¬ì´ ê°ˆë¦´ ë•Œ í•©ì¹˜ë˜ì§€ ì•ŠëŠ” ì˜ê²¬ë“¤ì— ëŒ€í•´ ë” ì£¼ì˜ë¥¼ ê¸°ìš¸ì¸ë‹¤.</p>
  </li>
  <li>Incrementally create models selectively using training examples based on some distribution.
    <ul>
      <li>Incrementallyí•˜ê²Œ sampleì´ subsetìœ¼ë¡œ selectionëœ í™•ë¥ ê°’ì„ ê°€ì§€ê³  ìˆìŒ</li>
    </ul>
  </li>
  <li>How boosting works?
    <ul>
      <li>Weights are assigned to each training example</li>
      <li>A series of k classifiers is iteratively learned</li>
      <li>After a classifier $M_i$  is learned, the weights are updated to allow the subsequent classifier, $M_i +1$, to pay more attention to the training examples that were misclassified by</li>
      <li>The final $M^*$ combines the votes of each individual classifier, where the weight of each classifierâ€™s vote is a function of its accuracy ğ’Š</li>
    </ul>
  </li>
  <li>ê° sampleë“¤ì´ weightë¥¼ ê°€ì§€ê³  ìˆìŒ.</li>
</ul>

<p>ê·¸ë¦¬ê³  ìš°ë¦¬ëŠ” kê°œì˜ classifierë¥¼ í•™ìŠµí•  ê²ƒ</p>

<p>ê·¸ëŸ°ë° M_iê°€ í•™ìŠµ ëœ ë‹¤ìŒ, classifierê°€ í•™ìŠµëœ ì´í›„ì—ëŠ”</p>

<p>weightì„ updateí•˜ëŠ”ë° ì• ë‹¨ê³„ì—ì„œ í•™ìŠµëœ modelë“¤ì´ misclassified ì— ë” ì£¼ì˜ë¥¼ ê¸°ìš¸ì¸ë‹¤ (weightë¥¼ ì˜¬ë¦°ë‹¤)</p>

<ul>
  <li>
    <blockquote>
      <p>higher chance to be selected</p>
    </blockquote>
  </li>
</ul>

<p>ì¦‰ misclassified sampleë“¤ì´ ì ì  ê·¸ ìª½ìœ¼ë¡œ selectë˜ë©´ì„œ hard sampleë“¤ì´ ì ì  ì¶”ê°€ë˜ì–´ ë’¤ìª½ classifier í•™ìŠµ</p>

<p>1~kê°œ classifierë¥¼ ì¡°í•©í•˜ì—¬ m*</p>

<ul>
  <li>weighted combination : weightëŠ” accuracyì— ë¹„ë¡€</li>
  <li>boosting ê¸°ë³¸ ì•„ì´ë””ì–´ : disagreement, hard sample</li>
</ul>

<p>Hard sampleì— ì´ˆì  ë§ì¶”ëŠ” ë°©ë²• : classifier 1ë²ˆì„ ë§Œë“¤ê³  misclassifiedì— ëŒ€í•´ì„œ classifier 2ë²ˆì„ ë§Œë“¤ê³  m1, m2ê°€ ë‹¤ë¥¸ ê²°ì •ì„ ë‚´ë¦¬ëŠ” sampleì— ëŒ€í•´ì„œ classifier 3ë²ˆì„ ë§Œë“¤ì–´ test sampleì´ ë“¤ì–´ì˜¤ë©´ m1, m2ë¥¼ ëŒë ¤ ìµœì¢… ê²°ê³¼ë¡œ ì‚¬ìš©í•˜ê³  ë‘ ë¶„ë¥˜ê¸° ê²°ê³¼ê°€ ë‹¤ë¥´ë©´ m3ë¥¼ í™œìš©í•˜ì—¬ ê²°ê³¼ ë„ì¶œ</p>

<p>(dataê°€ ë‹¤ì‹œ samplingë  í™•ë¥ ê°’ ì¡°ì •í•˜ëŠ” ë°©ì‹ : adaboosting)</p>

<ul>
  <li>M_i : weak classifier</li>
</ul>

<h2 id="adaboost">Adaboost</h2>

<ul>
  <li>Using Different Data Distribution
    <ul>
      <li>Start with uniform weighting</li>
      <li>misclassified sampleì˜ weight ì¦ê°€</li>
      <li>well classified sampleì— ëŒ€í•´ì„œëŠ” weight ê°ì†Œ</li>
      <li>During each step of learning
        <ul>
          <li>Increase weights of the examples which are not correctly learned by the weak learner</li>
          <li>Decrease weights of the examples which are correctly learned by the weak learner</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Idea
    <ul>
      <li>Focus on difficult examples which are not correctly classified in the previous steps</li>
      <li>difficult exampleì— ë” ì£¼ì˜ë¥¼ ê¸°ìš¸ì¸ ì¼€ì´ìŠ¤</li>
    </ul>
  </li>
  <li>Weighted Voting
    <ul>
      <li>Construct strong classifier by weighted voting of the weak classifiers</li>
      <li></li>
      <li>strong classifier ë§Œë“¤ ë•Œ weak classifier ì— weightë¥¼ ì£¼ê³  weighted voting / weighted sum ë“± ì¼ë°˜ì ì¸ ensemble ë°©ë²• ì ìš©</li>
      <li>weak classifierë¥¼ ë§ì´ ì²¨ê°€í•˜ì—¬ combined classifierì˜ accuracy ì¦ê°€ (strong classifier/learner)</li>
    </ul>
  </li>
  <li>Idea
    <ul>
      <li>Better weak classifier gets a larger weight</li>
      <li>Iteratively add weak classifiers
        <ul>
          <li>Increase accuracy of the combined classifier through minimization of a cost function Ensemble Learning Adaboost Introduction to Machine Learning Page 17</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<p><img src="11/Untitled_12.png" alt="Untitled" /></p>

<ul>
  <li>Differences with Bagging:baggingê³¼ì˜ ì°¨ì´ì 
    <ul>
      <li>Models are built sequentially on modified versions of the data
        <ul>
          <li>randomí•˜ê²Œ sampleëœê²Œ ì•„ë‹ˆë¼ weightì— ì˜í•´ sampleëœ dataì— ì˜í•´ í•™ìŠµ</li>
        </ul>
      </li>
      <li>The predictions of the models are combined through a weighted sum/vote
        <ul>
          <li>ì ì  hard sampleì— ëŒ€í•´ í•™ìŠµë˜ë‹ˆ easy sample / hard sampleì˜ classifierê°€ ë™ì¼í•œ weightë¥¼ ê°€ì§ˆ ìˆ˜ ì—†ìŒ : baggingì€ ë™ì¼í•œ ì¡°ê±´ìœ¼ë¡œ randomly sampling (no weight)
            <ul>
              <li>ê±°ì˜ ë™ë“±í•œ ì¡°ê±´ì´ê¸° ë•Œë¬¸ì— weightë¥¼ ì£¼ì§€ ì•ŠìŒ</li>
              <li>boostingì˜ ê²½ìš° misclassifiedì— ëŒ€í•´ overfitting (hard sampleì´ ì¦ê°€í•˜ëŠ” ë°©í–¥ìœ¼ë¡œ weight update)-&gt; ensembleí•˜ë©´ ì ì  hard sample ì¶”ê°€ë˜ë©° overfitting ìœ„í—˜</li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Boosting algorithm can be extended for numeric prediction
    <ul>
      <li>Comparing with bagging: Boosting tends to achieve greater accuracy, but it also risks overfitting the model to misclassified data</li>
    </ul>
  </li>
  <li>
    <p>The diagram should be interpreted with the understanding that the algorithm is sequential: classifier $C_k$ is created before classifier $C_{k+1}$, which in turn requires that $\beta_k$ and the current distribution $D_k$ be available</p>

    <p><img src="11/Untitled_13.png" alt="Untitled" /></p>

    <ul>
      <li>sequential = ì•ì„œ misclassified sampleì„ êµ¬í•´ì•¼ ì´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì¶”ê°€ í•™ìŠµì„ ì§„í–‰í•˜ê¸°ì—
  ckëŠ” ck+1ë³´ë‹¤ í•™ìŠµì´ ì‚¬ì „ì— ì´ë£¨ì–´ì ¸ì•¼ í•˜ë©°
  Beta k : 4th classiferì˜ error / í˜„ì¬ data distributionì„ ì•Œì•„ì•¼ ë‹¤ìŒ ë‹¨ê³„ classiferë¥¼ í•™ìŠµì‹œí‚¬ ìˆ˜ ìˆìŒ</li>
    </ul>
  </li>
</ul>

<h3 id="comments">Comments</h3>

<p>ì´ëŸ°ì‹ìœ¼ë¡œ sample distributionì— ì˜í•´ data updateí•˜ì—¬ í•™ìŠµí•˜ë©´
ì• ìª½ ë‹¨ê³„ misclassified dataë“¤ì´ current stageì— í•™ìŠµ</p>

<ul>
  <li>hard sampleì— í¸ì¤‘í•˜ì—¬ í•™ìŠµì´ ì¼ì–´ë‚¨ (undemocratic voting scheme)
Weighted majority voting : ensemble ëŒ€ìƒ ë‹¨ìœ„ì˜ classifierë“¤ì´ ì„±ëŠ¥ ìƒ í° ì°¨ì´ ì¡´ì¬ê°€ ìˆì–´ weightë¥¼ ì¤„ ìˆ˜ë°–ì— ì—†ê³  ì„±ëŠ¥ ì¢‹ì€ classifierì— ëŒ€í•´ ë” ë†’ì€ weightë¥¼ ì£¼ëŠ” ê²Œ ë” ìì—°ìŠ¤ëŸ¬ìš¸ ìˆ˜ ìˆê³ , ì´ëŸ°ê²Œ democraticí•˜ì§€ëŠ” ì•Šë‹¤.</li>
  <li>This distribution update ensures that instances misclassified bythe previous classifier are more likely to be included in the training data of the next classifier.</li>
  <li>Hence, consecutive classifiersâ€™ training data are geared towards increasingly hard-to-classify instances.</li>
  <li>Unlike Bagging, AdaBoost uses a rather undemocratic voting scheme, called the weighted majority voting.
    <ul>
      <li>The idea is an intuitive one: those classifiers that have shown good performance during training are rewarded with higher voting weights than the others.</li>
    </ul>
  </li>
</ul>

<p>##</p>

<h1 id="random-forest">Random Forest</h1>

<ul>
  <li>Random Forest: A variation of the bagging algorithm - baggingì²˜ëŸ¼ ì—¬ëŸ¬ ê°œ ensemble</li>
  <li>Created from individual decision trees
    <ul>
      <li>Diversity is guaranteed by selecting randomly at each split, a subset of the original features during the process of tree generation</li>
      <li><strong>tree êµ¬ì¡° : unstable êµ¬ì¡° â†’ diversityê°€ guaranteedë¨ automatically</strong></li>
    </ul>
  </li>
  <li>R.F í™œìš©
    <ul>
      <li>During classification, each tree votes and the most popular class is returned
        <ul>
          <li>classificationì—ì„œëŠ” : voteë¥¼ ê°€ì¥ ë§ì´ ë°›ì€ classê°€ ìµœì¢… ê²°ê³¼ decision</li>
        </ul>
      </li>
      <li>During regression, the result is the averaged prediction of all generated trees
        <ul>
          <li>regressionì—ì„œëŠ” :ê° treeë“¤ì´ resultì„ ë§Œë“œëŠ”ë° ì´ë¥¼ average ì·¨í•˜ë©´ random forest result</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>
    <p>random selectionì´ : feature selection / data sampling</p>
  </li>
  <li>
    <p>Two Methods to construct RandomForest:</p>

    <p>Random forest ë§Œë“œëŠ” ë‘ ê°€ì§€ ë°©ë²•</p>

    <ol>
      <li>Random input selection : ê° nodeì—ì„œ attributeë¥¼ randomly selectioní•˜ëŠ” ë°©ë²•
        <ul>
          <li>Forest-RI (random input selection): Randomly select, at each node, F attributes as candidates for the split at the node. The CART methodology is used to grow the trees to maximum size</li>
        </ul>
      </li>
      <li>Random linear combination: ê¸°ì¡´ featureì— ëŒ€í•œ linear combination ì‘ì—…ì„ ì·¨í•˜ì—¬ ì—¬ëŸ¬ ê°œì˜ attributeì— linear combinationì„ ë°”íƒ•ìœ¼ë¡œ tree í•™ìŠµì„ ì§„í–‰í•œë‹¤.
        <ul>
          <li>Forest-RC (random linear combinations): Creates new attributes (or features) that are a linear combination of the existing attributes (reduces the correlation between individual classifiers)</li>
        </ul>
      </li>
    </ol>
  </li>
  <li>adaboostì™€ ìœ ì‚¬í•œ íŠ¹ì§•ì„ ê°€ì§€ì§€ë§Œ error / outlierì— ë” robustí•œ íŠ¹ì„±ì„ ë³´ì¸ë‹¤.
    <ul>
      <li>Comparable in accuracy to Adaboost, but more robust to errors and outliers</li>
    </ul>
  </li>
  <li>ë¯¼ê°í•˜ê²Œ ë°˜ì‘í•˜ì§€ëŠ” ì•Šê³  ë” ë¹ ë¥´ê²Œ ì‹¤í–‰ : decision tree ìƒì„± ê³¼ì •ì€ í•™ìŠµ model ìì²´ê°€ íš¨ìœ¨ì ìœ¼ë¡œ êµ¬ì„±ë  ìˆ˜ ìˆê¸° ë•Œë¬¸ì— (tree êµ¬ì„±)
    <ul>
      <li>Insensitive to the number of attributes selected for consideration at each split, and faster than bagging or boosting</li>
    </ul>
  </li>
</ul>

<h1 id="model-selection">Model Selection</h1>

<ul>
  <li>
    <p>Given a problem, which algorithms should we use?</p>
  </li>
  <li>Golden rule: there is no algorithm that is the best one for all the problems
    <ul>
      <li>í•˜ë‚˜ì˜ íŠ¹ì • ì•Œê³ ë¦¬ì¦˜ì´ ë‹¤ë¥¸ ëª¨ë“  problem ëª¨ë‘ë¥¼ í•´ê²°í•˜ì§€ëŠ” ì•ŠëŠ”ë‹¤</li>
    </ul>
  </li>
  <li>Typically, two approaches (or both) can be adopted:
    <ul>
      <li>To choose the algorithm more suitable for the given problem</li>
      <li>To adapt the given data for the intended algorithm (using pre-processing, for instance)
        <ul>
          <li>ì£¼ì–´ì§„ dataë¥¼ ì˜ tuningí•  ìˆ˜ ìˆë„ë¡ í•œë‹¤ for ì‚¬ìš©í•˜ê³ ì í•˜ëŠ” algorithm (preprocessing)</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>The concept of â€œgood algorithmâ€ depends on the problem:
    <ul>
      <li>good algorithm : prob by prob</li>
      <li>Explainability : modelì´ ì–´ë–¤ íŒë‹¨ì„ ë‚´ë¦°ë‹¤ë©´ íŒë‹¨ì˜ ì •í™•ë„ë„ ì¤‘ìš”í•˜ë‚˜ ê·¸ ê²°ì •ì˜ ì´ìœ ë„ ì¤‘ìš”í•¨ : bayesian, decision treeëŠ” ì‰½ê²Œ ì„¤ëª…í•  ìˆ˜ ìˆëŠ”ë° ê·¸ ì™¸ì—ëŠ” ì„¤ëª…ì´ ì‰½ì§€ ì•ŠìŒ</li>
      <li>ë¶„ë¥˜ ê´€ë¦¬ ë¬¸ì œì— ìˆì–´ì„œëŠ” ì´ì†¡ ì‹œê°„ ì˜ˆì¸¡ ì •í™•ë„ê°€ ê°€ì¥ ì¤‘ìš”í•œ ì„ íƒìš”ì¸</li>
      <li>For a doctor, the interpretation of the model can be a major criterion for the selection of the model (decision trees and Bayesian networks are very appreciated)</li>
      <li>For logistics, the accuracy of travel time prediction is, typically, the most important selection criterion.</li>
    </ul>
  </li>
</ul>

<h1 id="statistical-validation">Statistical Validation</h1>

<ul>
  <li>Mixture of Experts
    <ul>
      <li>Combine votes or scores</li>
    </ul>

    <p><img src="11/Untitled_14.png" alt="Untitled" /></p>
  </li>
  <li>Stacking
    <ul>
      <li>Combiner f() is another learner (Wolpert, 1992)</li>
      <li>
        <p>adaboostë„ ìµœì¢… í•¨ìˆ˜ : ê°œë³„ classifierì— ê·¼ê±°í•˜ì—¬ accuracyë¡œë¶€í„° sequentially ìƒì„±</p>

        <p>Stacked generalization by <a href="https://www.sciencedirect.com/science/article/abs/pii/S0893608005800231#!">David H.Wolpert</a><a href="https://www.sciencedirect.com/science/article/abs/pii/S0893608005800231#aep-article-footnote-id1"></a></p>

        <p><a href="https://www.sciencedirect.com/science/article/abs/pii/S0893608005800231">https://www.sciencedirect.com/science/article/abs/pii/S0893608005800231</a></p>
      </li>
    </ul>

    <p><img src="11/Untitled_15.png" alt="Untitled" /></p>
  </li>
  <li>Cascading
    <ul>
      <li>Use next level of classifier if the previous decision is not confident enough</li>
    </ul>

    <p><img src="11/Untitled_16.png" alt="Untitled" /></p>
  </li>
</ul>
