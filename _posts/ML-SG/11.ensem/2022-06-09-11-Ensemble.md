---
title: "Ensemble Learning (Chapter 11)"
excerpt: "Chap11.Ensemble"
slug: "ML-11-Ensemble"
category: "ml"
lang: en
use_math: true
tags: ["ML", "AI", "Lecture"]
date: 2022-06-09T22:26:09-05:00
draft: false
---

# 11. Ensemble Learning

Property 1: Bishop 14

# Ensemble Learning

ì§€ê¸ˆê¹Œì§€ ë°°ìš´ ì—¬ëŸ¬ê°œì˜ classifier ê°ê° : parameter ë³€í™”í•˜ì—¬ ì—¬ëŸ¬ê°€ì§€ model

í•œ ê°€ì§€ modelë³´ë‹¤ëŠ” ì—¬ëŸ¬ modelì„ í™œìš©í•˜ì—¬ prediction

## Introduction

model ì—¬ëŸ¬ê°œë¥¼ ì¨ì„œ ì—¬ëŸ¬ê°œì˜ modelì„ ì¡°í•©í•˜ì—¬ ìµœì¢… modelì„ ê²°ì •

- Ensemble learning is a process that uses a set of models, each of them obtained by applying a learning process to a given problem. This set of models (ensemble) is integrated in some way to obtain the final prediction
- Aggregation of multiple learned models with the goal of improving accuracy
    - ëª©í‘œ : ì •í™•ë„ ë†’ì´ê¸° / classification, regression acc, clustering accì—ì„œ ì¢‹ì€ ì„±ëŠ¥ ì–»ê¸°
    - Intuition: simulate what we do when we combine an expert panel in a human decision-making process
        - ì‚¬ëŒë“¤ì´ ê²°ì • ë‚´ë¦´ ë•Œë„ í•œ ë‘ëª… ìƒê°ë³´ë‹¤ëŠ” ì „ë¬¸ ì§‘ë‹¨ panelì— ì˜í•´ì„œ ê²°ì • ë‚´ë¦¬ë©´ ì¢€ ë” í•©ë¦¬ì ì´ê³  ì¢€ ë” ì¢‹ì€ ê²°ì •ì„ ë‚´ë¦´ê±°ë¼ ìƒê°í•˜ëŠ” ê²ƒì²˜ëŸ¼ ê²°ì •

## Types of ensembles

- fusion
    - ë‹¤ë¥¸ versionì˜ dataset, algorithmì„ ê°–ê³  ì¢…í•©í•´ ìµœì¢… íŒë‹¨
    - ë‘ ê°œì˜ ìš©ì–´ëŠ” ë°”ìŠ·í•œ ê°œë…ìœ¼ë¡œ ì‚¬ìš©ë˜ê³ ë„ ìˆê³  ì—„ë°€í•˜ê²Œ ë¶„ë¥˜í•˜ê¸° ì• ë§¤í•¨
- ensemble
    - randomness : í•œ ìª½ì€ NN, SVM <- 2 model, no randomness
    - NN ì‚¬ìš© ì‹œ w randomly init : w ì´ˆê¸°ê°’ì´ ë‹¤ë¥¸ modelì„ ì—¬ëŸ¬ ê°€ì§€ ë§Œë“¤ë©´ randomnessë¡œ model ë§Œë“  ensemble

â†’ randomness ì¶”ê°€ëœ ë¶€ë¶„ : ensemble / Randomness excluded : Fusion (ì„ì–´ì„œ ì‚¬ìš©í•˜ê¸°ë„ í•¨)

---

- Ensemble methods are used for:
    - Classification
        - ê° modelë“¤ì´ ë¶„ë¥˜ ê²°ê³¼ë¥¼ ë§Œë“¤ì–´ë‚˜ë©´ ì´ë¥¼ ì¡°í•©í•˜ì—¬ ìµœì¢… (average)
        - classification alg ë°”ê¾¸ê¸° or dataset ë³€í™”ë¥¼ ì£¼ì–´ ë‹¤ë¥¸ ëª¨ë¸ ë§Œë“¤ ìˆ˜ ìˆìŒ
    - Regression
    - Clustering (also known as consensus clustering)
        - clustering : ì—¬ëŸ¬ versionì˜ clusteringì—ì„œ averageë¥¼ ì·¨í•˜ì—¬ ì–´ë–»ê²Œ avgí•  ê²ƒì¸ê°€ê°€ ì°¨ì´
            - groupì˜ categoryë¡œ ë¶„ë¥˜ : labelì´ í•™ìŠµí•  ë•Œ ì£¼ì–´ì§„ê²Œ ì•„ë‹˜
            â†’ 1ë²ˆ ê·¸ë£¹ ì•ˆ ì–´ë–¤ sampleë“¤ì´ ë™ì¼í•œ ê·¸ë£¹ ì•ˆì— ìˆë‹¤ëŠ” ê²Œ ì˜ë¯¸
            - 1ë²ˆ, 2ë²ˆ groupì€ ë‹¤ë¥¸ groupì„ì´ ì˜ë¯¸
            -> clusteringì˜ ê²°ê³¼ê°€ ë‚˜íƒ€ë‚  ë•Œ ì–´ë–»ê²Œ í•©ì¹  ê²ƒì¸ê°€
- Ensembles can also be classifed as :
    - Homogeneous: It uses only one induction algorithm
        - ë™ì¼í•œ ë¶„ë¥˜ algorithmì˜ ê²½ìš°ì—ë„ (induction algorithm = classifier)
            - Bayesian classifierì—ì„œë„ pdfë¡œ gaussian ì‚¬ìš© -> classifierì˜ êµ¬ì¡°ë¥¼ ë°”ê¾¸ë©´ ë‹¤ë¥¸ êµ¬ì¡°ì˜ algorithm
            - ì•„ì˜ˆ ë‹¤ë¥´ê²Œ ë°”ê¾¸ê²Œ ë˜ë©´ perceptron / svm/ decision tree ë“± DNN/ CNN ìì²´ë„ êµ¬ì¡°ëŠ” ì°¨ì´ê°€ ìˆìŒ
    - Heterogeneous: It uses different induction algorithms
        - algorithmì˜ êµ¬ì¡°ì— ì°¨ì´ê°€ ìˆìœ¼ë©´ heterogeneous

## Some Comments

- Combining models adds complexity
    
    
    - It is, in general, more difficult to characterize and explain predictions
        
        Modelì˜ ê°œìˆ˜ê°€ ëŠ˜ì–´ë‚˜ë‹ˆê¹Œ ì „ì²´ complexity ì¦ê°€ â†’ ì„¤ëª…í•˜ê¸°ë„ ì–´ë ¤ì›Œì§€ê³ , ì–´ë–¤ ê²°ê³¼ê°’ì´ ë‚˜ì˜¬ì§€, ê·¸ ê²°ê³¼ê°’ ì„¤ëª…í•˜ê¸°ë„ ì–´ë ¤ì›Œì§
        
    - The accuracy may increase
        
        í•˜ì§€ë§Œ ì •í™•ë„ëŠ” ì¼ë°˜ì ìœ¼ë¡œ í–¥ìƒë¨
        
- Violation of Ockhamâ€™s Razor: **â€œSimplicity leads to better accuracyâ€**
    - simple decision boundaryê°€ ì¼ë°˜ì ìœ¼ë¡œ ì¢‹ì€ ì„±ëŠ¥ì„ ë³´ì¸ë‹¤
    - Identifying the best model requires identifying the proper â€œmodel complexityâ€
        - ì¢‹ì€ ì„±ëŠ¥ì„ ë‚´ëŠ” modelì„ ì°¾ìœ¼ë ¤ë©´ complexityë¥¼ ê³ ë¯¼í•´ì•¼ í•˜ëŠ”ë°
        - occamì— ê±°ìŠ¤ë¥´ê¸´ í•˜ì§€ë§Œ, ì—¬ëŸ¬ model ì‚¬ìš©í•˜ê²Œ ë˜ë©´ ê°œë³„ modelì€ ë³µì¡í•˜ì§€ë§Œ ì¢…í•©í•œ modelì€ smoothí•œ ëª¨ì–‘ì¼ìˆ˜ë„ ìˆìŒ
        
        â†’ simpleí•˜ê²Œ ë§Œë“œëŠ” ê³¼ì •ì´ë¼ ë³¼ ìˆ˜ ìˆê³ , occamâ€™s razorì— ì í•©
        
        - simple boundary = simple algì´ë¼ê³  ìƒê°í–ˆëŠ”ë°, complexityë¥¼ ì˜¬ë¦¼ìœ¼ë¡œ dbê°€ simpleí•´ì§ˆìˆ˜ë„ ìˆë‹¤!
    - Decision boundary may become simpler, eventually. E

## The Ensemble Learning Process

dataê°€ ì£¼ì–´ì§€ë©´ ê·¸ì— ëŒ€í•´ í•¨ìˆ˜ë“¤ì„ ìƒì„± - ê°ê°ì´ ë¶„ë¥˜ í•¨ìˆ˜ë“¤ kê°œì˜ model ìƒì„±â†’ pruning

â†’ ìµœì¢… í•¨ìˆ˜ fk ë„ì¶œ

![Untitled](11/Untitled.png)

# Methods to Generate Ensembles

- Data Manipulation
    
    1. Train setì— ë³€í™”
    
    Supervised learningì—ì„œ train setì— ëŒ€í•´ train ë˜ì–´ ì–»ì–´ì§„ model
    
    - ë‹¤ë¥¸ train setì„ ì‚¬ìš©í•˜ë©´ -> ë‹¤ë¥¸ model : ë¶„ë¥˜ ì„±ëŠ¥ì´ ë‹¤ë¦„
    - it changes the training set in order to obtain different models
- Modeling process manipulation
    
    2. algorihtmì— ë³€í™”
    
    - model process manipulation : algorithmì˜ ë³€í™”
    - parameterë§Œ ë³€í™”í•˜ëŠ” ê²½ìš°ë„ ìˆê³ , classifier ìì²´ë¥¼ ë³€ê²½í• ìˆ˜ë„ ìˆìŒ, algorithm ìì²´ë¥¼ ë³€í™”ì‹œí‚¬ìˆ˜ë„ ìˆê³ 
    - â†’ ë‹¤ì–‘í•œ model (f1 _ /// fk)
    - it changes the induction algorithm, the parameter set or the model in order to obtain different models

![Untitled](11/Untitled_1.png)

## Data manipulation

Data, algorithmì— ëŒ€í•œ êµ¬ë¶„

- data : dataì˜ srcë¡œë¶€í„° subset ì¶”ì¶œ

Data src / sensor (visible, thermal, near infrared)

ë¹›ì˜ íŒŒì¥ì´ ê°€ì‹œê´‘ì„ , ì ì™¸ì„  ë“±ì— ë”°ë¼ ì˜ìƒì´ ë‹¤ì–‘íˆ ë‚˜íƒ€ë‚˜ëŠ”ë° ê°œë³„ì ì¸ srcë¡œ íŒë‹¨í•  ìˆ˜ ìˆê³  ì´ë¥¼ ì¡°í•©í•˜ëŠ” ê²ƒ ë˜í•œ ensembleì´ë¼ ë³¼ ìˆ˜ ìˆë‹¤

- ê·¸ëŸ° ì‹ì˜ ensembleì„ Fusionì´ë¼ê³  ë³¼ ìˆ˜ë„ ìˆë‹¤.

- Manipulating the input features
    
    ![Untitled](11/Untitled_2.png)
    
    movie data features : rating, actor, genre
    -> í¥í–‰í• ê²ƒì¸ì§€, ìˆ˜ìµì´ ì–´ëŠì •ë„ì¼ ê²ƒì¸ì§€, ê·¸ë£¹ì˜ ì‚¬ëŒë“¤ì´ ì¢‹ì•„í• ì§€
    
    - ì„¸ featureë¥¼ ë‹¤ë¥´ê²Œ ì¡°í•©í•˜ì—¬ ê°ê°ì˜ ê²½ìš°ê°€ ëª¨ë‘ ë‹¤ë¥¸ classifier model
- Sub-sampling from the training set
    
    ![Untitled](11/Untitled_3.png)
    
    dataì˜ ë¶„í•  : subsetë“¤ ì¡°í•©í•˜ì—¬ ìµœì¢… ê²°ê³¼
    

## Modeling process manipulation

- Manipulating the parameter sets
    - hyperparameter : network layer, ì´ˆê¸°ê°’ì„ ì–´ë–»ê²Œ ì„¤ì •í• ì§€,
    node ê°œìˆ˜, activation fnì€ ì–´ë–»ê²Œ ì„¤ì •í• ì§€
    
    ![Untitled](11/Untitled_4.png)
    
- Manipulating the induction algorithm
    
    ![Untitled](11/Untitled_5.png)
    

## How to Combine Models

- Algebraic methods : Scoreë¥¼ ì–´ë–»ê²Œ ì²˜ë¦¬í•  ê²ƒì¸ê°€
    - Average
    - Weighted average
    - Sum
    - Weighted sum
    - Product
    - Maximum
    - Minimum
    - Median
- Voting methods
    - Majority voting
    - Weighted majority voting
    - Borda count

![Untitled](11/Untitled_6.png)

![Untitled](11/Untitled_7.png)

# Characteristics of the Base Models

- The Base classifiers should be as accurate as possible and having diverse errors, while each classifier provides some positive evidences
    - diverseí•œ errorê°€ ë‚˜íƒ€ë‚˜ì•¼ í•¨. (Alg1, alg2 â€¦ ê²°ê³¼ê°€ diverse)
    - ì–´ëŠ ì •ë„ì˜ ì •í™•ë„ë¥¼ ê°€ì§€ë©´ì„œ ì •ë‹µì˜ ë‹¤ì–‘ì„±ì„ ê°€ì ¸ì•¼ classifyì˜ ì˜ë¯¸ê°€ ìˆë‹¤
    - ì—¬ëŸ¬ versionì˜ classifierë¥¼ ensembleí•˜ì—¬ ì¢‹ì€ classifier
- The average error of the base learners should be as small as possible
- The variance (of the predicted values) of the base learners should be as small as possible
    - Variance : when alg1 is trained
    
    Randomí•œ initial valueì— ì˜í•˜ì—¬ train ì—¬ëŸ¬ë²ˆí•˜ëŠ”ë°
    
    Variance ì •í™•ë„ê°€ ë§ì´ ë³€í™”í•œë‹¤ë©´ ì¢‹ì€ classifierê°€ ì•„ë‹ ê²ƒ
    
    diversity
    
    Stability : í•œ ì•Œê³ ë¦¬ì¦˜ì„ ë³¼ ë•Œ ë‹¤ì–‘í•˜ì§€ ì•Šì€ ê²°ê³¼
    
    bias : ansewrê³¼ì˜ ì°¨ì´
    
    Variance : ì–¼ë§ˆë‚˜ ì •ë‹µì´ ë‹¤ì–‘í•˜ê² ëŠ”ê°€
    

![Untitled](11/Untitled_8.png)

## Popular Ensemble Methods

Bagging:

- Averaging the prediction over a collection of predictors generated from **bootstrap samples** (both classification and regression)
    
    bootstrap sample :trian dataìˆìœ¼ë©´ subset sampling
    
    - ê°ê° samplingìœ¼ë¡œë¶€í„° classifier í•™ìŠµ
        
        Randomí•˜ê²Œ samplingí•˜ë©° ë‹¤ì–‘í•œ model
        

Boosting:

- Weighted vote with a collection of classifiers that were trained sequentially from training sets given priority to instances **wrongly classified**
    
    Boosting : ì—¬ëŸ¬ ë‹¨ê³„ë¥¼ ê±°ì³ classifier í•™ìŠµ
    
    - ì´ì „ ë‹¨ê³„ì˜ classifierì˜ ì˜¤ë‹µì— ì´ˆì ì„ ë§ì¶˜ë‹¤.
    
    ì˜¤ë¥˜ê°€ ë‚˜ì˜¤ëŠ” dataë“¤ì„ ëª¨ì•„ ë‹¤ìŒ stageì—ì„œ ì´ˆì ì„ ë§ì¶”ì–´ í•™ìŠµí•˜ì—¬ ìœµí•©í•œë‹¤
    

RandomForest:

- Averaging the prediction over a collection of trees constructed using a **randomly selected subset of features**
    - treeë¥¼ randomlyìƒì„±í•˜ì—¬ randomly selectí•´ì„œ ë§Œë“ ë‹¤.

Ensemble learning via negative correlation learning:

- Generating sequentially new predictors **negatively correlated** with the existing ones
    - í˜„ì¬ classifierí•˜ê³  negative corelationê°–ëŠ” classifierë¥¼ í•™ìŠµí•˜ì—¬ ìœµí•©í•œë‹¤

Heterogeneousensembles:

- Combining a set of **heterogeneous predictors**
    - NN + SVM + DT ë“± ìœµí•©

# Bagging: Bootstrap AGGregatING

![Untitled](11/Untitled_9.png)

- Analogy: Diagnosis based on multiple doctorsâ€™ majority vote
    
    ì—¬ëŸ¬ ëª…ì˜ ì˜ì‚¬ë“¤ì˜ ì§„ë‹¨ ê²°ê³¼ë¥¼ ìœµí•©í•˜ëŠ” ë°©ë²•
    
    Ex. Max score, average score ë“±
    
    - ì—¬ëŸ¬ modelì„ ë§Œë“¤ê¸° ìœ„í•´ì„œ bootstrap sampling

- Training
    - Given a set D of d tuples, at each iteration i, a training set $D_i$ of $d$ tuples is sampled with replacement from D (i.e. bootstrap)
        - bootstrap ë°©ë²• : sampling with replacement - ì „ì²´ datasetìœ¼ë¡œë¶€í„° samplingí•˜ì—¬ modelingí•˜ê³  ë‹¤ì‹œ ë³µì›
        - ê°ê°ì˜ data subsetì— ëŒ€í•˜ì—¬ modelì„ ë§Œë“¬

average, Sum ì€ ê°™ì€ ë°©ì‹ : sumì—ì„œ classifier numberë§Œí¼ ë‚˜ëˆ ì£¼ë©´ average value

- bootstrapping = original dataë¡œë¶€í„° sampling
- aggregating = ê·¸ê²ƒë“¤ë¡œë¶€í„° ê°ê°ì˜ classifierë¥¼ ë§Œë“¤ì–´ ë³‘í•©í•˜ëŠ” ë°©ë²•
- Classification: classify an unknown sample X
    - Each classifier $M_i$ returns its class prediction
    - The bagged classifier $M^*$ counts the votes and assigns the class with the most votes to X
        - ê° classifierê°€ sampleì— ëŒ€í•œ class ì˜ˆì¸¡ê°’ì„ ê³„ì‚°í•˜ê³ , ê·¸ë¦¬ê³  ìµœì¢… íŒë‹¨ì€ voting / sum/ ë“± ì—¬ëŸ¬ ë°©ë²•ì„ ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤.
- Prediction:
    - can be applied to the prediction of continuous values by taking the average value of each prediction for a given test tuple
    

- Accuracy
    - Often significantly better than a single classifier derived from
        
        significantly better : 5 ~ 10% ìƒìŠ¹
        
        - ë¬¼ë¡  modelì„ ì—¬ëŸ¬ë²ˆ ì“°ê³  ì—°ì‚°ëŸ‰ì€ ê·¸ë§Œí¼ ì¦ê°€
        - test stage : Test sampleì—ì„œëŠ” modelë“¤ ë‹¤ ìœ ì§€í•´ì„œ ê·¸ë§Œí¼ ë¶„ë¥˜ ì‘ì—… ìˆ˜í–‰ í›„ ìœµí•©
        
        Train + test stage ì—°ì‚°ëŸ‰ ì¦ê°€
        
    - For noisy data: not considerably worse, more robust
        - noisy data : robustí•˜ê²Œ ë¨ (boost sampling : Noisy dataê°€ ë¹ ì§„ í˜•íƒœë¡œ í•™ìŠµ)
    - Proved improved accuracy in prediction
- Requirement: need unstable classifier types
    - Unstablemeansasmallchangetothetrainingdatamayleadtomajor decision changes
    - requirement : unstable classifier
    
    Unstable : train dataë¥¼ ì¡°ê¸ˆ ë°”ê¿€ ê²½ìš° model decisionì´ í¬ê²Œ ë°”ë€ŒëŠ” modelì„ ì˜ë¯¸í•¨
    
    (Remind) Modelì´ ë°”ë€ train dataì— ëŒ€í•´ì„œ diverseí•œ error = varianceê°€ ì»¤ì•¼ í•œë‹¤.
    
    - baggingì˜ ê´€ì ì—ì„œëŠ” var í°ê²Œ ì¢‹ë‹¤ (ì¼ë°˜ì ìœ¼ë¡œëŠ” ë³„ë¡œ ì•ˆ ì¢‹ë‹¤)

- Stability in Training
    - Training: construct classifier from
    - Stability: small changes on results in small changes on
        
        Training : fë¥¼ dë¡œë¶€í„° í˜•ì„±
        
    - Decision trees are a typical unstable classifier

![Untitled](11/Untitled_10.png)

![Untitled](11/Untitled_11.png)

# Boosting

- Analogy: Consult several doctors, when there are disagreements, we focus more attention on that case
    
    ì˜ì‚¬ë“¤ì˜ ì˜ê²¬ì´ ê°ˆë¦´ ë•Œ í•©ì¹˜ë˜ì§€ ì•ŠëŠ” ì˜ê²¬ë“¤ì— ëŒ€í•´ ë” ì£¼ì˜ë¥¼ ê¸°ìš¸ì¸ë‹¤.
    

- Incrementally create models selectively using training examples based on some distribution.
    - Incrementallyí•˜ê²Œ sampleì´ subsetìœ¼ë¡œ selectionëœ í™•ë¥ ê°’ì„ ê°€ì§€ê³  ìˆìŒ
- How boosting works?
    - Weights are assigned to each training example
    - A series of k classifiers is iteratively learned
    - After a classifier $M_i$  is learned, the weights are updated to allow the subsequent classifier, $M_i +1$, to pay more attention to the training examples that were misclassified by
    - The final $M^*$ combines the votes of each individual classifier, where the weight of each classifier's vote is a function of its accuracy ğ’Š
- ê° sampleë“¤ì´ weightë¥¼ ê°€ì§€ê³  ìˆìŒ.

ê·¸ë¦¬ê³  ìš°ë¦¬ëŠ” kê°œì˜ classifierë¥¼ í•™ìŠµí•  ê²ƒ

ê·¸ëŸ°ë° M_iê°€ í•™ìŠµ ëœ ë‹¤ìŒ, classifierê°€ í•™ìŠµëœ ì´í›„ì—ëŠ”

weightì„ updateí•˜ëŠ”ë° ì• ë‹¨ê³„ì—ì„œ í•™ìŠµëœ modelë“¤ì´ misclassified ì— ë” ì£¼ì˜ë¥¼ ê¸°ìš¸ì¸ë‹¤ (weightë¥¼ ì˜¬ë¦°ë‹¤)

- > higher chance to be selected

ì¦‰ misclassified sampleë“¤ì´ ì ì  ê·¸ ìª½ìœ¼ë¡œ selectë˜ë©´ì„œ hard sampleë“¤ì´ ì ì  ì¶”ê°€ë˜ì–´ ë’¤ìª½ classifier í•™ìŠµ

1~kê°œ classifierë¥¼ ì¡°í•©í•˜ì—¬ m*

- weighted combination : weightëŠ” accuracyì— ë¹„ë¡€
- boosting ê¸°ë³¸ ì•„ì´ë””ì–´ : disagreement, hard sample

Hard sampleì— ì´ˆì  ë§ì¶”ëŠ” ë°©ë²• : classifier 1ë²ˆì„ ë§Œë“¤ê³  misclassifiedì— ëŒ€í•´ì„œ classifier 2ë²ˆì„ ë§Œë“¤ê³  m1, m2ê°€ ë‹¤ë¥¸ ê²°ì •ì„ ë‚´ë¦¬ëŠ” sampleì— ëŒ€í•´ì„œ classifier 3ë²ˆì„ ë§Œë“¤ì–´ test sampleì´ ë“¤ì–´ì˜¤ë©´ m1, m2ë¥¼ ëŒë ¤ ìµœì¢… ê²°ê³¼ë¡œ ì‚¬ìš©í•˜ê³  ë‘ ë¶„ë¥˜ê¸° ê²°ê³¼ê°€ ë‹¤ë¥´ë©´ m3ë¥¼ í™œìš©í•˜ì—¬ ê²°ê³¼ ë„ì¶œ

(dataê°€ ë‹¤ì‹œ samplingë  í™•ë¥ ê°’ ì¡°ì •í•˜ëŠ” ë°©ì‹ : adaboosting)

- M_i : weak classifier

## Adaboost

- Using Different Data Distribution
    - Start with uniform weighting
    - misclassified sampleì˜ weight ì¦ê°€
    - well classified sampleì— ëŒ€í•´ì„œëŠ” weight ê°ì†Œ
    - During each step of learning
        - Increase weights of the examples which are not correctly learned by the weak learner
        - Decrease weights of the examples which are correctly learned by the weak learner
- Idea
    - Focus on difficult examples which are not correctly classified in the previous steps
    - difficult exampleì— ë” ì£¼ì˜ë¥¼ ê¸°ìš¸ì¸ ì¼€ì´ìŠ¤

- Weighted Voting
    - Construct strong classifier by weighted voting of the weak classifiers
    - 
    - strong classifier ë§Œë“¤ ë•Œ weak classifier ì— weightë¥¼ ì£¼ê³  weighted voting / weighted sum ë“± ì¼ë°˜ì ì¸ ensemble ë°©ë²• ì ìš©
    - weak classifierë¥¼ ë§ì´ ì²¨ê°€í•˜ì—¬ combined classifierì˜ accuracy ì¦ê°€ (strong classifier/learner)
- Idea
    - Better weak classifier gets a larger weight
    - Iteratively add weak classifiers
        - Increase accuracy of the combined classifier through minimization of a cost function Ensemble Learning Adaboost Introduction to Machine Learning Page 17

![Untitled](11/Untitled_12.png)

- Differences with Bagging:baggingê³¼ì˜ ì°¨ì´ì 
    - Models are built sequentially on modified versions of the data
        - randomí•˜ê²Œ sampleëœê²Œ ì•„ë‹ˆë¼ weightì— ì˜í•´ sampleëœ dataì— ì˜í•´ í•™ìŠµ
    
    - The predictions of the models are combined through a weighted sum/vote
        - ì ì  hard sampleì— ëŒ€í•´ í•™ìŠµë˜ë‹ˆ easy sample / hard sampleì˜ classifierê°€ ë™ì¼í•œ weightë¥¼ ê°€ì§ˆ ìˆ˜ ì—†ìŒ : baggingì€ ë™ì¼í•œ ì¡°ê±´ìœ¼ë¡œ randomly sampling (no weight)
            - ê±°ì˜ ë™ë“±í•œ ì¡°ê±´ì´ê¸° ë•Œë¬¸ì— weightë¥¼ ì£¼ì§€ ì•ŠìŒ
            - boostingì˜ ê²½ìš° misclassifiedì— ëŒ€í•´ overfitting (hard sampleì´ ì¦ê°€í•˜ëŠ” ë°©í–¥ìœ¼ë¡œ weight update)-> ensembleí•˜ë©´ ì ì  hard sample ì¶”ê°€ë˜ë©° overfitting ìœ„í—˜
- Boosting algorithm can be extended for numeric prediction
    - Comparing with bagging: Boosting tends to achieve greater accuracy, but it also risks overfitting the model to misclassified data
    
- The diagram should be interpreted with the understanding that the algorithm is sequential: classifier $C_k$ is created before classifier $C_{k+1}$, which in turn requires that $\beta_k$ and the current distribution $D_k$ be available
    
    ![Untitled](11/Untitled_13.png)
    
    - sequential = ì•ì„œ misclassified sampleì„ êµ¬í•´ì•¼ ì´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì¶”ê°€ í•™ìŠµì„ ì§„í–‰í•˜ê¸°ì—
    ckëŠ” ck+1ë³´ë‹¤ í•™ìŠµì´ ì‚¬ì „ì— ì´ë£¨ì–´ì ¸ì•¼ í•˜ë©°
    Beta k : 4th classiferì˜ error / í˜„ì¬ data distributionì„ ì•Œì•„ì•¼ ë‹¤ìŒ ë‹¨ê³„ classiferë¥¼ í•™ìŠµì‹œí‚¬ ìˆ˜ ìˆìŒ

### Comments

ì´ëŸ°ì‹ìœ¼ë¡œ sample distributionì— ì˜í•´ data updateí•˜ì—¬ í•™ìŠµí•˜ë©´
ì• ìª½ ë‹¨ê³„ misclassified dataë“¤ì´ current stageì— í•™ìŠµ

- hard sampleì— í¸ì¤‘í•˜ì—¬ í•™ìŠµì´ ì¼ì–´ë‚¨ (undemocratic voting scheme)
Weighted majority voting : ensemble ëŒ€ìƒ ë‹¨ìœ„ì˜ classifierë“¤ì´ ì„±ëŠ¥ ìƒ í° ì°¨ì´ ì¡´ì¬ê°€ ìˆì–´ weightë¥¼ ì¤„ ìˆ˜ë°–ì— ì—†ê³  ì„±ëŠ¥ ì¢‹ì€ classifierì— ëŒ€í•´ ë” ë†’ì€ weightë¥¼ ì£¼ëŠ” ê²Œ ë” ìì—°ìŠ¤ëŸ¬ìš¸ ìˆ˜ ìˆê³ , ì´ëŸ°ê²Œ democraticí•˜ì§€ëŠ” ì•Šë‹¤.
- This distribution update ensures that instances misclassified bythe previous classifier are more likely to be included in the training data of the next classifier.
- Hence, consecutive classifiersâ€™ training data are geared towards increasingly hard-to-classify instances.
- Unlike Bagging, AdaBoost uses a rather undemocratic voting scheme, called the weighted majority voting.
    - The idea is an intuitive one: those classifiers that have shown good performance during training are rewarded with higher voting weights than the others.

## 

# Random Forest

- Random Forest: A variation of the bagging algorithm - baggingì²˜ëŸ¼ ì—¬ëŸ¬ ê°œ ensemble
- Created from individual decision trees
    - Diversity is guaranteed by selecting randomly at each split, a subset of the original features during the process of tree generation
    - **tree êµ¬ì¡° : unstable êµ¬ì¡° â†’ diversityê°€ guaranteedë¨ automatically**
- R.F í™œìš©
    - During classification, each tree votes and the most popular class is returned
        - classificationì—ì„œëŠ” : voteë¥¼ ê°€ì¥ ë§ì´ ë°›ì€ classê°€ ìµœì¢… ê²°ê³¼ decision
    - During regression, the result is the averaged prediction of all generated trees
        - regressionì—ì„œëŠ” :ê° treeë“¤ì´ resultì„ ë§Œë“œëŠ”ë° ì´ë¥¼ average ì·¨í•˜ë©´ random forest result
- random selectionì´ : feature selection / data sampling

- Two Methods to construct RandomForest:
    
    Random forest ë§Œë“œëŠ” ë‘ ê°€ì§€ ë°©ë²•
    
    1. Random input selection : ê° nodeì—ì„œ attributeë¥¼ randomly selectioní•˜ëŠ” ë°©ë²•
        - Forest-RI (random input selection): Randomly select, at each node, F attributes as candidates for the split at the node. The CART methodology is used to grow the trees to maximum size
    2. Random linear combination: ê¸°ì¡´ featureì— ëŒ€í•œ linear combination ì‘ì—…ì„ ì·¨í•˜ì—¬ ì—¬ëŸ¬ ê°œì˜ attributeì— linear combinationì„ ë°”íƒ•ìœ¼ë¡œ tree í•™ìŠµì„ ì§„í–‰í•œë‹¤.
        - Forest-RC (random linear combinations): Creates new attributes (or features) that are a linear combination of the existing attributes (reduces the correlation between individual classifiers)

- adaboostì™€ ìœ ì‚¬í•œ íŠ¹ì§•ì„ ê°€ì§€ì§€ë§Œ error / outlierì— ë” robustí•œ íŠ¹ì„±ì„ ë³´ì¸ë‹¤.
    - Comparable in accuracy to Adaboost, but more robust to errors and outliers

- ë¯¼ê°í•˜ê²Œ ë°˜ì‘í•˜ì§€ëŠ” ì•Šê³  ë” ë¹ ë¥´ê²Œ ì‹¤í–‰ : decision tree ìƒì„± ê³¼ì •ì€ í•™ìŠµ model ìì²´ê°€ íš¨ìœ¨ì ìœ¼ë¡œ êµ¬ì„±ë  ìˆ˜ ìˆê¸° ë•Œë¬¸ì— (tree êµ¬ì„±)
    - Insensitive to the number of attributes selected for consideration at each split, and faster than bagging or boosting

# Model Selection

- Given a problem, which algorithms should we use?

- Golden rule: there is no algorithm that is the best one for all the problems
    - í•˜ë‚˜ì˜ íŠ¹ì • ì•Œê³ ë¦¬ì¦˜ì´ ë‹¤ë¥¸ ëª¨ë“  problem ëª¨ë‘ë¥¼ í•´ê²°í•˜ì§€ëŠ” ì•ŠëŠ”ë‹¤
- Typically, two approaches (or both) can be adopted:
    - To choose the algorithm more suitable for the given problem
    - To adapt the given data for the intended algorithm (using pre-processing, for instance)
        - ì£¼ì–´ì§„ dataë¥¼ ì˜ tuningí•  ìˆ˜ ìˆë„ë¡ í•œë‹¤ for ì‚¬ìš©í•˜ê³ ì í•˜ëŠ” algorithm (preprocessing)
- The concept of â€œgood algorithmâ€ depends on the problem:
    - good algorithm : prob by prob
    - Explainability : modelì´ ì–´ë–¤ íŒë‹¨ì„ ë‚´ë¦°ë‹¤ë©´ íŒë‹¨ì˜ ì •í™•ë„ë„ ì¤‘ìš”í•˜ë‚˜ ê·¸ ê²°ì •ì˜ ì´ìœ ë„ ì¤‘ìš”í•¨ : bayesian, decision treeëŠ” ì‰½ê²Œ ì„¤ëª…í•  ìˆ˜ ìˆëŠ”ë° ê·¸ ì™¸ì—ëŠ” ì„¤ëª…ì´ ì‰½ì§€ ì•ŠìŒ
    - ë¶„ë¥˜ ê´€ë¦¬ ë¬¸ì œì— ìˆì–´ì„œëŠ” ì´ì†¡ ì‹œê°„ ì˜ˆì¸¡ ì •í™•ë„ê°€ ê°€ì¥ ì¤‘ìš”í•œ ì„ íƒìš”ì¸
    - For a doctor, the interpretation of the model can be a major criterion for the selection of the model (decision trees and Bayesian networks are very appreciated)
    - For logistics, the accuracy of travel time prediction is, typically, the most important selection criterion.

# Statistical Validation

- Mixture of Experts
    - Combine votes or scores
    
    ![Untitled](11/Untitled_14.png)
    
- Stacking
    - Combiner f() is another learner (Wolpert, 1992)
    - adaboostë„ ìµœì¢… í•¨ìˆ˜ : ê°œë³„ classifierì— ê·¼ê±°í•˜ì—¬ accuracyë¡œë¶€í„° sequentially ìƒì„±
        
        Stacked generalization by [David H.Wolpert](https://www.sciencedirect.com/science/article/abs/pii/S0893608005800231#!)[](https://www.sciencedirect.com/science/article/abs/pii/S0893608005800231#aep-article-footnote-id1)
        
        [https://www.sciencedirect.com/science/article/abs/pii/S0893608005800231](https://www.sciencedirect.com/science/article/abs/pii/S0893608005800231)
        
    
    ![Untitled](11/Untitled_15.png)
    
- Cascading
    - Use next level of classifier if the previous decision is not confident enough
    
    ![Untitled](11/Untitled_16.png)