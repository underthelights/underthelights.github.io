---
layout: post
date: 2022-09-08
title: "[ALG] 1.3. Order of Algorithms (1)"
tags: [Algorithm, ]
categories: [Notes, ]
---

- ì œì¼ ë¨¼ì € ìƒê°
	- Input Size
		- Problemì„ í’€ê³ ì í•˜ëŠ”ë° ì´ë¥¼ solí•˜ê³ ì í•˜ëŠ” algorithmì—ì„œ, ì•Œê³ ë¦¬ì¦˜ì— ë“¤ì–´ì˜¤ëŠ” dataì˜ í¬ê¸°ëŠ” ì–´ë–»ê²Œ ë˜ëŠ”ê°€.
			- ë¬¸ì œ ìƒí™©ì— ë”°ë¼ ê°€ë¡œ n, ì„¸ë¡œ mì˜ ë°ì´í„°ê°€ ë“¤ì–´ì˜¤ë©´ (n,m) nì¼ìˆ˜ë„ nmì¼ìˆ˜ë„ ìˆë‹¤.
		- data sizeê°€ ì»¤ì§ì— ë”°ë¼ ì–¼ë§ˆë‚˜ ì‹œê°„ì´ ê±¸ë¦¬ëŠ”ê°€ ë¶„ì„ : ì–¼ë§ˆë‚œ í¬ê¸°ì˜ ë°ì´í„°ê°€ ë“¤ì–´ì™”ì„ ë•Œ, ì–¼ë§ˆë‚˜ ì‹œê°„ì´ ê±¸ë¦¬ëŠ”ê°€.
		- ì‹œê°„ ë¶„ì„ì˜ ê¸°ë³¸ ìš”ì†Œ : dataì˜ ìˆ˜
	- Cost : $g(n)$
		- ë¬¸ì œê°€ ìˆì„ ë•Œ nì— ëŒ€í•˜ì—¬ ì–´ëŠ ì •ë„ì˜ ë¹„ìš©ì´ ê±¸ë¦¬ëŠ”ê°€.

# _O_Â (BigÂ $O$Â Notation)


> ğŸ’¡ for given two functionsÂ $f(n)$Â andÂ $g(n)$,  
>   
> $g(n) = O(f(n))\iff \exists c \in \mathbb{R}, N \in \mathbb{N}\quad g(n)\leq c\cdot f(n), \forall n \geq N$

- complexityë¥¼ ë”°ì§ˆ ë•Œ, data sizeê°€ ì‘ì„ ë•Œ ë³´ë‹¤ëŠ” ì»¤ì§ˆ ë•Œ ë¬¸ì œê°€ ë°œìƒí•¨ì„ í™•ì¸í•˜ê³  ì‹¶ìŒ.
	- â†’ ëª¨ë“  nì¼ í•„ìš”ëŠ” ì—†ê³ , ê·¸ Në³´ë‹¤ í° ëª¨ë“  input sizeì— ëŒ€í•´ì„œ ì´ëŸ¬í•œ ì¡°ê±´ì„ ë§Œì¡±í•˜ë©´.
- then we say that :$g(n)$Â is big O ofÂ $f(n)$
- ì˜ˆ)
	- ì½”ë“œì˜ ë¹„ìš©ì„ ë¶„ì„í•´ ë´¤ë”ë‹ˆ
		- for loop 1, for loop n ììŠ¹ë§Œí¼ ëŒê²Œ ëœë‹¤.

		```c
		x = x + 1;
		for (i = 1; i <= n; i++)
			y = y + 2;
		for (i = n; i >=1; i--)
			for (j = n; j >= 1; j--)
				z = z + 1;
		```

- ë¹„ìš© :Â $g(n) = c_0 + c_1 n + c_2 n^2$
- ì˜ˆ :Â $g(n) = 5 + 6 + 7n^2 \leq 8n^2 \quad \forall n \geq 8$
	- $8 \cdot n^2 = c \cdot f(n), N = 8$
		- nì´ ì»¤ì§€ë©´ g(n)ì´ ì••ë„ì ìœ¼ë¡œ ë‹¤ë¥¸ ì¹œêµ¬ë“¤ì„ ëˆ„ë¥´ê²Œ ëœë‹¤.
		- ë‹¤ì‹œí•œë²ˆ ì´ì•¼ê¸°í•˜ì§€ë§Œ, nì´ ì»¤ì§ˆ ë•Œ, ë‚´ê°€ ë¶„ì„í•œ ë¹„ìš©ì€ f(n)ì´ë¼ëŠ” í•¨ìˆ˜ì— ëˆŒë¦¬ê²Œ ë˜ëŠ” upper bound ê°œë…
	- $g(n) = O(n^2)$
- $g(n) = O(n^{1000})$?
	- ì •ì˜ì— ì˜í•˜ë©´ ë§ìŒ : ê·¸ë˜í”„ìƒ í™•ì¸í•´ë³´ì•„ë„ ë§ìŒ.
	- $f(n)\geq g(n) \cdot c$

## Notes for big O

- [Note 1] The big O puts anÂ <u>**asymptotic**</u>Â <u>upper bound</u> on a function.
	- ë³µì¡ë„ë¥¼ ë”°ì§ˆ ë•Œ â€˜ëª‡ ì´ˆ'ê°€ ê±¸ë¦°ë‹¤ê¸° ë³´ë‹¤ëŠ” ì–¼ë§ˆë‚˜ íš¨ìœ¨ì ì¸ê°€ë¥¼ ë”°ì§€ëŠ” ì²™ë„
		- PL, HW ìƒí™©ì— ëŒ€í•´ì„œ implementation ê´€ì ì—ì„œ ê°œì¸ ì°¨ ë°œìƒ
	- Asymptotic analysis (from Wikipedia)
		- asymptotic : ì ê·¼ì ì¸
		- data sizeê°€ ì»¤ì§ˆ ë•Œ, ì´ ì•Œê³ ë¦¬ì¦˜ì´ ì‹œê°„ì´ë‚˜ í•„ìš”ë¡œ í•˜ëŠ” ë©”ëª¨ë¦¬ ì‚¬ì´ì¦ˆê°€ ì–¼ë§ˆë‚˜ ë‚˜ë¹ ì§€ëŠ”ê°€? â†’ ì•Œê³ ë¦¬ì¦˜ì´ ìš”êµ¬í•˜ëŠ” ì‹œê°„, ë©”ëª¨ë¦¬ ì–‘ ë“±ì´ ì–¼ë§ˆë‚˜ ë‚˜ìœ í˜•íƒœë¡œ ë³€í™”í•˜ëŠ”ì§€ â€˜í˜•íƒœâ€™

			> IfÂ $f(n) = n^2 + 3n$, then as n becomes very large, the termÂ $3n$Â becomes insignificant compared toÂ $n^2$. The functionÂ f(n)f(n)Â is said to be "asymptotically equivalent toÂ n^2n2, asÂ $nâ†’âˆ$". This is often written symbolically asÂ $f(n) -> n^2$, which is read as "$f(n)$Â is asymptotic toÂ $n^2$".

	- ê³„ì‚° ë¹„ìš©ì´Â $0.01n^2$Â ê³¼Â $100n$Â ì•Œê³ ë¦¬ì¦˜ ì¤‘ ì–´ë–¤ ê²ƒì´ ë” íš¨ìœ¨ì ì¸ê°€?
		- ì´ë¡ ì ì¸ ê´€ì ì—ì„œ $100n = O(n), 0.01n^2 = O(n^2)$
		- input size $n=3$: $0.09 \quad 300$
		- input size $n=10^6$: $0.01 \cdot (10^6)^2 = 10^{10}$, $100 \cdot 10^6 = 10^8$

		â†’ ê²°êµ­ $O(n^2)$

	- (Tight) upper bound
		- $37log n + 0.1n = O(n)$
			- nì´ ì»¤ì§€ë©´ $\log n$ì€ ìƒë‹¹íˆ ì‘ì•„ì§
		- $n^2 + 10n = O(n^2)$
		- $4(\log n)^2 + n \log n + 100n = O(n \log n)$
			- $\log n$ vs $n$ â†’ ë‹¹ì—°í•˜ê²Œ $\log n$
			- $n \log n > {\log n} ^2$
		- $n^2 + 10n = O(n^{200})$???
			- upper bound ë§ì•„ í‹€ë¦° ë§ì€ ì•„ë‹ˆì§€ë§Œ, ì¼ë°˜ì ìœ¼ë¡œ O Notationì„ í™œìš©í•  ë•Œì—ëŠ” tight upper boundë¥¼ ì„ íƒí•˜ì—¬ í‘œí˜„í•œë‹¤.

		> ğŸ’¡ Dominating Term   
		> - ì§€ë°°í•˜ëŠ” termì„ ì°¾ì•„ Upper Boundë¥¼ ì°¾ëŠ”ë‹¤.

			- ì§€ë°°í•˜ëŠ” termì„ ì°¾ì•„ Upper Boundë¥¼ ì°¾ëŠ”ë‹¤.
		- $\log _en $ë“± baseëŠ” ì™œ ê³ ë ¤í•˜ì§€ ì•ŠëŠëƒ â†’ ìƒìˆ˜ì— í•´ë‹¹í•˜ë¯€ë¡œ ìƒê´€ ì—†ê¸° ë•Œë¬¸ì—.
			- $\log _2 n = \frac {\log_e n }{\log_e2}$

	### Growth Rates of Some Common Complexity Functions

	- ì´ë¡ ì ìœ¼ë¡œëŠ” $n^3$ì€ efficientí•˜ì§€ë§Œ í˜„ì‹¤ì ìœ¼ë¡œëŠ” ë¬¸ì œê°€ ë°œìƒí•  ìˆ˜ ìˆì„ë§Œí•œ ë³µì¡ë„
	- ê°ë‹¹í•  ìˆ˜ ì—†ì„ ì •ë„ë¡œ ì»¤ì§€ë„¤ ë“± asymptotic íŠ¹ì„±ì„ ë¶„ì„í•˜ê¸° ìœ„í•¨

![0](/assets/img/2022-09-08-[ALG]-1.3.-Order-of-Algorithms-(1).md/0.png)

- [Note 2] Given a cost function g(n), how do you find the proper complexity functionÂ f(n)Â such that $g(n) = O(f(n))$?
	- Suppress lower-order terms and constant factors!
	- Example:
		- $10^3 + 10^3n + 10^-3 n^2 = O(n^2)$
			- thenÂ $lim_{n \to \infty} \frac{n^2}{n} = \infty$
		- $5n \log_3 n + 3(\log_2 n)^2 + n + 6n^2 = O(n^2)$
			- thenÂ $lim_{n \to \infty} \frac{n}{log_en} = lim _{n \to \infty} = \infty$
		- $3(log_2 n)^2+ 0.1n = O(?)$
			- Dominate Termì´ ë¬´ì—‡ì¼ê¹Œ?
			- $\lim _{n \rightarrow \infty}{\frac {(\log n)^2}{n}}$ =$\infty, c, 0$
				- $\infty$ : $(\log n)^2$ , $0$ : $n$ , $c$ :
				- Lâ€™Hospital Theorem : $\lim _{n \rightarrow \infty}{\frac {f(n)}{g(n)}} = \lim _{n \rightarrow \infty}{\frac {f'(n)}{g'(n)}} $
					- $\lim _{n \rightarrow \infty}{\frac {(\log n)^2}{n}} = \lim = \lim _{n \rightarrow \infty} {\frac {2}{n}{}}0$
			- â†’ Linear Time
		- $2^{n+5} = O(2^n)$Â ??
		- $2^{5n} = O(2^n)$??

## Comparing Orders of Growth

- How do you compare orders of growth of two functions?
	- One possible way is to compute the limit of the ratio of two functions in question.
	- $x = lim_{n \to \infty } \frac{f_1(n)}{f_2(n)}$
		- ifÂ _x_=0,Â _f_1Â has a smaller order of growth thanÂ _f_2
		- ifÂ $x=c$,Â $f_1$Â has a same order of growth thanÂ $f_2$
		- ifÂ $x=\infty$,Â $f_1$Â has a larger order of growth thanÂ $f_2$
	- Ex.1:Â $\log_2 n $ vs. $\sqrt{n}$
		- $lim_{n \to \infty} \frac{log_2 n}{\sqrt(n)} = lim_{n \to \infty} \frac{(log_2 n)'}{(\sqrt(n))'} = lim_{n \to \infty} \frac{(log_2 e)\frac{1}{n}}{\sqrt\frac{1}{2\sqrt(n)}} $
	- Ex.1.2.:Â $\log_2 n $ vs. $n^{0.0001}$
		- $lim_{n \to \infty} \frac{log_2 n}{\sqrt(n)} $
	- Ex.2:Â $n!$Â vsÂ $2^n$ - factorial vs. exponential
		- $lim_{n \to \infty} \frac{ n!}{2^n} = lim_{n \to \infty} \frac{\sqrt{2 \pi n} (\frac {n}{e})^n}{2^n}=lim_{n \to \infty }\sqrt{2 \pi n} \frac{({n})^n}{2^n e^n}$
		- stirling's formula :Â $n! \approx \sqrt{2 \pi n} (\frac {n}{e})^n$

# $Î©$Â (Big Omega Notation)


â†’ Lower Bound

- for two given functionsÂ $f(n), g(n)$

> ğŸ’¡ $g(n) = \Omega(f(n))g(n)=Î©(f(n)) âŸº \exists c \in \mathbb{R}, âˆƒcâˆˆR,$ and $N \in \mathbb{Z^+ \cup {0}}$, s.t.Â $g(n) \geq cf(n) \forall n \geq N$

- We say thatÂ g(n)_g_(_n_)Â isÂ _Ï‰_Â ofÂ $f(n)$.
- TheÂ Î©Â puts an asymptotic lower bound on a function.
- Ex:
	- $37\log n+0.1n=\Omega(n)$
	- $n^2 + 10n = \Omega(n^2)$
	- $4(logn)^2 +nlogn+100n=\Omega(nlogn)$
	- $n^{200} +10n=\Omega(n^2)$
	- 

		![1](/assets/img/2022-09-08-[ALG]-1.3.-Order-of-Algorithms-(1).md/1.png)


# $Î˜$Â (Big Theta Notation)

- for two given functionsÂ f(n)_f_(_n_)Â ,Â g(n)_g_(_n_)
	- ìœ„ì—ì„œ ëˆ„ë¥´ê³ 
	- ì•„ë˜ì—ì„œ ëˆ„ë¥´ê³ 

> ğŸ’¡ $g(n) = \Theta(f(n))g(n)=Î˜(f(n))Â \iffâŸºÂ g(n) = O(f(n))g(n)=O(f(n))Â andÂ g(n) = \Omega (f(n))$

- that is,

	> ğŸ’¡ $g(n) = \Theta (f(n))g(n)=Î˜(f(n))Â \iffâŸºÂ \exists c,d \in \mathbb{R}âˆƒc,dâˆˆRÂ andÂ N \in \mathbb{Z^+ \cup {0}}NâˆˆZ+âˆª0Â s.t.Â g(n) \geq cf(n)g(n)â‰¥cf(n)Â \forall n \geq Nâˆ€nâ‰¥N$

- We say thatÂ g(n)_g_(_n_)Â is order ofÂ f(n)_f_(_n_).
- TheÂ \ThetaÎ˜Â puts an asymptotic bound on a function.
- $0.1n + 10n^2 = O(n^{1000}) / O(n^2)$
	- Big O ë¡œ N ììŠ¹ì´ë‹¤ í•˜ë©°ëŠ” tight upper boundë¥¼ ì´ì•¼ê¸°í•œë‹¤.
- Ex:
	- $37\log n+0.1n=\Theta(n)$
		- $O(n), \Omega(n)$
	- $n^2 + 10n = \Theta(n^2)$
	- $4(logn)^2 +nlogn+100n=\Theta(nlogn)$

> ğŸ’¡ $\Theta(1)<\Theta(log n)<\Theta(n)<\Theta(n log n)<\Theta(n^2)<\Theta(n^3)<\Theta(n^j)<\Theta(n^k)<\Theta(a^n)<\Theta(b^n)<\Theta(n!)$

- forÂ $k>j>3 , b>a>1$
- O(1)Â orÂ _O_(_c_)Â : constant
	- $g(n) = 0.000001 \cdot n$
	- g(n) = 1000000_g_(_n_)=1000000
- Ref. Neapolitan Ex. (pp.42) 19, 24, 26, 28]

# Big O, Omega, and Order

- 

	![2](/assets/img/2022-09-08-[ALG]-1.3.-Order-of-Algorithms-(1).md/2.png)

- Ref._Ref_.Â [ Neapolitan Chapter 1.]

## Execution Times for Algorithms with the Given Time Complexities

- nì˜ kìŠ¹, ì´ë¥¼ polynomial time (ë‹¤í•­ ì‹œê°„)ì´ë¼ê³  ì¹­í•œë‹¤.
- n log n, logëŠ” ë‹¤í•­ì‹ì´ ì•„ë‹Œë° ì™œ polynomialì´ë¼ê³  í•˜ëŠ”ê°€?
	- upperbound : n ììŠ¹ë³´ë‹¤ ë¹ ë¥´ë‹ˆê¹Œ polynomial timeì— ë“¤ì–´ê°€ê²Œ ë¨.
- ì‹¤ì œì ìœ¼ë¡œ íšŒì‚¬ ê°€ì„œ ì´ ë¬¸ì œë¥¼ í’€ì–´ì£¼ëŠ” SWë¥¼ í•´ì„œ, ì œì¶œí•œë‹¤ê³  í–ˆì„ ë•Œ, nì´ ì»¤ì§ì— ë”°ë¼ ì´ëŸ¬í•œ íš¨ìœ¨ì ì¸ ì•Œê³ ë¦¬ì¦˜ë“¤ì€ ê·¸ í”¼í•´ê°€ ëœí•œë°, ë’¤ëŠ” í”¼í•´ê°€ í˜„ì‹¤ì ìœ¼ë¡œ ë°›ì•„ë“¤ì¼ ìˆ˜ ì—†ì„ ì •ë„ë¡œ ì»¤ì§„ë‹¤. ìš°ë¦¬ê°€ ì´ë¡ ì ìœ¼ë¡œ êµ¬ë³„ì€ ë¬´ì—‡ì´ëƒ í•˜ë©´ì€ polynomial - nonpolynomial alg.ë¥¼ êµ¬ë¶„
	- $n^6$ì€ ì´ë¡ ì ìœ¼ë¡œëŠ” polynomial - ì•ˆ ì¢‹ê¸´ í•˜ì§€ë§Œ
- exponential, factorial algorithmì€ inefficient, polynomial algorithmì€ algorithm
	- í˜„ì‹¤ì ìœ¼ë¡œëŠ” cubicë„ ë¹¡ì…ˆ
- ì•Œê³ ë¦¬ì¦˜ :
	- ì´ë¡ ì ì¸ ì¸¡ë©´ì—ì„œ polynomial time ì— ì“°ì´ëŠ”ê°€
	- í”„ë¡œê·¸ë¨ì„ êµ¬í˜„í•´ì„œ ëŒë¦´ ë•Œ, ë‹¹ì—°íˆ cuvicë³´ë‹¤ëŠ” íš¨ìœ ì ìœ¼ë¡œ ëŒì•„ê°ˆ ê²ƒì´ë‹¤
- í˜„ì‹¤ì ìœ¼ë£¨ nì´ ì‘ì„ ë•Œì—ëŠ” ê·¸ë ‡ê²Œ í° ì§€ì¥ì´ ì—†ì§€ë§Œ, nì´ ì ì°¨ ì»¤ì§ˆ ë•Œ log, linearëŠ” ì˜ ë²„í‹°ëŠ” ë°ì— ë°˜í•´ exp, factorial
	- processorê°€ ì¢‹ì•„ì§„ë‹¤ í•œë“¤ í’€ê³ ì í•˜ëŠ” ë¬¸ì œê°€ ë” ì»¤ì§€ê¸° ë•Œë¬¸ì— ì‹œëŒ€ì  needsë¼ê¸° ë³´ë‹¤ëŠ” í•­ìƒ ìš°ë¦¬ ê³ì˜ needs

	![3](/assets/img/2022-09-08-[ALG]-1.3.-Order-of-Algorithms-(1).md/3.png)

	- logarithmic, linear, n log n, quadratic, cubic << exp << factorial

![4](/assets/img/2022-09-08-[ALG]-1.3.-Order-of-Algorithms-(1).md/4.png)


# Worst-Case vs. Average-Case Time Complexity

- complexity : time, space complexity
	- í¸ì˜ìƒ time complexityë¥¼ ë”°ì§
- **Expected value** (from Wikipedia)
	- letÂ X_X_Â be a random variable with a finite number of finite outcomesÂ $x_1, x_2, ..., x_k$Â occuring with probabilitiesÂ $p_1, p_2, ... p_k$Â respectively.
	- the Expectation of X is defined as :
		- $E(X) = \sum_{i=1}^{k }{x_i p_i} = x_1p_1+ x_2 p_2 + ... + x_k p_k$
	- since the sum of all probabilitiesÂ $p_i$Â is 1 (\sum_{i=1}^{k} {p_i}=1âˆ‘_i_=1_k__pi_=1) , the expected value is the weighted sum of theÂ x_i_xi_Â values, with theÂ p_i_pi_Â values being the weights
	- $a_1, a_2, a_3 \rightarrow b_1 , b_2, b_3$
		- input size nì— ëŒ€í•´ì„œ ëª¨ë“  ê°€ëŠ¥í•œ inputë“¤ì˜ ì§‘í•©ì„ S_n
		- ì„ì˜ì˜ input $I$
		- $c(I) $ :
		- $p(I):$
- Worst-case complexity :
	- ëª¨ë“  ê°€ëŠ¥í•œ ì¸í’‹ì¤‘ì— ìµœì•…ìœ¼ë¡œ ê°€ì¥ ë§ì€ ì‹œê°„ì´ ê±¸ë¦¬ëŠ” ê²½ìš°. ê°€ì¥ ì‹œê°„ì´ ë§ì´ ê±¸ë¦¬ëŠ” ê²½ìš°
	- $T_W (n) = max \{ c(I)| I \in S_n \}$
- Average-case complexity
	- ëª¨ë“  inputì— ëŒ€í•´ì„œ ì¼ì–´ë‚  í™•ë¥  ë“±ì— ëŒ€í•´ í‰ê· ì„ ë‚¸ ê²ƒ.
	- $T_A (n) = \sum_{I \in S_n} p(I) c(I)$
- Problem
	- Find the index of a given valueÂ _a_Â in a givven arrayÂ $(a_0, a_1, ...,a _{n-1})$. ifÂ _a_Â doesn't exist in the array returnÂ âˆ’1
- Cost for a linear search algorithm
	- letÂ P_i_Pi_Â be the probability such thatÂ a= a_i_a_=_ai_
	- then the average cost is :

		$g(n) = 1 \cdot P_0 + 2 \cdot P_1 + 3 \cdot P_2 + ...+ n \cdot P_{n-1} + n (1 - \sum_{k=0}^{n-1} P_k)$


		$= \sum_{k=0}^{n-1} (k+1)P_k + n (1 - \sum_{k=0}^{n-1} P_k)=âˆ‘k=0nâˆ’1(k+1)Pk+n(1âˆ’âˆ‘k=0nâˆ’1Pk)$

		- Ex.1.Â $n = 10^9$,Â $P_0 + P_1 + ...+ P_{10^3} = 1$Â soÂ $g(n)=O(1)$
		- Ex.2.Â $n = 10^9$,Â $P_0 + P_1 + ...+ P_{\frac n {100} }= 1$, soÂ g(n) = O(n)_g_(_n_)=_O_(_n_)

> ğŸ’¡ ë°°ìš°ê²Œ ë  ë‚´ìš©, ì¤‘ìš”

- [ì¤‘ìš”] **ì°¸ê³ : Quick sort ì•Œê³ ë¦¬ì¦˜ â†’**
	- Worst-caseÂ : $O(n^2)$
	- Average-CaseÂ : $O(n \log n)$

# Reviews


## Summation

- Sums of powers
	- $\sum_{i=1}^{n} i = \frac {n(n+1)} {2}$
	- $\sum_{i=1}^{n} i^2 = \frac {n(n+1)(2n+1)} {6}$
	- $\sum_{i=1}^{n} i^3 = (\frac {n(n+1)} {2})^2$
	- $\sum_{i=1}^{n} i^4 = \frac {n(n+1)(2n+1)(3n^2+3n-1)} {30}$
	- $\sum_{i=1}^{n} i^s = \frac {(n+1)^{s+1}} {s+1} + \sum_{k=1}^{s} \frac {B_k} {s-k+1} {s \choose k} (n+1)^{s-k+1}$
		- $B_k$Â is theÂ $k^{th}$Â Bernoulli Number.
	- $\sum_{i=1}^{n} i^{-s} = \prod_{p prime} \frac {1} {1 - p^{-s}} = \zeta(s)$
		- $\zeta_k$ is the Riemann zeta function
- Growth rates
	- $\sum_{i=1}^{n} i^c \in \Theta(n^{c+1})$
		- for realÂ cÂ greater thanÂ 1âˆ’1
	- $\sum_{i=1}^{n} \frac 1 i \in \Theta(log n)$
	- $\sum_{i=1}^{n} c^i \in \Theta( n \cdot log(n)^{c+1})$
		- for realÂ c_c_Â greater thanÂ 11
	- $\sum_{i=1}^{n} log(i)^c \in \Theta(n \cdot log(n)^{c})$Â for nonnegative realÂ $c$
	- $
	
	\sum_{i=1}^{n} log(i)^c \cdot i^d \in \Theta(n^{d+1} \cdot log(n)^{c})$Â for nonnegative realÂ $c, d$
	- $\sum_{i=1}^{n} log(i)^c \cdot i^d \cdot b^i \in \Theta(n^{d} \cdot log(n)^{c} \cdot b^n)Â $for nonnegative realÂ $b>1, c, db>1,c,d$
- **Read**Â [_Summation_](http://en.wikipedia.org/wiki/Summation),Â [_Mathematical Series_](http://en.wikipedia.org/wiki/List_of_mathematical_series.)

## Run Time Analysis


What is the worst-case time complexity of each loop?

- ì–´ë””ê°€ dominateí•œê°€ : SWë¥¼ ê°œë°œ í•  ë•Œ ìµœì í™”ë¥¼ í•´ ì£¼ì–´ì•¼ í•¨. programì´ ë„ëŠ” ê²ƒì„ ë³´ë©´ ì–´ë””ê°€ bottleneckì´ ë˜ì–´ ë¹„íš¨ìœ¨ì ì¸ê°€
- (1) Matrix Addition : $O(n^2)$

	```c
	for (i = 0; i < N; i++)
	  for (j = 0; j < N; j++)
	    a[i][j] = b[i][j] + c[i][j];
	```

- (2)$O(n^2)$
	- `x+= i+j`ê°€ ië²ˆ ìˆ˜í–‰ë˜ê³ , $\Sigma_{i=1}^N i = \frac{N(N+1)}{2} = \frac {N^2} 2 + \frac N 2$

	```c
	x = 0;
	for (i = 1; i <= N; i++)
	  for (j = 1; j <= i; j++)
	    x += i + j;
	```

- (3) $O(n^2)$

	```c
	for (i = 1; i <= N; i++)
	        if (i % 2 == 0)
	            a[i] = 1;
	        else
	            a[i] = -1;
	    // N^2
	for (i = 1; i <= N; i++)
	        for (j = 1; j <= N; j++)
	            a[i][j] = i + j;
	    // N^2
	```

- (4) $O(n^3)$
	- $\frac N 2 \cdot N$ . $\frac N 2 N^2$ â†’ $n^3$ì´ dominate

	```c
	for (i = 1; i <= N; i++)
	    {
	        if (i % 2)
	        {
	            for (j = 1; j <= N; j++)
	                a[i][j] = i + j;
	        }
	        else
	        {
	            for (j = 1; j <= N; j++)
	            {
	                a[i][j] = 0;
	                for (k = 1; k <= N; k++)
	                    a[i][j] += k;
	            }
	        }
	    }
	```

- (5)
	- $\Sigma_{i=1}^N \Sigma_{j=1}^i j= O(N^3)$

	```c
	x = 0;
	for (i = 1; i <= N; i++)
	  for (j = 1; j <= i; j++)
		//What if this is i*i?
			for (k = 1; k <= j; k++)
			x += i + j + k;
	```

- (6)$ \rightarrow O(N^4)$
	- $\Sigma_{i=1}^N \Sigma_{j=1}^{i^2} \Sigma_{k=1}^j k= O(N^4)$

	```c
	x = 0;
	for (i = 1; i<=N;i++)
	  for (j = 1; j <= i*i; j++)
	    if (j % i == 0) //jê°€ iì˜ ë°°ìˆ˜ì´ë©´
	      for (k = 1; k <= j; k++)
	        x++;
	```

	- n ì´ ì‘ëƒ, í¬ëƒì— ë”°ë¼ì„œ 10ë§Œì´ëƒ 100ë§Œì´ëƒ í•  ë•Œ ì–´ë–¤ ì†ë„ë¡œ ë‚˜ë¹ ì§ˆ ê²ƒì¸ê°€? ì–¼ë§ˆë‚˜ ì˜ ìœ ì§€ë  ê²ƒì¸ê°€?
	- í•­ìƒ jê°€ 1ë¶€í„° i^2ê¹Œì§€ ë„ëŠ”ë°, j%i ==0 ì¼ ë•Œ ê¹Œì§€ë§Œ ëˆë‹¤ : jê°€ iì˜ ë°°ìˆ˜ì¸ ê²½ìš°
	- jê°€ iì˜ ë°°ìˆ˜ì¼ ë•Œ :
		- $i = 1,2, ... n$
		- $j = 1, i, 2i, ..., i^2$
		- $\Sigma_{i=1}^{N}{(1 + i + ... + i^2)}$
		- 

What is the worst-case time complexity of each loop?

- (1)

	```c
	// n = 2^k for some positive
	    // integer k
	    for (i = 1; i < N; i++)
	    {
	        j = n;
	        while (j >= 1)
	        {
	            // some O(1) computation
	            j = j / 2;
	        }
	    }
	```

- (2)

	```c
	// n = 2^k for some positive
	    // integer k
	    i = n;
	    while (i >= 1)
	    {
	        j = i;
	        while (j <= n)
	        {
	            // some O(1) computation
	            j = 2 * j;
	        }
	        i = i / 2;
	    }
	```

- (3) Could this be faster?

	```c
	//
	    float x[n][n + 1];
	    for (i = 0; i <= n - 2; i++)
	        for (j = i + 1; j <= n - 1; j++)
	            for (k = i; k <= n; k++)
	                x[j][k] = x[j][k] â€“ x[i][k] * x[j][i] / x[i][i];
	```

- (4) Magic square : Could this be faster?

	```c
	// n: odd integer
	    for (i = 0; i < n; i++)
	        for (j = 0; j < n; j++)
	            s[i][j] = 0;
	    s[0][(n - 1) / 2] = 1;
	    j = (n - 1) / 2;
	    for (key = 2; key <= n * n; key++)
	    {
	        k = (i) ? (i - 1) : (n - 1);
	        l = (j) ? (j - 1) : (n - 1);
	        if (s[k][l])
	            i = (i + 1) % n;
	        else
	        {
	            i = k;
	            j = l;
	        }
	        s[i][j] = key;
	    }
	```

- (5)$Â O(\log n)$

	```c
	// compute x^n (n >= 0)
	    m = n;
	    power = 1;
	    z = x;
	    while (m > 0)
	    {
	        while (!(m % 2))
	        {
	            m /= 2;
	            z *= z;
	        }
	        m--;
	        power *= z;
	    }
	```

- time complexity. :Â $c_0 + c_1 n + c_2 n^2 = O(n^2)$

	```c
			x = x + 1;
	    for (i = 1; i <= n; i++)
	        y = y + 2;
	    for (i = n; i >= 1; i--)
	        for (j = n; j >= 1; j--)
	            z = z + 1;
	```

- time complexity. :Â $c( âŒŠ{log_2 n}âŒ‹+1) \cdot n^2 = O(n^2)$

	```c
	c = 0;// n > 0
	for (i = 1; i <= n; i++)
	  for (j = 1; j <= n; j++)
	    for (k = 1; k <= n; k = k*2)
	      c += 2;
	```

	- for k=1;kâ‰¤n;k=k*2
	- floor : 3.7 â†’ 3, ceil : 3.7 â†’ 4
	- $n=15 \rightarrow \lfloor log_2 15 \rfloor = \lfloor 3.*** \rfloor = 3$
	- 1 2 4 8 15
- time complexity. :Â $??= O( \sqrt n)$

	```c
			i = 1;
	    j = 1;
	    m = 0; // n > 0
	    while (j <= n)
	    {
	        i++;
	        j = j + i;
	        m = m + 2;
	    }
	```

