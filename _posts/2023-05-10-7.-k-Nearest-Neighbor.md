---
layout: post
date: 2023-05-10
title: "7. k-Nearest Neighbor"
tags: [ML, ]
categories: [Notes, ]
use_math: true
---


# k-Nearest Neighbor Classifier

- perceptron : linear classifier
- SVM : linear, nonlinear classifier
	- given data â†’ ë¶„ë¥˜í•˜ëŠ” hyperplaneë¥¼ êµ¬í•œë‹¤.
	- GDë¡œ ê³„ì‚°í•œë‹¤ : iteration ìˆ˜ì²œ~ìˆ˜ë°± epoch ì—°ì‚°ì„ ìˆ˜í–‰í•œë‹¤
	- (1) phi fnìœ¼ë¡œ ê³ ì°¨ì› mapping data ì²˜ë¦¬ ê°€ëŠ¥
		- kernel trick ($\epsilon$â†’0)
	- (2) soft margin

![0](/assets/img/2023-05-10-7.-k-Nearest-Neighbor.md/0.png)

- Store all training samples $(x_i, y_i)$ - ëª¨ë“  ë°ì´í„° ì €ì¥
	- í•™ìŠµ data ì €ì¥í•˜ë©´ ëì´ë‹¤ (extra data task ë¶ˆí•„ìš”)
- Nearest Neighbor classifier
	- Given a test sample $x_t$, locate the nearest training sample $(x_{n1}, y_{n1})$,
	- then assign $y_{n1}$as the class label of $x_t$
	- 
- k-Nearest Neighbor classifier
	- Given a test sample $x_t$, locate the k-nearest training samples $(x_{n1}, y_{n1}), ...,(x_{nk}, y_{nk}) $ then assign **majority class label** of $y_{n1}, ... y_{nk}$ to $x_t$
		- **majority class label**  : majority voting, nearst sample labeling algorithm
	- Take mean of , if they are real valued $\hat{f} = \frac 1 k \Sigma_{i=1}^{k} f(x_{ni})$
	- Also called as **Instance** based learning

![1](/assets/img/2023-05-10-7.-k-Nearest-Neighbor.md/1.png)

- test ì‹œì ì—ì„œ data ë“¤ì–´ì˜¤ë©´ í•´ë‹¹ dataì™€ near sample íŒë‹¨ : **sampleê°„ ê±°ë¦¬ ê³„ì‚°**
	- $L^2$-norm : $\sqrt{\Sigma_{d=1}^{D} |x_{td} - x_{nd}|^2}$
	- test dataì— ëŒ€í•´ ê±°ë¦¬ë¥¼ ê³„ì‚°í•  ë•Œ test sampleê³¼ nearí•œì§€ distance ê³„ì‚°í•œë‹¤.
- sample dimensionì´ ì¦ê°€í•˜ë©´ â†’ calculation cost ì¦ê°€
	- ëª¨ë“  sampleë“¤ë¼ë¦¬ ê±°ë¦¬ë¥¼ ê³„ì‚°í•˜ë©´ - dimension ì¦ê°€ë˜ë©´

	(í•´ê²°) kNN Optimization

		- í•™ìŠµ ì‹œ ê³„ì‚° ì ê²Œ storage í¬ê²Œ : ì–´ë–»ê²Œ saveí•˜ëŠ”ê°€?
		- test ì‹œ ê³„ì‚° ë§ì´ :  ì–´ë–»ê²Œ ì¤„ì´ëŠ”ê°€?
- When to consider
	- Vector features
	- ~Less than 20 attributes (=20 dim)
	- Sufficient amount of training data

![2](/assets/img/2023-05-10-7.-k-Nearest-Neighbor.md/2.png)

- **Advantages**
	- <u>**Training is very fast :**</u> feature extraction & save
	- Learn complex target functions
	- Donâ€™t lose information
- **Disadvantages**
	- Slow at query time ( â‡’ test time)
		- **train ì—ì„œì˜ ì—°ì‚°ì´ ë§ì€ ê²ƒì´ ì¢‹ì„ê¹Œ?** ì•„ë‹ˆë©´ test ì—ì„œì˜ ì—°ì‚°ì´ ë§ì€ ê²ƒì´ ì¢‹ì„ê¹Œ?
			- testí•˜ê¸° ì „ enough timeìœ¼ë¡œ ì €ì¥ (sample ê·¸ëŒ€ë¡œ ì €ì¥)
			- train offline / test online,offline
			- **soâ†’ test ì‹œì  ì—°ì‚°ì´ ì ì€ ê²ƒì´ ìœ ë¦¬í•¨**
	- Requires large storage
	- Not robust against irrelevant **attributes or outliers**
		- SVM, Perceptron : boundaryì— í¬ê²Œ ì˜í–¥ì´ ì—†ìŒ
		- kNN : test sampel decisionì— ë³€í™” ê°€ëŠ¥í•¨

# Distance Metric

- Search operation is expensive with high dimensions

## Distance


![3](/assets/img/2023-05-10-7.-k-Nearest-Neighbor.md/3.png)

- ëª¨ë“  classify / regressì—ì„œ ì¤‘ìš”í•¨
	- decision tree : nominal data â†’ distance ë¶ˆí•„ìš”
	- ê° feature ê°’ë“¤ì´ ì„œë¡œ ë‹¤ë¥¸ wë¥¼ ê°€ì§ˆ ìˆ˜ ìˆìŒ
		- ex. age, height
- distance = 1/similarity

![4](/assets/img/2023-05-10-7.-k-Nearest-Neighbor.md/4.png)


feature vectorí•˜ë‚˜ dimensionì€ ì˜ë¯¸ê°€ ì—†ìœ¼ë©°, ì „ì²´ê°€ ëª¨ì—¬ ì••ì¶•ëœ í˜•íƒœë¡œ í‘œí˜„ëœë‹¤.


## equations of selected distance functions


![5](/assets/img/2023-05-10-7.-k-Nearest-Neighbor.md/5.png)

- 

## Value difference metric (VDM)

- attribute class í†µí•´ íŒë‹¨ â†’ ë™ì¼ class ì†í•˜ëŠ” ê²ƒì´ ê´€ì°°ë˜ë©´ attributeê°€ ë” ê°€ê¹ë‹¤ê³  ë³¸ë‹¤.
	- ex. R-Bê°€ ë” ë§ì´ ê°™ì€ classë¥¼ ê°€ì§€ê³  ìˆìœ¼ë©´ â†’ R-B > R-G
- Providing distance measurements for nominal attributes
	- $vdm_a(x,y) = \Sigma_{c=1}^C (\frac{N_{a,x,c}}{N_{a,x}}-\frac{N_{a,y,c}}{N_{a,y}})^2$
	- ğ‘µğ’‚,ğ’™ : # times attribute a had value x
	ğ‘µğ’‚,ğ’™,ğ’„: # times attribute a had value x and class was c
	ğ‘ª : # output classes
	- a : color / x : big / y : medium, small ??
- Two values are considered closer if they have more similar classifications, i.e., if they have more similar correlations with the output classes

# Problem with Euclidean distance

- High dimensional data
	- dimension : color, length, weightâ€¦ë“±ì˜ data attribute
	- ë” ë§ì€ attributeë¥¼ ì¶”ê°€í• ìˆ˜ë¡ performance ìƒìŠ¹
	- ê·¸ëŸ¬ë‚˜ ë„ˆë¬´ ë§ì´ feature/ attribute ì¦ê°€ì‹œí‚¤ë©´ dimì´ ë„ˆë¬´ ë§ì´ ì¦ê°€í•˜ì—¬ curse of dimensionalityì— ë¹ ì§ˆ ìˆ˜ ìˆë‹¤
		- dataê°€ ê³ ì°¨ì›ì¼ìˆ˜ë¡ ì·¨ê¸‰ì´ ì–´ë ¤ì›€
		dataì— ê´€ë ¨í•˜ì—¬ ì•Œì•„ì•¼ í•  ì •ë³´ê°€ ë§ì€ë° ì˜ í™œìš©ì¹˜ ëª»í•˜ë©´ classifier ì„±ëŠ¥ ì €í•˜ë¨
	- (Number of samples - available info) ì–‘ì´ ë§ê³  + ì •í™•í•œ ì •ë³´ë¼ë©´ dimensionì´ ë†’ì•„ë„ ê´œì°®ë‹¤.
- Can produce counter-intuitive results
- Shrinking density â€“ sparsification effect
	- data dimensionì„ ë‚®ì¶”ì–´ ì²˜ë¦¬í•˜ë©´ dataê°€ sparseí•œ í˜•íƒœ
- dê°€ ê°™ë‹¤ê³  data featureë¥¼ ì˜ ë°˜ì˜í•˜ëŠ”ì§€ì— ëŒ€í•´ì„œëŠ” ì˜ ê³ ë ¤í•´ë´ì•¼ í•¨
	- ê° feature dimesionì´ data significanceë¥¼ ë‚˜íƒ€ëƒ„ â†’ featureê±°ë¦¬ë¥¼ ì¸¡ì •í•˜ëŠ” ê²ƒì´ ìœ ì‚¬ì„±ê³¼ ì°¨ì´ì ì„ ëœ ë°˜ì˜í•  ìˆ˜ë„ ìˆìŒ
	- binary feature : ê³¼ì—° dê°€ ë™ì¼í•œ ê°’ì´ë¼ê³  data featureë¥¼ ì˜ ë‚˜íƒ€ë‚´ëŠ” ê²ƒì¼ê¹Œ?
	- 1 1 1 1 1 1 1 1 1 1 1 0
	0 1 1 1 1 1 1 1 1 1 1 1
	d = 1.4142
	- 1 0 0 0 0 0 0 0 0 0 0 0
	0 0 0 0 0 0 0 0 0 0 0 1
	d = 1.4142
- hamming distance
	- ê° bit ì‚¬ì´ ê°™ì€/ë‹¤ë¥¸ bit ë‚˜íƒ€ë‚˜ëŠ”ì§€ íŒë‹¨
- histogram intersection
	- $\Sigma_i \min(h_{1i}, h_{2i})$

		![6](/assets/img/2023-05-10-7.-k-Nearest-Neighbor.md/6.png)


# Behavior in the limit

- ì„±ëŠ¥
	- kNN ($\epsilon$ up) < optimal classifier ($\epsilon$ down)
- $\epsilon^*(x) $ **:** Error of optimal prediction
- $\epsilon_{NN}(x) $ : Error of nearaest neighbor

Theorem : 


$$
\lim_{n\rightarrow \inf} \leq 2\epsilon^*
$$

- proof:
	- $p_+$ : dataê°€ (+)ì¼ í™•ë¥ , $p_{NN\in(-)}$ : nearest neighborì´ (-)ì¼ í™•ë¥ 
	- $\epsilon_{NN} = p_+p_{NN\in(-)}+p_-p_{NN\in(+)} = p_+(1-p_{NN\in(+) })+(1-p_+)p_{NN\in(+)} $
		- $\epsilon_{NN} \sim\epsilon_{+} $ : Nearest Neighborê³¼ optimalí•œ sampleì˜ ê²°ê³¼ê°€ ë™ì¼í•˜ë‹¤
	- $\lim_{n \rightarrow \infty} p_{NN\in (+)} = p_+$$lim_{n \rightarrow \inf} p_{NN\in (+)} = p_+$
	- $\lim_{n \rightarrow \inf} p_{NN\in (-)} = p_-$
	- $\lim_{n \rightarrow \inf} \epsilon_{NN} = p_+(1-p_+) + (1-p_+)p_+ = 2p_+(1-p_+) = 2\epsilon^*(1-\epsilon^*)\leq2 \epsilon^*$
		- $2p_+(1-p_+)$
			- prediction : (+) or (-)
			- $p_+$ê°€ ë§ìœ¼ë©´ $(1-\epsilon^*)$ ë§ê³  $\epsilon^*$ í‹€ë¦¼
			- $p_+$ê°€ ë§ìœ¼ë©´ $(1-\epsilon^*)$ í‹€ë¦¬ê³  $\epsilon^*$ ë§ìŒ
		- $\epsilon^* \in [0,1]$
	- NN Classifier can have up to twice as much error of **optical error**
		- = Bayesian Classifier
		- Samplingí•  ë•Œ íŠ¹ì • dë³´ê³  sample â†’ ì „ì²´ population know
		- â†’ ì „ì²´ population ì •ë³´ê°€ì§€ê³  classifierë¥¼ ìƒì„±í•˜ë©´ classifier error : optimal classification errorì´ê³ 
		- ìš°ë¦¬ê°€ ì–»ì„ ìµœì†Œì˜ error â†’ optimal classifier
- Theorem :

	$$
	\lim_{n\rightarrow \infty, k \rightarrow \infty, \frac k n \rightarrow 0 } \leq \epsilon^*
	$$


# Standardization

- Transform raw feature values into z-scores $z_{ij} = \frac{x_{ij} - \mu_j}{\sigma_j}$
	- $x_{ij}$ is the ith sample and jth feature (dimension)
	- $\mu_j$ is the average of all for feature
	- $\sigma_j$ is the standard deviation of all for feature

![7](/assets/img/2023-05-10-7.-k-Nearest-Neighbor.md/7.png)


# Efficient searching

- sample ì£¼ë³€ì„ í™•ì¸í•´ì•¼ í•¨ : ì–´ë– í•œ data ì£¼ë³€ì— ì¡´ì¬í• ì§€
- KD trees
	- nê°œ KD Tree êµ¬ì„±í•˜ë©´ - $\epsilon \downarrow$ : ê° class score, $w_{sum}$ê³„ì‚°

		![8](/assets/img/2023-05-10-7.-k-Nearest-Neighbor.md/8.png)

- Choose dimension
- Choose pivot (median)
- Split data, repeat
- 

![9](/assets/img/2023-05-10-7.-k-Nearest-Neighbor.md/9.png)


![10](/assets/img/2023-05-10-7.-k-Nearest-Neighbor.md/10.png)


![11](/assets/img/2023-05-10-7.-k-Nearest-Neighbor.md/11.png)


# Choosing k

- Choosing the value of k:
	- If k is too small, sensitive to noise points
	- If k is too large, neighborhood may include points from other classes
- Rule of thumb:
k = sqrt(N)
N: number of training samples
- Use N fold cross validation â€“ Pick k to minimize the cross validation error
- For each of N test example
	- Find its k nearest neighbors
	- Make a classification based on these k neighbors
	- Calculate classification error
	- Output average error over all examples
- Use the k that gives lowest average error on the training data
	- ì´ë¡ ì ìœ¼ë¡œ $k \uparrow$ì´ë©´ smootherí•´ì§

![12](/assets/img/2023-05-10-7.-k-Nearest-Neighbor.md/12.png)

- Bayes-optimal boundary given true generating model
	- ideal case : $k, n \rightarrow \infty$, $\frac k n \rightarrow 0$, $\epsilon^* \leq \epsilon_{kNN}\leq2\epsilon^*$

	$$
	\lim_{n\rightarrow \infty, k \rightarrow \infty, \frac k n \rightarrow 0 } \leq \epsilon^*
	$$


	![13](/assets/img/2023-05-10-7.-k-Nearest-Neighbor.md/13.png)

- As number of training <u>samples</u> $\rightarrow \infty$, and k becomes large, k-Nearest Neighbor classifier shows performance as good as that of Bayes classifier

# Cross-validation


![14](/assets/img/2023-05-10-7.-k-Nearest-Neighbor.md/14.png)

- train errorê°€ ë‚®ì•„ì§€ëŠ”ê²Œ ë¬´ì‘ì • ì¢‹ì§€ëŠ” ì•Šì„ ìˆ˜ ìˆë‹¤ (overfitting)

# Condensing

- Aim is to reduce the number of training samples

	ë” ì‘ì€ datasetìœ¼ë¡œì˜ ì••ì¶•

- Retain only the samples that are needed to define the decision boundary
- Decision Boundary Consistent â€“ a **subset** whose nearest neighbor decision boundary is identical to the boundary of the entire training set
	- ì „ì²´ original dataì™€ ë™ì¼í•œ boundary êµ¬í•´ì§
- Minimum Consistent Set â€“ the **smallest subset** of the training data that correctly classifies all of the original training data
	- ê°€ì¥ ì‘ì€ Sample sel
- Original data

	![15](/assets/img/2023-05-10-7.-k-Nearest-Neighbor.md/15.png)

	- decision boundary êµ¬ì„± Sampleë§Œì„ ë‚¨ê²¨ ì´ dataë§Œì„ ë‚¨ê¸´ë‹¤.
	- 2,3ê°œ ì”© ì €ì¥í•´ë„ boundaryê°€ ìœ ì§€ëœë‹¤.
	- ì „ì²´ Datasetì„ í™œìš©í•´ë„ decision boundaryëŠ” ë™ì¼í•˜ê²Œ êµ¬í•´ì§„ë‹¤.
- Condensed data

	![16](/assets/img/2023-05-10-7.-k-Nearest-Neighbor.md/16.png)

- Minimum Consistent Set

	![17](/assets/img/2023-05-10-7.-k-Nearest-Neighbor.md/17.png)


## Condensed Nearest Neighbor (CNN)

- iteration ìµœì†Œ dataë¡œ errorless êµ¬í•˜ê¸°??
1. Initialize subset with a single (or k) training example
	1. í•˜ë‚˜ì˜ Dataë¥¼ ì„ì˜ë¡œ ì¶”ì¶œ
2. Classify all remaining samples using the subset, and transfer any incorrectly classified samples to the other subset
	1. ê·¸ Dataë¥¼ ì´ìš©í•˜ì—¬ NN â†’ $\epsilon$ì´ ë‚˜ì˜¤ëŠ” ì„ì˜ì˜ Data ì„ íƒ
3. Return to 2 until no transfers occurred or the subset is full
	1. data, error êµ¬í•˜ê¸°

![18](/assets/img/2023-05-10-7.-k-Nearest-Neighbor.md/18.png)


â†’ ì´ˆê¸° Data ì„ íƒì— ë”°ë¼ decision boundary ë³€í™”

- kNN â†’ decision boundary
	- ì–´ë–¤ groupì— test data ì†í•˜ëŠ”ì§€ í™•ì¸

# Condensation

- Each cell contains one sample, and every location within the cell is closer to that sample than to any other sample.
- A **Voronoi diagram** divides the space into such cells.
	- êµ¬íšìœ¼ë¡œ ë‚˜ëˆ„ê³  boundaryì— ì˜í–¥ ì—†ëŠ” Sampleì„ ì œê±°í•˜ëŠ” ë°©í–¥
- Every query point will be assigned the classification of the sample within that cell. The decision boundary separates the class regions based on the 1-NN decision rule.
- Knowledge of this boundary is sufficient to classify new points.
- The boundary itself is rarely computed; many algorithms seek to retain only those points necessary to generate an identical boundary.

![19](/assets/img/2023-05-10-7.-k-Nearest-Neighbor.md/19.png)

- data ì£¼ì–´ì§€ë©´ ê·¸ë¡œë¶€í„° ì–»ëŠ” 1NN boundaryê°€ ìœ ì¼í•˜ê²Œ ê²°ì •ë¨
	- â†’ train data ì „ë¶€ ì €ì¥í•˜ì§€ ì•Šì•„ë„ boundaryë§Œ ì €ì¥í•˜ë©´ ë¨

## Voronoi Diagram êµ¬ì„±í•˜ëŠ” ë°©ë²•


![20](/assets/img/2023-05-10-7.-k-Nearest-Neighbor.md/20.png)

- **Delaunay triangulation (Delone triangulation)** for a given set P of discrete points is a triangulation DT(P) such that no point in P is inside the **circumcircle** of any triangle in DT(P)
- Circumcircle (circumscribed circle) of a polygon is a circle that passes through all the vertices of the polygon
- ì–´ë– í•œ ì ë„ í•´ë‹¹ Circle ë‚´ ë“¤ì–´ê°€ì§€ ì•Šë„ë¡, ë‹¤ê°í˜•ì„ ë‘˜ëŸ¬ì‹¼ ì› í˜•ì„±
	- â†’ì„¸ ì ì„ ë‘˜ëŸ¬ì‹¼ ì›, ì› ë‚´ë¶€ì— ì£¼ì–´ì§„ ì ì„ í¬í•¨í•˜ë©´ ì•ˆ ë˜ëŠ” í˜•íƒœë¡œ
- Avoid sliver triangles. (maximize angles of triangles) 
ì˜ˆê°ì‚¼ê°í˜•ì„ í”¼í•˜ê³ , ê°ë„ ìµœëŒ€í™”
	- Delaunay triangulation is not unique

		![21](/assets/img/2023-05-10-7.-k-Nearest-Neighbor.md/21.png)

- Circumcenters of Delaunay triangles are the vertices of the Voronoi diagram
- If two triangles share an edge in the Delaunay triangulation, their circumcenters are to be connected with an edge in the Voronoi tesselation
- ë°©ë²• : delaunary triangle ë§Œë“¤ê³  â†’ circumcircleì˜ ê°€ìš´ëƒì  pt â†’ centered point ì—°ê²°
- Voronoi Diagramì˜ ìµœì¢… ëª©í‘œ : ëª¨ë“  ì˜ì—­ ì•ˆì— ì„ì˜ì˜ ì§€ì ì€ ê·¸ ì•ˆì˜ Sample pointì™€ ìµœë‹¨ ê±°ë¦¬ë¥¼ ê°–ëŠ” ì˜ì—­ìœ¼ë¡œ ì •ì˜í•¨

![22](/assets/img/2023-05-10-7.-k-Nearest-Neighbor.md/22.png)


![23](/assets/img/2023-05-10-7.-k-Nearest-Neighbor.md/23.png)


![24](/assets/img/2023-05-10-7.-k-Nearest-Neighbor.md/24.png)

- ê° sample pointì˜ classì— ë”°ë¼, ì „ì²´ ì˜ì—­ì˜ class ê²°ì •

![25](/assets/img/2023-05-10-7.-k-Nearest-Neighbor.md/25.png)

- test Data ë“¤ì–´ì˜¤ë©´, ì´ë¥¼ í¬í•¨í•œ voronoi diagramì´ë©´ class ê²°ì •

![26](/assets/img/2023-05-10-7.-k-Nearest-Neighbor.md/26.png)

