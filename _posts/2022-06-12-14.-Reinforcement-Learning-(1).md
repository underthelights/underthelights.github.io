---
layout: post
date: 2022-06-12
title: "14. Reinforcement Learning (1)"
tags: [ML, ]
categories: [Notes, ]
use_math: true
---


# Characteristics of Reinforcement Learning 


![0](/assets/img/2022-06-12-14.-Reinforcement-Learning-(1).md/0.png)

- What makes reinforcement learning different from other machine learning paradigms?
	- supervised l. vs unsupervised l. vs. RL
		- supervised : label + data
		- Unsupervised : just use given data
		- RL : data + reward - Rewardì— í•´ë‹¹í•˜ëŠ” ì¶”ê°€ì ì¸ inputì´ ì¡´ì¬í•¨

	â†’ There is no supervisor, only a reward signal

- Feedback is delayed, not instantaneous
- Time really matters (sequential, non i.i.d. data)
	- ì‹œê°„ì´ ì¤‘ìš”í•œ ìš”ì†Œ ì¤‘ í•˜ë‚˜
	- sequential : ì „ë°˜ì˜ ì„ íƒì´ í›„ë°˜ì˜ ì„ íƒì— ì˜í–¥
	iid = independent identically distributed - ìƒí˜¸ ì—°ê´€
- Agentâ€™s actions affect the subsequent data it receives
	- agent actionì´ ì´í›„ dataì— ì˜í–¥ì„ ë¯¸ì¹œë‹¤.

## Examples of Reinforcement Learning

- Fly stunt manoeuvres in a helicopter
	- í—¬ë¦¬ì½¥í„°ì˜ ë¹„í–‰ ëª¨í˜•
- Defeat the world champion at Backgammon
	- backgammon ê²Œì„ì—ì„œì˜ ì‘ìš©
- Manageaninvestmentportfolio
- Controlapowerstation
- Makeahumanoidrobotwalk
- Play many different Atari games better than humans
	- ë¡œë´‡, íˆ¬ì í¬íŠ¸í´ë¦¬ì˜¤, ì•„íƒ€ë¦¬ ê²Œì„ì—ì„œì˜ í•™ìŠµ

	![1](/assets/img/2022-06-12-14.-Reinforcement-Learning-(1).md/1.png)


	![2](/assets/img/2022-06-12-14.-Reinforcement-Learning-(1).md/2.png)


# Rewards

- A reward ğ’• is a scalar feedback signal
- Indicate show well agent is doing at step t & The agentâ€™s job is to maximize cumulative reward
- ê°ê°ì˜ ì‹œê°„ì— ì–¼ë§ˆë‚˜ ì˜ í–‰ë™ í–ˆëŠ”ì§€ ë³´ê³  reward ìµœëŒ€í™”ë˜ëŠ” ë°©í–¥ìœ¼ë¡œ í–‰ë™í•˜ë„ë¡ í•™ìŠµ
- Reinforcementlearning is based on the reward hypothesis
	- reward = ì‚¬ëŒì´ ë§Œë“  ê¸°ì¤€
	ex. Atari game : target ë³„ ìµœëŒ€í•œì˜ ì ìˆ˜ë¥¼ í•™ìŠµí•  ìˆ˜ ìˆë„ë¡ í•™ìŠµì´ ë˜ê¸°ë„ í•¨. ì ìˆ˜ê°€ ë§ì€ ìª½ì„ ë” ë¹¨ë¦¬ ì–»ì„ ìˆ˜ ìˆë„ë¡ í•™ìŠµì‹œí‚¤ëŠ” ì–‘ìƒì´ ìƒê¸¸ ìˆ˜ ìˆë‹¤,
	- Reward hypothesis: all goals can be described by the m**aximization of expected cumulative reward**

## Examples of Rewards

- Fly stunt manoeuvres in a helicopter

	(+) : ì›í•˜ëŠ” ê¶¤ì ì„ ê·¸ë¦¬ë©° ë‚ ì•„ê°ˆ ë•Œ
	(-) : crashing ì‹œ ë§ˆì´ë„ˆìŠ¤ ã…ê³¼

	- +ve reward for following desired trajectory
	- âˆ’ve reward for crashing
- Defeat the world champion at Backgammon
	- +/âˆ’ve reward for winning/losing a game
- Manage an investment portfolio

	(+) : ì›í•˜ëŠ” ì´ìµ
	(-) : ì†ì‹¤

	- +ve reward for each $ in bank
- Control a power station

	(+) : ì ì ˆí•œ ì „ë ¥ ê³µê¸‰
	(-) :

	- +ve reward for producing power
	- âˆ’ve reward for exceeding safety thresholds
- Make a humanoid robot walk

	(+) : ì£¼ì–´ì§„ í™˜ê²½ì—ì„œ target ë¬¼ì§ˆì„ í™•ë³´ì—ì„œ mission ì˜ ìˆ˜í–‰
	(-) : ë„˜ì–´ì§

	- +ve reward for forward motion
	- âˆ’ve reward for falling over
- Play many different Atari games better than humans

	(+) : ì ìˆ˜ ì–»ê±°ë‚˜
	(-) : ì ìˆ˜ ìƒê±°ë‚˜
	-> ë¹ ë¥¸ ì‹œê°„ ì•ˆì— ì ìˆ˜ë¥¼ ë§ì´ ì–»ëŠ” ë°©í–¥ìœ¼ë¡œ

	- +/âˆ’ve reward for increasing/decreasing score

# Sequential Decision Making

- í˜„ì¬ì˜ actionì´ ë‹¤ìŒ í„´ actionì— ì˜í–¥ì„ ë¯¸ì¹˜ëŠ”ë°, ì˜¤ëœ turnì— ëŒ€í•´ ì˜í–¥ì„ ë¼ì¹ ìˆ˜ë„ ìˆìŒ.
- Goal: select actions to maximize total future reward
	- ì¼ë ¨ì˜ í–‰ë™ì— ë”°ë¥¸ rewardê°€ ìµœëŒ€ê°€ ë˜ë„ë¡ í•™ìŠµí•œë‹¤
- Actions may have long term consequences
	- stateê°€ ìˆê³  actionì„ ì·¨í•´ì„œ s1-(a1)->s2-(a2)->s3
- Reward may be delayed rewardëŠ” delayë¥¼ ìˆ˜ë°˜í•˜ì—¬ ì£¼ì–´ì§ˆ ìˆ˜ ìˆë‹¤
- í˜„ì¬ actionìœ¼ë¡œ ì¸í•œ rewardì— ë” ì¤‘ì ì„ ë‘˜ ê²ƒì¸ì§€, ë¯¸ë˜ì˜ rewardì— ì¤‘ì ì„ ë” ë‘˜ ê²ƒì¸ì§€ : user settingí•  ìˆ˜ë„ ìˆê³  í•™ìŠµ ë‹¨ê³„ì—ì„œ ì–´ë–»ê²Œ parameterë¥¼ ì„¤ì •í–ˆëŠ”ì§€ì— ë”°ë¼ / í•™ìŠµì´ ì˜ íš¨ê³¼ì ìœ¼ë¡œ ì´ë£¨ì–´ì§ˆìˆ˜ ìˆëŠ”ì§€ë¥¼ ê³ ë ¤í•˜ì—¬ ëª¨ìˆ˜ ì¡°ì •
	- (greedy) í˜„ì¬ rewardì— ì´ˆì ì„ ë§ì¶”ëŠ” ê²½ìš° - current reward
	- (optimal) ì „ì²´ rewardì— ì´ˆì ì„ ë§ì¶”ëŠ” ê²½ìš° - total reward
		- Itmay be better to sacrifice immediate reward to gain more long-term reward (greedy optimal)
- Examples:

	Ex.
	-íˆ¬ì :ë‹¹ì¥ì€ ì†í•´ê°€ ë‚˜ë”ë¼ë„ ë¯¸ë˜ ì‹œì ì— ìˆ˜ìµ
	-í—¬ë¦¬ì½¥í„° ì£¼í–‰ ì¤‘ ì—°ë£Œ ì£¼ì… : crashí•˜ë©´ negative penaltyí•˜ê¸°ì— í˜„ì¬ë¡œì„œëŠ” reward ì¤„ì§€ë§Œ optimalí•˜ê²ŒëŠ” ëŠ˜ì–´ë‚˜ëŠ” reward
	-ì²´ìŠ¤ì—ì„œ ìƒëŒ€ë°© ì´ë™ : ë³¸ì¸ ì ìˆ˜ ì·¨í•˜ëŠ” ê²ƒë³´ë‹¤ ìƒëŒ€ë°© ë°©í•´ê°€ ì „ì²´ì ìœ¼ë¡œ ë” ì´ë“ì¼ìˆ˜ë„ ìˆëŠ” ê²½ìš°

	- A financial investment (may take months to be mature)
	- Refueling a helicopter (might prevent a crash in several hours)
	- Blocking opponent moves (might help winning chances many moves from now)

# Agent and Environment


![3](/assets/img/2022-06-12-14.-Reinforcement-Learning-(1).md/3.png)

- At each stept the agent: agentê°€ ì£¼ë³€ì„ ê´€ì°°í•˜ê³ , rewardë¥¼ ë°›ì•„ actionì„ ì·¨í•¨
	- Executes action At
	- Receives observation Ot
	- Receives scalar reward Rt

![4](/assets/img/2022-06-12-14.-Reinforcement-Learning-(1).md/4.png)

- The environment:
	- Receives action At
	- Emits observation Ot+1
	- Emits scalar reward Rt+1
- t increments at env. step

<agent, environmentì˜ ìƒí˜¸ì‘ìš©>
agentëŠ” actionì„ ì·¨í•˜ê³  stateì— ë”°ë¼ Rewardë¥¼ ë°›ê²Œ ë¨
envëŠ” actionì„ ë°›ì•„ë“¤ì—¬ì„œ agentì—ê²Œ ì£¼ê³  ë³€í™˜ëœ statementë¥¼ agentì—ê²Œ ì¤Œ

- tíƒ€ì„ìœ¼ë¡œ ì´ë£¨ì–´ì§€ëŠ” ìš”ì†Œë“¤

actionì— ëŒ€í•´ì„œ rewardì™€ statementì˜ ë³€í™”


![5](/assets/img/2022-06-12-14.-Reinforcement-Learning-(1).md/5.png)


# Major Components of an RL Agent 

- An RL agent may include one or more of these components:
	- Policy: agentâ€™s behavior function í–‰ë™ ì •ì˜
	- Value function: how good is each state and/or action ì–¼ë§ˆë‚˜ ì¢‹ì€ê°€
	- Model: agentâ€™s representation of the environment  í•™ìŠµ ëª¨ë¸

## Example - Maze 


![6](/assets/img/2022-06-12-14.-Reinforcement-Learning-(1).md/6.png)

- Agent: explores environment and gets reward
- Environment: agent ëŒì•„ë‹¤ë‹ˆëŠ” í™˜ê²½ situation being explored by the agent
- States: ìœ„ì¹˜ - positions/locations in the environment
- Actions: ìƒí•˜ì¢Œìš° - allowed movements for the agent
- Reward: what agent gets as it moves

![7](/assets/img/2022-06-12-14.-Reinforcement-Learning-(1).md/7.png)

- For example, bomb has reward -10, germ has reward 10, every other move has rewards -1

	â†’ ë¶ˆí•„ìš”í•œ ì´ë™ì„ ìµœì†Œí™”ì‹œí‚¤ê¸° ìœ„í•œ ì¥ì¹˜


s6 is blocked


![8](/assets/img/2022-06-12-14.-Reinforcement-Learning-(1).md/8.png)


![9](/assets/img/2022-06-12-14.-Reinforcement-Learning-(1).md/9.png)


# Bellman equation 


$$
V(s) = max_a(R(s,a) + \gamma V(s'))
$$

- $R(s,a)Â $: reward: stateì—ì„œ ì·¨í•œ actionì— ë”°ë¥¸ reward
- $V(s)$ : is the value function - value function:ì „ì²´ reward ë¥¼ ì–´ë–»ê²Œ í‘œí˜„í•  ê²ƒì¸ê°€
- $\gamma$ : is the discounting factor
	- í˜„ì¬-ë¯¸ë˜ rewardì¤‘ ì–´ëŠ ê²ƒì— ì´ˆì ì„ ë§ì¶œ ê²ƒì¸ì§€ ì¤‘ìš”ë„ ë§ì¶”ëŠ” ìƒìˆ˜
- $s'$ : is the next state agent can go from
	- s : í˜„ì¬ state, sâ€™ : next state
- Bellman equation is used to calculate the value function 
â†’ ê° stateì— ëŒ€í•œ value functionê°’ìœ¼ë¡œ ì£¼ì–´ì§€ê²Œ ë¨ : í™˜ê²½ì´ ë°”ë€Œë©´ ê²°ê³¼ê°€ ë°”ë€Œê²Œ ë¨
R(s,a) : current reward
V(sâ€™) : all futer reward
- ì¼ë°˜ì ì¸ ê·œì¹™:í­íƒ„,ë³´ì„ì´ ìˆê³  envê°€ ë‹¬ë ¤ì ”ì„ ë•Œ í•™ìŠµì„ ë” ì˜ í• ê²ƒì¸ê°€->bellman eqë¡œ value fnìœ¼ë¡œ í•˜ëŠ”ê±°ëŠ” í™˜ê²½ ë°”ë€Œë©´ ë‹¤ì‹œ ì ìš©í•´ì•¼ í•¨

![10](/assets/img/2022-06-12-14.-Reinforcement-Learning-(1).md/10.png)

- T calculate V(1) ,consider a path s1- s2-s3-s7-s11-s12

s1ì— ëŒ€í•´ ê°€ì¥ í° state functionì˜ ê²°ê³¼ë¥¼ ë§Œë“œëŠ” ê°’ì„ ì·¨í•˜ë„ë¡ í–ˆë‹¤.

- (assume $\gamma = 1$)
	- V(1) = R(s1, â†’) + V(2) = -1+V(2)
	- V(2) = R(s2, â†’) + V(3) = -1+V(3)
	- V(3) = R(s3, $\downarrow$) + V(7) = -1+V(7)
	- V(7) = R(s7, $\downarrow$) + V(11) = -1+V(11)
	- V(11) = R(s11, â†’) + V(12) = -1+V(12)
- Since V(12) = 10
	- We can get V(11)=9, V(7)=8, V(3)=7, V(2) = 6, V(1) = 5
- We can consider other path, s1-s2-s3-s4- s3-s7-s11-s12 to calculate V(1), in which case V(1) will be less than 5 14

![11](/assets/img/2022-06-12-14.-Reinforcement-Learning-(1).md/11.png)


$$
V(s) = max_a(R(s,a) + \gamma V(s'))
$$

- By calculating V(s) for all states
	- Agent can move to the state with larger state value
- ì„ì˜ì˜ ì¶œë°œì ì—ì„œ state function ì»¤ì§€ëŠ” ìª½ìœ¼ë¡œ actionì„ ì·¨í•˜ë©´ ëœë‹¤
â†’ equationì„ ì´ìš©í•´ì„œ value funcitionì„ êµ¬í•œí›„ ìµœì ì˜ pathë¥¼ êµ¬í•  ìˆ˜ ìˆë‹¤
